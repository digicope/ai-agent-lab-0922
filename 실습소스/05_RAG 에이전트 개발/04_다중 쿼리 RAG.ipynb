{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f334aa0f-913e-464f-8511-b60994d8934d",
   "metadata": {},
   "source": [
    "## 다중 쿼리 RAG(Multi-Query RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437095b7-db14-445a-8279-eb630e382bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일의 내용 불러오기\n",
    "load_dotenv(\"C:/env/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deccf48d-9354-4de0-9b54-e21b3a789826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 에이전트 답변 ===\n",
      "LangChain에서 Multi-Query RAG(Reading and Generating) 시스템의 장점과 활용 방법에 대한 구체적인 정보는 제공되지 않았습니다. 그러나 일반적으로 Multi-Query RAG의 장점과 활용 방법에 대해 설명할 수 있습니다.\n",
      "\n",
      "### 장점\n",
      "1. **효율성**: 여러 쿼리를 동시에 처리할 수 있어, 정보 검색과 응답 생성의 속도를 높일 수 있습니다.\n",
      "2. **정확성**: 다양한 쿼리를 통해 더 많은 정보를 수집하고, 이를 바탕으로 보다 정확한 응답을 생성할 수 있습니다.\n",
      "3. **유연성**: 다양한 데이터 소스와 통합하여 사용할 수 있어, 다양한 도메인에서 활용할 수 있습니다.\n",
      "4. **사용자 경험 향상**: 사용자에게 더 나은 응답을 제공함으로써, 전반적인 사용자 경험을 개선할 수 있습니다.\n",
      "\n",
      "### 활용 방법\n",
      "1. **정보 검색**: 사용자가 입력한 여러 질문에 대해 동시에 정보를 검색하고, 이를 종합하여 응답을 생성할 수 있습니다.\n",
      "2. **대화형 시스템**: 챗봇이나 가상 비서와 같은 대화형 시스템에서 여러 쿼리를 처리하여 사용자와의 상호작용을 개선할 수 있습니다.\n",
      "3. **데이터 분석**: 다양한 데이터 소스에서 정보를 수집하고 분석하여, 인사이트를 도출하는 데 활용할 수 있습니다.\n",
      "4. **교육 및 학습**: 학습 자료를 제공하거나, 질문에 대한 답변을 제공하는 교육용 애플리케이션에서 사용할 수 있습니다.\n",
      "\n",
      "이와 같은 장점과 활용 방법을 통해 LangChain의 Multi-Query RAG는 다양한 분야에서 유용하게 사용될 수 있습니다. 추가적인 정보가 필요하다면, 관련 문서를 참조하거나 구체적인 질문을 해주시면 더 도움을 드릴 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-Query RAG 에이전트 예제 (LangChain v1.0 + PDF)\n",
    "- PDF 문서를 로드하여 Chroma에 저장\n",
    "- MultiQueryRetriever를 사용해 다중 질의 수행\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "# ① LangChain 핵심 및 OpenAI\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# ② PDF 로더\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# ③ MultiQueryRetriever (구버전 경로)\n",
    "from langchain_classic.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# ④ Agent 관련\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 벡터스토어 준비\n",
    "# ---------------------------------------------------------\n",
    "def build_or_load_vectorstore(pdf_dir: str = \"./docs\",\n",
    "                              persist_directory: str = \"./chroma_mqrag_pdf\") -> Chroma:\n",
    "    \"\"\"\n",
    "    지정 폴더의 PDF 문서를 로드하여 Chroma 벡터스토어를 생성/로드한다.\n",
    "    \"\"\"\n",
    "    os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "    # PDF 폴더 내 PDF가 없으면 안내\n",
    "    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(\".pdf\")]\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"{pdf_dir} 폴더에 PDF 문서가 없습니다. 예시용 PDF를 한두 개 넣어주세요.\"\n",
    "        )\n",
    "\n",
    "    # 모든 PDF 문서 로드\n",
    "    docs_all = []\n",
    "    for file in pdf_files:\n",
    "        loader = PyPDFLoader(os.path.join(pdf_dir, file))\n",
    "        docs = loader.load()\n",
    "        docs_all.extend(docs)\n",
    "\n",
    "    # 텍스트 분할\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=120,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    docs_split: List[Document] = splitter.split_documents(docs_all)\n",
    "\n",
    "    # 임베딩 & 벡터스토어\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    if os.path.exists(persist_directory):\n",
    "        vs = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "        if docs_split:\n",
    "            vs.add_documents(docs_split)\n",
    "    else:\n",
    "        vs = Chroma.from_documents(\n",
    "            documents=docs_split,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "    return vs\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Multi-Query Retriever 준비\n",
    "# ---------------------------------------------------------\n",
    "def build_multi_query_retriever(vs: Chroma, llm: ChatOpenAI) -> MultiQueryRetriever:\n",
    "    mq_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\",\n",
    "             \"너는 정보 검색 전문가이다. 사용자의 질문을 다른 관점의 5개 검색 질의로 재작성한다. \"\n",
    "             \"동의어, 상위/하위 개념, 원인/결과, 예시/반례, 비교/대조 관점 등을 섞어서 \"\n",
    "             \"중복 없이 간결한 질의만 한 줄에 하나씩 출력한다.\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    base_retriever = vs.as_retriever(search_kwargs={\"k\": 5})\n",
    "    mq_retriever = MultiQueryRetriever.from_llm(\n",
    "        retriever=base_retriever,\n",
    "        llm=llm,\n",
    "        prompt=mq_prompt\n",
    "    )\n",
    "    return mq_retriever\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 답변 생성 프롬프트\n",
    "# ---------------------------------------------------------\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "         \"너는 근거 중심의 도우미이다. 주어진 문서 조각들을 바탕으로 응답을 생성한다. \"\n",
    "         \"사실만 말하고 추측을 피한다. 답변 끝에 참고한 출처를 [출처 n] 형식으로 나열한다.\"),\n",
    "        (\"human\",\n",
    "         \"질문:\\n{question}\\n\\n\"\n",
    "         \"다음은 참고 문서 조각이다:\\n\\n\"\n",
    "         \"{context}\\n\\n\"\n",
    "         \"요구사항:\\n\"\n",
    "         \"- 한국어로 간결하게 답한다.\\n\"\n",
    "         \"- 모르면 모른다고 말한다.\\n\"\n",
    "         \"- 답변 하단에 [출처 1], [출처 2] 형태로 인용한다.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def format_context(docs: List[Document]) -> str:\n",
    "    lines = []\n",
    "    for idx, d in enumerate(docs, start=1):\n",
    "        src = d.metadata.get(\"source\", \"unknown\")\n",
    "        lines.append(f\"[출처 {idx}] source={src}\\n{d.page_content[:500].strip()}\\n\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 툴 정의: Multi-Query RAG 질의 도구\n",
    "# ---------------------------------------------------------\n",
    "@tool\n",
    "def ask_multi_rag(question: str) -> str:\n",
    "    \"\"\"PDF 기반 Multi-Query RAG 질의\"\"\"\n",
    "    global MQ_RETRIEVER, LLM\n",
    "    retrieved = MQ_RETRIEVER.invoke(question)\n",
    "    if not retrieved:\n",
    "        return \"관련 문서를 찾지 못했습니다.\"\n",
    "\n",
    "    ctx = format_context(retrieved)\n",
    "    prompt = ANSWER_PROMPT.format_messages(question=question, context=ctx)\n",
    "    resp = LLM.invoke(prompt)\n",
    "    return resp.content\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 메인\n",
    "# ---------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    LLM = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    vectorstore = build_or_load_vectorstore(\"./docs\", \"./chroma_mqrag_pdf\")\n",
    "    MQ_RETRIEVER = build_multi_query_retriever(vectorstore, LLM)\n",
    "\n",
    "    agent = create_agent(LLM, [ask_multi_rag])\n",
    "\n",
    "    query = \"LangChain에서 Multi-Query RAG의 장점과 활용 방법은?\"\n",
    "    result = agent.invoke({\n",
    "        \"messages\": [HumanMessage(content=query)]\n",
    "    })\n",
    "\n",
    "    print(\"\\n=== 에이전트 답변 ===\")\n",
    "    print(result[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b818a2-bc61-4319-bcf1-33992eb7092b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
