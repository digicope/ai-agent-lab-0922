{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e353a79-7eec-47fe-8813-d2918f9f1967",
   "metadata": {},
   "source": [
    "### 이미지 포함 문서 요약 및 질의응답 처리 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c41388-046b-411d-b40a-298af6f84e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일의 내용 불러오기\n",
    "load_dotenv(\"C:/env/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a320628-f743-42ee-9a9e-b3070960687c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) 문서 로드 및 이미지 캡셔닝 진행 중...\n",
      "   - 생성된 Document 개수: 9\n",
      "2) 벡터스토어 빌드 및 저장 중...\n",
      "   - Chroma 디렉터리: chroma_store\n",
      "\n",
      "3) 전체 요약 생성 중...\n",
      "\n",
      "[문서 요약]\n",
      "\n",
      "**통합 요약**\n",
      "\n",
      "본 문서는 엣지 컴퓨팅의 필요성과 구조, 관련 기술 및 프로젝트에 대한 포괄적인 개요를 제공한다. 엣지 컴퓨팅은 스마트 기기와 IoT 단말의 증가로 인해 발생하는 데이터 전송 및 처리 지연 문제를 해결하기 위해 등장했으며, 네트워크의 가장자리에서 클라우드 컴퓨팅 기술을 제공하여 데이터 수집 및 분석을 사용자 단말 근처에서 수행함으로써 네트워크 자원을 절약하고 데이터 전송 시간을 단축한다. \n",
      "\n",
      "MEC(Mobile Edge Computing) Reference Architecture는 엣지 컴퓨팅을 위한 프레임워크로, Mobile Edge Host Management와 Mobile Edge System Management로 구성된다. MEC 플랫폼은 가상화 인프라에서 MEC 애플리케이션을 실행하고, 서비스 관리를 위한 기능을 제공하며, OPNFV와 같은 오픈소스 프로젝트와의 연계를 통해 엣지 클라우드의 요구사항을 수용하기 위한 작업이 진행되고 있다. \n",
      "\n",
      "StarlingX와 Akraino Edge Stack은 엣지 클라우드 환경을 위한 주요 플랫폼으로, 각각 고가용성, QoS, 보안, 저지연성을 제공하며, 다양한 관리 기능과 배포 시나리오를 지원한다. Living Edge Lab은 오픈 엣지 컴퓨팅 플랫폼으로 다양한 테스트 환경을 제공하며, MobiledgeX는 ETSI MEC 표준에 따라 엣지 클라우드 인프라를 구성하고 모바일 단말에 근접한 애플리케이션 배포 구조를 제시한다.\n",
      "\n",
      "현재 엣지 컴퓨팅 기술은 연구 중이며, 기존 클라우드와 비교해 명확한 스펙이나 플랫폼이 부족하지만, 많은 클라우드 사업자들이 실제 서비스 플랫폼을 제공하고 있으며, 표준 단체 및 오픈소스 플랫폼에서 엣지 클라우드 구조가 설계되고 구현되고 있다. 엣지 클라우드 시장은 앞으로 더욱 크고 빠르게 성장할 것으로 예상되며, 관련 연구 및 개발에 대한 참여가 필요하다.\n",
      "\n",
      "필자들은 SDN/NFV, 클라우드, IoT 분야에 대한 전문성을 가지고 있으며, 연구 및 교육 경력을 통해 후배 연구자들을 지도하고 있다.\n",
      "\n",
      "4) 질의응답 테스트 예시\n",
      "\n",
      "Q: 문서의 핵심 결론은 무엇인가?\n",
      "A: 문서의 핵심 결론은 조작된 리뷰 탐지 및 극단적인 리뷰 탐지를 위한 다양한 방법론과 기술들이 제시되었으며, 특히 감성 분석과 토픽 모델링 기법이 효과적으로 활용되었다는 점입니다. 연구에서는 SVM 알고리즘을 통해 리뷰 데이터의 감정적 극성을 자동으로 분류하고, LDA 모델을 통해 리뷰 텍스트의 주요 주제를 심층적으로 분석하였습니다. 이러한 접근은 조작된 리뷰의 탐지 성능을 개선하는 데 기여할 수 있음을 시사합니다[유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지.pdf p.9].\n",
      "\n",
      "Q: 그림이나 도표가 설명하는 핵심 메시지는 무엇인가?\n",
      "A: 그림이나 도표가 설명하는 핵심 메시지는 특정 변수들이 서울시 지하철 혼잡도 예측에 어떻게 기여하는지를 시각적으로 나타내는 것입니다. 예를 들어, 의사결정 플롯(Decision Plot)은 각 변수의 기여도를 내림차순으로 보여주며, 혼잡도를 높이는 변수와 낮추는 변수를 구분하여 설명합니다. 이를 통해 사용자는 혼잡도 예측의 근거를 이해하고, 특정 상황에서 변수들이 혼잡도에 미치는 영향을 파악할 수 있습니다. 이러한 시각화는 정책 입안자나 시민들이 예측 결과를 이해하는 데 도움을 줍니다(출처: [인공지능 활용 서울시 지하철 혼잡도 예측.pdf, p.11]).\n",
      "\n",
      "Q: 한계나 주의사항이 언급되었는가?\n",
      "A: 제공된 컨텍스트에서는 인공지능을 활용한 서울시 지하철 혼잡도 예측에 대한 한계나 주의사항이 언급되고 있습니다. 특히, 머신러닝과 딥러닝 알고리즘이 높은 성능의 예측값을 제공하지만, 그 예측의 근거를 사람이 직관적으로 이해하기 어렵다는 점이 지적되었습니다. 이러한 알고리즘은 \"블랙박스\" 모델로 불리며, 내부 의사결정 과정이 불투명하여 사용자가 모델의 결정 이유를 해석하거나 검증하기 어렵다는 한계가 있습니다. 이를 극복하기 위해 설명 가능한 인공지능(Explainable AI, XAI)에 대한 관심이 높아지고 있다는 내용이 포함되어 있습니다[인공지능 활용 서울시 지하철 혼잡도 예측.pdf p.4].\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "이미지 포함 문서 요약 및 질의응답 에이전트 (LangChain v1.0 + OpenAI API)\n",
    "\n",
    "폴더 구조 예시\n",
    ".\n",
    "├─ vision_rag_agent.py\n",
    "└─ docs/\n",
    "   ├─ sample.pdf\n",
    "   ├─ figure1.png\n",
    "   └─ scan_page.jpg\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --------- 환경 설정 ----------\n",
    "OPENAI_VISION_MODEL = \"gpt-4o-mini\"\n",
    "OPENAI_TEXT_MODEL = \"gpt-4o-mini\"\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "CHROMA_DIR = \"chroma_store\"\n",
    "\n",
    "# --------- 공통 LLM/임베딩 ----------\n",
    "llm = ChatOpenAI(model=OPENAI_TEXT_MODEL, temperature=0.1)\n",
    "emb = OpenAIEmbeddings(model=EMBED_MODEL)\n",
    "\n",
    "# --------- 유틸: 이미지 바이트 -> PNG base64 ----------\n",
    "def to_png_bytes(img: Image.Image) -> bytes:\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"PNG\")\n",
    "    return buf.getvalue()\n",
    "\n",
    "def pil_from_bytes(raw: bytes) -> Image.Image:\n",
    "    return Image.open(io.BytesIO(raw)).convert(\"RGB\")\n",
    "\n",
    "def b64_png(raw: bytes) -> str:\n",
    "    return base64.b64encode(raw).decode(\"utf-8\")\n",
    "\n",
    "# --------- OpenAI Vision 캡셔닝 ----------\n",
    "def caption_image_bytes(raw_bytes: bytes, prompt: str = \"이미지의 핵심 내용을 간결히 설명하라. 글머리표 없이 1~3문장으로 요약하라.\") -> str:\n",
    "    \"\"\"\n",
    "    ChatOpenAI 래퍼는 텍스트 중심 설계이므로, 이미지 캡셔닝은 messages에 image_url(data URI)을 직접 넣어 호출한다.\n",
    "    \"\"\"\n",
    "    data_uri = f\"data:image/png;base64,{b64_png(raw_bytes)}\"\n",
    "    # LangChain 래퍼의 .invoke에 멀티모달 content를 직접 전달\n",
    "    resp = llm.invoke(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri}},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return resp.content.strip()\n",
    "\n",
    "# --------- 문서 로드: PDF에서 텍스트와 이미지 추출 ----------\n",
    "@dataclass\n",
    "class PageExtraction:\n",
    "    page_number: int\n",
    "    text: str\n",
    "    image_captions: List[str]\n",
    "\n",
    "def extract_from_pdf(pdf_path: str) -> List[PageExtraction]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    results: List[PageExtraction] = []\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text(\"text\") or \"\"\n",
    "        captions = []\n",
    "        # 페이지 내 이미지 객체 순회\n",
    "        for img_index, img in enumerate(page.get_images(full=True) or []):\n",
    "            xref = img[0]\n",
    "            pix = fitz.Pixmap(doc, xref)\n",
    "            if pix.alpha:  # RGBA -> RGB\n",
    "                pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "            raw = pix.tobytes(\"png\")\n",
    "            try:\n",
    "                cap = caption_image_bytes(raw)\n",
    "                captions.append(f\"[이미지{i+1}-{img_index+1}] {cap}\")\n",
    "            except Exception as e:\n",
    "                captions.append(f\"[이미지{i+1}-{img_index+1}] 캡션 실패: {e}\")\n",
    "        results.append(PageExtraction(page_number=i + 1, text=text, image_captions=captions))\n",
    "    doc.close()\n",
    "    return results\n",
    "\n",
    "# --------- 단일 이미지 파일 로드 ----------\n",
    "def extract_from_image(path: str) -> List[PageExtraction]:\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw = f.read()\n",
    "    cap = caption_image_bytes(raw)\n",
    "    # 이미지 단독 문서도 page처럼 포맷을 맞춰 반환\n",
    "    return [PageExtraction(page_number=1, text=\"\", image_captions=[f\"[이미지] {cap}\"])]\n",
    "\n",
    "# --------- Document 생성 ----------\n",
    "def build_documents(paths: List[str]) -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for p in paths:\n",
    "        name = os.path.basename(p)\n",
    "        ext = os.path.splitext(p.lower())[1]\n",
    "        if ext in [\".pdf\"]:\n",
    "            pages = extract_from_pdf(p)\n",
    "            for pe in pages:\n",
    "                combined = \"\"\n",
    "                if pe.text.strip():\n",
    "                    combined += f\"[본문 p.{pe.page_number}]\\n{pe.text.strip()}\\n\\n\"\n",
    "                if pe.image_captions:\n",
    "                    combined += \"[이미지 캡션]\\n\" + \"\\n\".join(pe.image_captions) + \"\\n\"\n",
    "                if combined.strip():\n",
    "                    docs.append(\n",
    "                        Document(\n",
    "                            page_content=combined.strip(),\n",
    "                            metadata={\"source\": name, \"page\": pe.page_number},\n",
    "                        )\n",
    "                    )\n",
    "        elif ext in [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\"]:\n",
    "            pages = extract_from_image(p)\n",
    "            for pe in pages:\n",
    "                combined = \"\"\n",
    "                if pe.image_captions:\n",
    "                    combined += \"[이미지 캡션]\\n\" + \"\\n\".join(pe.image_captions) + \"\\n\"\n",
    "                docs.append(\n",
    "                    Document(\n",
    "                        page_content=combined.strip(),\n",
    "                        metadata={\"source\": name, \"page\": pe.page_number},\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            # 단순 텍스트 파일도 허용\n",
    "            if ext in [\".txt\", \".md\"]:\n",
    "                with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    content = f.read()\n",
    "                docs.append(\n",
    "                    Document(\n",
    "                        page_content=content.strip(),\n",
    "                        metadata={\"source\": name, \"page\": 1},\n",
    "                    )\n",
    "                )\n",
    "    return docs\n",
    "\n",
    "# --------- 인덱싱 ----------\n",
    "def build_vectorstore(docs: List[Document]) -> Chroma:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=150)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    vs = Chroma.from_documents(documents=chunks, embedding=emb, persist_directory=CHROMA_DIR)\n",
    "    return vs\n",
    "\n",
    "# --------- 요약 체인 (맵-리듀스 스타일) ----------\n",
    "def summarize_docs(docs: List[Document]) -> str:\n",
    "    map_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"너는 전문 문서 요약가이다. 텍스트와 이미지 캡션을 모두 고려해 핵심을 압축하여 메모 형태로 요약하라.\"),\n",
    "            (\"human\", \"다음 내용을 5~8개의 항목으로 핵심만 요약하라.\\n\\n{chunk}\"),\n",
    "        ]\n",
    "    )\n",
    "    reduce_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"너는 여러 요약을 하나로 통합하는 편집자이다.\"),\n",
    "            (\"human\", \"다음 부분 요약들을 서로 겹치지 않게 통합 요약하라.\\n\\n{chunks}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    map_chain = map_prompt | llm | StrOutputParser()\n",
    "    reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # 간단한 수동 맵-리듀스\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "    texts = [d.page_content for d in docs]\n",
    "    merged = \"\\n\\n\".join(texts)\n",
    "    parts = splitter.split_text(merged)\n",
    "\n",
    "    partial_summaries = [map_chain.invoke({\"chunk\": p}) for p in parts]\n",
    "    final_summary = reduce_chain.invoke({\"chunks\": \"\\n\\n\".join(partial_summaries)})\n",
    "    return final_summary.strip()\n",
    "\n",
    "# --------- 질의응답 체인 (RAG) ----------\n",
    "def build_rag_chain(vs: Chroma):\n",
    "    retriever = vs.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                (\n",
    "                    \"너는 문서 기반 어시스턴트이다. 항상 주어진 컨텍스트만을 근거로 대답하라. \"\n",
    "                    \"근거가 없으면 모른다고 답하고, 가능한 경우 인용 출처(source, page)를 함께 제시하라.\"\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"질문: {question}\\n\\n다음은 검색된 컨텍스트이다:\\n{context}\\n\\n규칙: 1) 추론을 꾸미지 말라 2) 한국어로 답하라\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def format_docs(docs: List[Document]) -> str:\n",
    "        out = []\n",
    "        for d in docs:\n",
    "            src = d.metadata.get(\"source\")\n",
    "            page = d.metadata.get(\"page\")\n",
    "            out.append(f\"[{src} p.{page}]\\n{d.page_content}\")\n",
    "        return \"\\n\\n\".join(out)\n",
    "\n",
    "    # LCEL: 병렬로 retrieve와 질문 전달\n",
    "    setup = RunnableParallel(\n",
    "        context=retriever | (lambda docs: format_docs(docs)),\n",
    "        question=RunnablePassthrough(),\n",
    "    )\n",
    "    rag_chain = setup | qa_prompt | llm | StrOutputParser()\n",
    "    return rag_chain\n",
    "\n",
    "# --------- 실행 예시 ----------\n",
    "def main():\n",
    "    docs_dir = \"docs\"\n",
    "    paths = [os.path.join(docs_dir, f) for f in os.listdir(docs_dir)] if os.path.isdir(docs_dir) else []\n",
    "    if not paths:\n",
    "        print(\"docs 폴더에 PDF나 이미지 파일을 넣은 뒤 다시 실행한다.\")\n",
    "        return\n",
    "\n",
    "    print(\"1) 문서 로드 및 이미지 캡셔닝 진행 중...\")\n",
    "    docs = build_documents(paths)\n",
    "    print(f\"   - 생성된 Document 개수: {len(docs)}\")\n",
    "\n",
    "    print(\"2) 벡터스토어 빌드 및 저장 중...\")\n",
    "    vs = build_vectorstore(docs)\n",
    "    print(f\"   - Chroma 디렉터리: {CHROMA_DIR}\")\n",
    "\n",
    "    # 요약\n",
    "    print(\"\\n3) 전체 요약 생성 중...\")\n",
    "    summary = summarize_docs(docs)\n",
    "    print(\"\\n[문서 요약]\\n\")\n",
    "    print(summary)\n",
    "\n",
    "    # 질의응답\n",
    "    rag = build_rag_chain(vs)\n",
    "    print(\"\\n4) 질의응답 테스트 예시\")\n",
    "    sample_questions = [\n",
    "        \"문서의 핵심 결론은 무엇인가?\",\n",
    "        \"그림이나 도표가 설명하는 핵심 메시지는 무엇인가?\",\n",
    "        \"한계나 주의사항이 언급되었는가?\",\n",
    "    ]\n",
    "    for q in sample_questions:\n",
    "        print(f\"\\nQ: {q}\")\n",
    "        ans = rag.invoke(q)\n",
    "        print(f\"A: {ans}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298db6c-d612-4d02-9019-acaefa948245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
