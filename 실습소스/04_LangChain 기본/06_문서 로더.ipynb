{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d80f0d-8e13-4ce9-9fd0-624cf3e9e510",
   "metadata": {},
   "source": [
    "# 문서 로더(document Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a536a1-f554-4204-b653-3a53b7eeab34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일의 내용 불러오기\n",
    "load_dotenv(\"C:/env/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e93a6-cd39-41a9-b515-42e1d893c699",
   "metadata": {},
   "source": [
    "### Document 객체\n",
    ": LangChain에서 Document 객체는 모든 데이터의 기본 단위이며, <br>\n",
    "  Loader로 불러온 텍스트를 모델이 이해할 수 있는 표준 구조로 정리한 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90de8ae5-dcb2-471a-9664-f115656bd808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n",
      "page_content: 이 문서는 LangChain의 Document 객체 설명입니다.\n",
      "metadata: {'source': 'lecture_note', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "doc = Document(\n",
    "    page_content=\"이 문서는 LangChain의 Document 객체 설명입니다.\",\n",
    "    metadata={\"source\": \"lecture_note\", \"page\": 1}\n",
    ")\n",
    "print(type(doc))\n",
    "\n",
    "print('page_content:',doc.page_content)\n",
    "print('metadata:',doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3066307e-4266-4c57-a2d5-82df8614389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1장. LangChain 개요...' metadata={'source': 'data/guide.pdf', 'page': 1}\n",
      "page_content='2장. Document Loader의 종류...' metadata={'source': 'data/guide.pdf', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "docs = \\\n",
    "[\n",
    "  Document(\n",
    "    page_content=\"1장. LangChain 개요...\",\n",
    "    metadata={\"source\": \"data/guide.pdf\", \"page\": 1}\n",
    "  ),\n",
    "  Document(\n",
    "    page_content=\"2장. Document Loader의 종류...\",\n",
    "    metadata={\"source\": \"data/guide.pdf\", \"page\": 2}\n",
    "  ),\n",
    "]\n",
    "print(docs[0])\n",
    "print(docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998993df-e7f1-40bd-b4d1-1f78beec8d74",
   "metadata": {},
   "source": [
    "## 문서 로더(Document Loader)\n",
    ": LangChain의 Document Loader는 외부 문서(파일, 웹, 데이터베이스 등)를 읽어서  <br>\n",
    "LangChain이 처리할 수 있는 Document 객체(list[Document]) 형태로 변환하는 구성요소이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01027b29-6400-4f93-966a-aed562f4fa88",
   "metadata": {},
   "source": [
    "###  [1] TextLoader\n",
    ": TextLoader 는 LangChain에서 가장 기본적인 문서 로더(Document Loader) 로, <br>\n",
    "일반 텍스트 파일(.txt)을 읽어들여 Document 객체로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d0d97ea-1e49-491a-97d2-c5ab162b0625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample.txt\n",
    "이 문서는 LangChain TextLoader 예시입니다.\n",
    "여러 줄의 텍스트를 포함합니다.\n",
    "TextLoader 는 LangChain에서 가장 기본적인 Document Loader이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "456fba62-ef46-4015-9db1-0c90edf35d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n",
      "page_content='이 문서는 LangChain의 Document 객체 설명입니다.' metadata={'source': 'lecture_note', 'page': 1}\n",
      "총 4 개의 청크가 저장되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sample.txt'}, page_content='이 문서는 LangChain TextLoader 예시입니다.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='여러 줄의 텍스트를 포함합니다.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='TextLoader 는 LangChain에서 가장 기본적인 Document'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Document Loader이다')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 1) 텍스트 로드\n",
    "loader = TextLoader(\"sample.txt\",encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(type(doc))  # Document 객체\n",
    "print(doc)\n",
    "\n",
    "# 2) 청크 단위로 나누기\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=50,chunk_overlap=10)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# 3) 임베딩 생성 및 벡터 저장\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = FAISS.from_documents(split_docs,embeddings)\n",
    "\n",
    "print(\"총\", len(split_docs), \"개의 청크가 저장되었습니다.\")\n",
    "split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458f34b-fa21-4030-9b49-d032e4c9b5f7",
   "metadata": {},
   "source": [
    "###  [2] DirectoryLoader\n",
    ": DirectoryLoader는 폴더(디렉터리) 안의 여러 파일을 자동으로 탐색하여 <br>\n",
    "각 파일을 개별 Document 객체로 읽어들이는 문서 로더(Document Loader) 이다. <br>\n",
    "이 클래스는 실제 프로젝트에서 대규모 문서 일괄 처리 시 거의 항상 사용되는 핵심 도구이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4240ca5-64bc-4aa6-bef0-dc5d8fe2496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85dfae6c-4128-443c-be2b-920c6c1d2c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sample1.txt 파일 생성 완료\n",
      " sample2.txt 파일 생성 완료\n",
      " sample3.txt 파일 생성 완료\n",
      "\n",
      "📂 'data' 폴더에 3개의 샘플 텍스트 파일이 준비되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. data 폴더 생성 (이미 존재하면 무시)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# 2. 샘플 텍스트 데이터\n",
    "samples = {\n",
    "    \"sample1.txt\": \"\"\"LangChain은 LLM(대규모 언어모델)을 활용한 파이프라인 구축을 위한 오픈소스 프레임워크입니다.\n",
    "Document Loader를 통해 다양한 외부 데이터를 로드하고,\n",
    "Text Splitter로 문서를 나눈 후, Embedding 및 VectorStore를 이용해 RAG 시스템을 구성할 수 있습니다.\"\"\",\n",
    "\n",
    "    \"sample2.txt\": \"\"\"FAISS는 Facebook AI Research에서 개발한 벡터 검색 라이브러리입니다.\n",
    "LangChain에서는 문서 임베딩을 벡터로 변환한 후,\n",
    "FAISS를 이용해 빠른 유사도 검색을 수행합니다.\"\"\",\n",
    "\n",
    "    \"sample3.txt\": \"\"\"pgvector는 PostgreSQL 데이터베이스에서 벡터 데이터를 저장하고 검색할 수 있게 해주는 확장 모듈입니다.\n",
    "LangChain은 pgvector를 통해 RAG 시스템을 SQL 기반 환경에서도 구현할 수 있습니다.\"\"\"\n",
    "}\n",
    "\n",
    "# 3. 파일 생성 및 저장\n",
    "for filename, content in samples.items():\n",
    "    path = os.path.join(\"data\", filename)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(f\" {filename} 파일 생성 완료\")\n",
    "\n",
    "print(\"\\n📂 'data' 폴더에 3개의 샘플 텍스트 파일이 준비되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "975b3cd0-57c9-42ba-9b8a-0880d4f8d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████▌                                                           | 3/6 [00:00<00:00, 431.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 개의 문서를 불러왔습니다.\n",
      "\n",
      "문서 1:\n",
      " page_content='LangChain은 LLM(대규모 언어모델)을 활용한 파이프라인 구축을 위한 오픈소스 프레임워크입니다.\n",
      "Document Loader를 통해 다양한 외부 데이터를 로드하고,\n",
      "Text Splitter로 문서를 나눈 후, Embedding 및 VectorStore를 이용해 RAG 시스템을 구성할 수 있습니다.' metadata={'source': 'data\\\\sample1.txt'}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "문서 2:\n",
      " page_content='FAISS는 Facebook AI Research에서 개발한 벡터 검색 라이브러리입니다.\n",
      "LangChain에서는 문서 임베딩을 벡터로 변환한 후,\n",
      "FAISS를 이용해 빠른 유사도 검색을 수행합니다.' metadata={'source': 'data\\\\sample2.txt'}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "문서 3:\n",
      " page_content='pgvector는 PostgreSQL 데이터베이스에서 벡터 데이터를 저장하고 검색할 수 있게 해주는 확장 모듈입니다.\n",
      "LangChain은 pgvector를 통해 RAG 시스템을 SQL 기반 환경에서도 구현할 수 있습니다.' metadata={'source': 'data\\\\sample3.txt'}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"data/\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=lambda path : TextLoader(path,autodetect_encoding=True), #  자동 인코딩 감지\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(len(docs), \"개의 문서를 불러왔습니다.\\n\")\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(f'문서 {i+1}:\\n', doc)\n",
    "    print('-'*140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4302710-1bbe-40fc-8a8c-c15b3c28a542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\sample1.txt'}, page_content='LangChain은 LLM(대규모 언어모델)을 활용한 파이프라인 구축을 위한 오픈소스 프레임워크입니다.\\nDocument Loader를 통해 다양한 외부 데이터를 로드하고,\\nText Splitter로 문서를 나눈 후, Embedding 및 VectorStore를 이용해 RAG 시스템을 구성할 수 있습니다.'),\n",
       " Document(metadata={'source': 'data\\\\sample2.txt'}, page_content='FAISS는 Facebook AI Research에서 개발한 벡터 검색 라이브러리입니다.\\nLangChain에서는 문서 임베딩을 벡터로 변환한 후,\\nFAISS를 이용해 빠른 유사도 검색을 수행합니다.'),\n",
       " Document(metadata={'source': 'data\\\\sample3.txt'}, page_content='pgvector는 PostgreSQL 데이터베이스에서 벡터 데이터를 저장하고 검색할 수 있게 해주는 확장 모듈입니다.\\nLangChain은 pgvector를 통해 RAG 시스템을 SQL 기반 환경에서도 구현할 수 있습니다.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs  # 리스트로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb9fe3-b89a-4acb-b084-51808df2a778",
   "metadata": {},
   "source": [
    "### [3] PDF Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811d5ba-48cb-4aa4-9cb8-2ba80e0a23c8",
   "metadata": {},
   "source": [
    "####  (1) PyPDFLoader\n",
    ": PyPDFLoader는 PDF 파일을 텍스트로 변환하여 Document 객체로 만드는 로더입니다.<br>\n",
    "내부적으로 pypdf (이전 명칭 PyPDF2) 라이브러리를 사용하며,\n",
    "PDF의 각 페이지를 개별 Document 객체로 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bc5b297-0527-4977-a29d-76f5e5a5df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dd37326-5c42-4b3f-9d10-aade9317df17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 16 페이지 로드됨\n",
      "강화학습을 이용한 하천 녹조 발생 저감 모형 연구   47한국빅데이터학회지\n",
      "제10권 제1호, 2025, pp. 47-62 https://doi.org/10.36498/kbigdt.2025.10.1.47\n",
      "유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지: \n",
      "사용자 생성 콘텐츠에서의 극단성과 허위성 분류 \n",
      "Decoding Review Anomalies: Classifying Extremity and Falsity in \n",
      "User-Generated Content\n",
      "가중정1⋅김엘레나2⋅최재원3†\n",
      "순천향대학교 경영학과1, 순천향대학교 경영학과2, 순천향대학교 경영학과3\n",
      "요  약\n",
      "본 연구는 YouTube 플랫폼의 호텔 리뷰를 대상으로 머신러닝과 자연어처리(NLP) 기법을 활용해 극단적 \n",
      "및 조작된 리뷰를 식별⋅필터링하고자 한다. 소셜 미디어는 소비자 구매 결정에 중요한 영향을 미치며, \n",
      "사용자 생성 리뷰는 마켓플레이스 신뢰도의 핵심 요소로 작용한다. 그러나 일부 판매자들의 평점 조작과 \n",
      "조작된 리\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"총\", len(docs), \"페이지 로드됨\")\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b98f0e3b-246a-4a3c-b91d-22450292ab2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 78 개의 문서(페이지) 로드 완료\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"./\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"총\", len(docs), \"개의 문서(페이지) 로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e4951ac-cbd1-4a38-9ece-2e071e04b717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 문서가 벡터 DB로 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 1) PDF 로드\n",
    "loader = PyPDFLoader(\"유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2) 문서 분할\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# 3) 임베딩 및 벡터 저장\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "print(\"PDF 문서가 벡터 DB로 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90c65a7b-6792-4c35-acfa-95033d506865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c027f3f-9afb-4b0f-80d3-12848e069045",
   "metadata": {},
   "source": [
    " #### (2) PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "115ae16b-ba3f-4810-ab53-1d89555d15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61861804-f22c-4814-a0a7-ba1068548dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 16 페이지 로드됨\n",
      "강화학습을 이용한 하천 녹조 발생 저감 모형 연구   47\n",
      "한국빅데이터학회지\n",
      "제10권 제1호, 2025, pp. 47-62\n",
      "https://doi.org/10.36498/kbigdt.2025.10.1.47\n",
      "유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지: \n",
      "사용자 생성 콘텐츠에서의 극단성과 허위성 분류 \n",
      "Decoding Review Anomalies: Classifying Extremity and Falsity in \n",
      "User-Generated Content\n",
      "가중정1⋅김엘레나2⋅최재원3†\n",
      "순천향대학교 경영학과1, 순천향대학교 경영학과2, 순천향대학교 경영학과3\n",
      "요  약\n",
      "본 연구는 YouTube 플랫폼의 호텔 리뷰를 대상으로 머신러닝과 자연어처리(NLP) 기법을 활용해 극단적 \n",
      "및 조작된 리뷰를 식별⋅필터링하고자 한다. 소셜 미디어는 소비자 구매 결정에 중요한 영향을 미치며, \n",
      "사용자 생성 리뷰는 마켓플레이스 신뢰도의 핵심 요소로 작용한다. 그러나 일부 판매자들의 평점 조작과 \n",
      "조작된 \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# PyMuPDF 로더 인스턴스 생성\n",
    "loader = PyMuPDFLoader(\"유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지.pdf\")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"총\", len(docs), \"페이지 로드됨\")\n",
    "\n",
    "# 문서의 내용 출력\n",
    "print(docs[10].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab3e11b-ac7c-4967-857a-b89a281008ba",
   "metadata": {},
   "source": [
    "#### (4) PDFPlumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0115678c-43a1-433f-b4ce-0d89e0d34358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61eab5b0-cfaf-4087-99fb-c187ea2b5ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 16 페이지 로드됨\n",
      "유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지 57\n",
      "추출된 토픽 주제의 해석력을 높이기 위해, 동시출현 네트워크는 사용자 리뷰의 다차원\n",
      "본 연구는 동시출현 점수(co-occurrence score)를 적인 토픽 구조를 보여준다. 다양한 색상의 토\n",
      "계산하여 각 토픽의 의미적 일관성을 평가하였 픽 노드는 여행 및 숙박 경험, 지역 리뷰, 동영상\n",
      "다. 이 점수는 특정 주제 내에서 키워드들이 동 콘텐츠 피드백 등과 같은 다양한 의미적 시나리\n",
      "일한 문서 내에 얼마나 자주 함께 등장하는지를 오를 다루며, 고빈도 키워드는 여러 토픽을 동\n",
      "수치화한 것으로, 주제어 간의 상호 연관성을 시 발생 관계를 통해 서로 연결하여 리뷰 콘텐\n",
      "측정하는 지표로 활용된다. 수치화한 것으로, 주 츠의 다양성과 집중도를 보여준다. <그림 2>는\n",
      "제어 간의 상호 연관성을 측정하는 지표로 활용 시각화 된 동시출현 네트워크를 나타낸다.\n",
      "된다. 동시출현 점수가 높을수록 키워드들이 밀 Word Cloud 시각화는 사용 시나리\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "# PDF 문서 로더 인스턴스 생성\n",
    "loader = PDFPlumberLoader(\"유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지.pdf\")\n",
    "\n",
    "# 문서 로딩\n",
    "docs = loader.load()\n",
    "print(\"총\", len(docs), \"페이지 로드됨\")\n",
    "\n",
    "# 첫 번째 문서 데이터 접근\n",
    "print(docs[10].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9d24e8-0713-432c-a1c1-b5356db91973",
   "metadata": {},
   "source": [
    "#### (5) PyPDFium2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d25aece7-2d13-4f42-888f-81d29416b468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 16 페이지 로드됨\n",
      "유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지 57\n",
      "추출된 토픽 주제의 해석력을 높이기 위해, \n",
      "본 연구는 동시출현 점수(co-occurrence score)를 \n",
      "계산하여 각 토픽의 의미적 일관성을 평가하였\n",
      "다. 이 점수는 특정 주제 내에서 키워드들이 동\n",
      "일한 문서 내에 얼마나 자주 함께 등장하는지를 \n",
      "수치화한 것으로, 주제어 간의 상호 연관성을 \n",
      "측정하는 지표로 활용된다. 수치화한 것으로, 주\n",
      "제어 간의 상호 연관성을 측정하는 지표로 활용\n",
      "된다. 동시출현 점수가 높을수록 키워드들이 밀\n",
      "접하게 연결되어 있는 일관된 주제를 형성하며, \n",
      "예를 들어 ‘호텔 시설’, ‘청결’, ‘침대’, ‘조식’ 등\n",
      "이 함께 자주 등장하는 경우, 해당 리뷰는 특정 \n",
      "호텔의 물리적 조건에 집중된 논의임을 시사한\n",
      "다. 반면, 동시출현 점수가 낮은 토픽은 단어 간 \n",
      "맥락적 연결성이 약하거나, 리뷰 작성자들이 다\n",
      "양한 맥락에서 단어를 더 자유롭게 사용하는 경\n",
      "향이 있음을 나타낸다. 이는 주제가 분산되어 \n",
      "있거나\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "# PyPDFium2 로더 인스턴스 생성\n",
    "loader = PyPDFium2Loader(\"유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지.pdf\")\n",
    "\n",
    "# 데이터 로드\n",
    "docs = loader.load()\n",
    "print(\"총\", len(docs), \"페이지 로드됨\")\n",
    "\n",
    "# 문서의 내용 출력\n",
    "print(docs[10].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8cb2e9-25a1-4cd3-8685-1a21a8b4157f",
   "metadata": {},
   "source": [
    "### [4] CSVLoader 와 DataFrameLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e796e61-1777-4aa8-87dc-3e2223b4c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n",
      "{'source': 'WHO_first9cols.csv', 'row': 0}\n",
      "page_content='Country: Afghanistan\n",
      "CountryID: 1\n",
      "Continent: 1\n",
      "Adolescent fertility rate (%): 151\n",
      "Adult literacy rate (%): 28\n",
      "Gross national income per capita (PPP international $): \n",
      "Net primary school enrolment ratio female (%): \n",
      "Net primary school enrolment ratio male (%): \n",
      "Population (in thousands) total: 26088' metadata={'source': 'WHO_first9cols.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# CSV 로더 생성\n",
    "loader = CSVLoader(file_path= \"WHO_first9cols.csv\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "print(docs[0].metadata)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c7a9a3e-2260-4409-980c-6f9a296d0a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>CountryID</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Adolescent fertility rate (%)</th>\n",
       "      <th>Adult literacy rate (%)</th>\n",
       "      <th>Gross national income per capita (PPP international $)</th>\n",
       "      <th>Net primary school enrolment ratio female (%)</th>\n",
       "      <th>Net primary school enrolment ratio male (%)</th>\n",
       "      <th>Population (in thousands) total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>151.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>98.7</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>3172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>69.9</td>\n",
       "      <td>5940.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>33351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angola</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>146.0</td>\n",
       "      <td>67.4</td>\n",
       "      <td>3890.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>16557.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>198</td>\n",
       "      <td>6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>90.3</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>86206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>West Bank and Gaza</td>\n",
       "      <td>199</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Yemen</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>54.1</td>\n",
       "      <td>2090.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>21732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>201</td>\n",
       "      <td>3</td>\n",
       "      <td>161.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>11696.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>202</td>\n",
       "      <td>3</td>\n",
       "      <td>101.0</td>\n",
       "      <td>89.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>13228.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Country  CountryID  Continent  Adolescent fertility rate (%)  \\\n",
       "0           Afghanistan          1          1                          151.0   \n",
       "1               Albania          2          2                           27.0   \n",
       "2               Algeria          3          3                            6.0   \n",
       "3               Andorra          4          2                            NaN   \n",
       "4                Angola          5          3                          146.0   \n",
       "..                  ...        ...        ...                            ...   \n",
       "197             Vietnam        198          6                           25.0   \n",
       "198  West Bank and Gaza        199          1                            NaN   \n",
       "199               Yemen        200          1                           83.0   \n",
       "200              Zambia        201          3                          161.0   \n",
       "201            Zimbabwe        202          3                          101.0   \n",
       "\n",
       "     Adult literacy rate (%)  \\\n",
       "0                       28.0   \n",
       "1                       98.7   \n",
       "2                       69.9   \n",
       "3                        NaN   \n",
       "4                       67.4   \n",
       "..                       ...   \n",
       "197                     90.3   \n",
       "198                      NaN   \n",
       "199                     54.1   \n",
       "200                     68.0   \n",
       "201                     89.5   \n",
       "\n",
       "     Gross national income per capita (PPP international $)  \\\n",
       "0                                                  NaN        \n",
       "1                                               6000.0        \n",
       "2                                               5940.0        \n",
       "3                                                  NaN        \n",
       "4                                               3890.0        \n",
       "..                                                 ...        \n",
       "197                                             2310.0        \n",
       "198                                                NaN        \n",
       "199                                             2090.0        \n",
       "200                                             1140.0        \n",
       "201                                                NaN        \n",
       "\n",
       "     Net primary school enrolment ratio female (%)  \\\n",
       "0                                              NaN   \n",
       "1                                             93.0   \n",
       "2                                             94.0   \n",
       "3                                             83.0   \n",
       "4                                             49.0   \n",
       "..                                             ...   \n",
       "197                                           91.0   \n",
       "198                                            NaN   \n",
       "199                                           65.0   \n",
       "200                                           94.0   \n",
       "201                                           88.0   \n",
       "\n",
       "     Net primary school enrolment ratio male (%)  \\\n",
       "0                                            NaN   \n",
       "1                                           94.0   \n",
       "2                                           96.0   \n",
       "3                                           83.0   \n",
       "4                                           51.0   \n",
       "..                                           ...   \n",
       "197                                         96.0   \n",
       "198                                          NaN   \n",
       "199                                         85.0   \n",
       "200                                         90.0   \n",
       "201                                         87.0   \n",
       "\n",
       "     Population (in thousands) total  \n",
       "0                            26088.0  \n",
       "1                             3172.0  \n",
       "2                            33351.0  \n",
       "3                               74.0  \n",
       "4                            16557.0  \n",
       "..                               ...  \n",
       "197                          86206.0  \n",
       "198                              NaN  \n",
       "199                          21732.0  \n",
       "200                          11696.0  \n",
       "201                          13228.0  \n",
       "\n",
       "[202 rows x 9 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(\"WHO_first9cols.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68ecaec5-6bf2-4b50-9203-546642215e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afghanistan\n",
      "{'CountryID': 1, 'Continent': 1, 'Adolescent fertility rate (%)': 151.0, 'Adult literacy rate (%)': 28.0, 'Gross national income per capita (PPP international $)': nan, 'Net primary school enrolment ratio female (%)': nan, 'Net primary school enrolment ratio male (%)': nan, 'Population (in thousands) total': 26088.0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "# 데이터 프레임 로더 설정, 페이지 내용 컬럼 지정\n",
    "loader = DataFrameLoader(df, page_content_column=\"Country\")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 데이터 출력\n",
    "print(docs[0].page_content)\n",
    "\n",
    "# 메타데이터 출력\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501cdeb7-5dfe-44ae-a1a4-e01ceed79e85",
   "metadata": {},
   "source": [
    "### [5] JSONLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b08b623-2222-4c92-bc6f-9159be795be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting employee.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile employee.json\n",
    "[\n",
    "    {\n",
    "        \"name\": \"홍길동\",\n",
    "        \"age\": 30,\n",
    "        \"address\": \"서울특별시 강남구\",\n",
    "        \"phoneNumbers\": [\n",
    "            {\n",
    "                \"type\": \"mobile\",\n",
    "                \"number\": \"010-1234-5678\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"home\",\n",
    "                \"number\": \"02-987-6543\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"김철수\",\n",
    "        \"age\": 25,\n",
    "        \"address\": \"부산광역시 해운대구\",\n",
    "        \"phoneNumbers\": [\n",
    "            {\n",
    "                \"type\": \"mobile\",\n",
    "                \"number\": \"010-1111-2222\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"이영희\",\n",
    "        \"age\": 28,\n",
    "        \"address\": \"대구광역시 수성구\",\n",
    "        \"phoneNumbers\": [\n",
    "            {\n",
    "                \"type\": \"mobile\",\n",
    "                \"number\": \"010-3333-4444\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"work\",\n",
    "                \"number\": \"053-555-6666\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97f38eb9-1b17-4989-9a82-aa72d9f6c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb6c4752-836d-409c-9249-f7f0a08b08b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\storm\\\\바탕 화면\\\\AI_Agent_Lab_PM\\\\04_LangChain 기본\\\\employee.json', 'seq_num': 1}, page_content='[{\"type\": \"mobile\", \"number\": \"010-1234-5678\"}, {\"type\": \"home\", \"number\": \"02-987-6543\"}]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\storm\\\\바탕 화면\\\\AI_Agent_Lab_PM\\\\04_LangChain 기본\\\\employee.json', 'seq_num': 2}, page_content='[{\"type\": \"mobile\", \"number\": \"010-1111-2222\"}]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\storm\\\\바탕 화면\\\\AI_Agent_Lab_PM\\\\04_LangChain 기본\\\\employee.json', 'seq_num': 3}, page_content='[{\"type\": \"mobile\", \"number\": \"010-3333-4444\"}, {\"type\": \"work\", \"number\": \"053-555-6666\"}]')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# JSONLoader 생성\n",
    "loader = JSONLoader(\n",
    "    file_path=\"employee.json\",\n",
    "    jq_schema=\".[].phoneNumbers\",\n",
    "    text_content=False,\n",
    ")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 결과 출력\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa8aa0-c4cb-44e8-8c98-1d3d267230c7",
   "metadata": {},
   "source": [
    "### [6] WebBaseLoader\n",
    ": URL(웹 페이지 주소) 를 입력받아,\n",
    "그 웹 페이지의 본문 텍스트를 추출하여 LangChain의 Document 객체로 변환해주는 로더입니다. <br>\n",
    "실제 서비스형 RAG나 뉴스 크롤링 실습할 때 가장 자주 쓰는 웹 문서 로더입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b3cd44-92da-45f2-8cbb-697ac70566e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 개의 문서 로드됨\n",
      "[Document(metadata={'source': 'https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/', 'title': 'LangChain & LangGraph 1.0 alpha releases', 'language': 'en'}, page_content='\\n\\n\\nLangChain & LangGraph 1.0 alpha releases\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain & LangGraph 1.0 alpha releases\\n\\n3 min read\\nSep 2, 2025\\n\\n\\n\\n\\n\\nToday we are announcing alpha releases of v1.0 for langgraph  and langchain, in both Python and JS. LangGraph is a low-level agent orchestration framework, giving developers durable execution and fine-grained control to run complex agentic systems in production. LangChain helps developers ship AI features fast with standardized model abstractions and prebuilt agent patterns, making it easy to build complex applications without vendor lock-in. We are working towards an official 1.0 release in late October - please give us any feedback here!LangGraphlanggraph has been battle tested as companies like Uber, LinkedIn, and Klarna use it in production. We are promoting it to 1.0 with no breaking changes. It comes with a built in agent runtime (durable execution, short term memory, human in the loop patterns, streaming), and be used to build arbitrary workflows and agentic patterns.LangChainlangchain has always contained high level interfaces for getting started with different agent patterns as easily as possible. Early on, there were a handful of these patterns (these made up all the chains and agents originally in langchain). Over the past two years, we have realized that:A number of use cases need completely custom patterns. For these - we recommend langgraph and building your ownThe rest have consolidated around a particular implementation of an “agent”This \"agent\" abstraction is largely:Give an LLM access to some toolsCall it with some inputIf it calls a tool:Execute that toolReturn to step 2, adding back in the tool call and tool resultIf it doesn\\'t call a tool: finishWe\\'ve had this abstraction in LangChain from the early days - in November of 2022. It\\'s evolved over the years as things like tool calling have emerged to make this easier.In LangChain 1.0 we are focusing the langchain package centered around this abstraction, and are introducing a new create_agent implementation - same high level interface, different underpinning. We built this implementation on top of langgraph to take advantage of the underlying agent runtime. While this is new to the langchain package, it\\'s not new to the LangChain ecosystem. This has been battle tested over the past year as part of langgraph.prebuilts. You can try this out easily with:Python: from langchain.agents import create_agentJS: import { createAgent } from \"langchain\"If you are using existing langchain chains and agents - don\\'t worry. We will be releasing a langchain-legacy package, allowing developers to continue using these old chains and agents, while also updating the new and improved langchain 1.0 should they choose.LangChain CoreA key part of langchain that is staying the same are the integration abstractions. LangChain contains 1000s of integrations with providers like OpenAI, Anthropic, etc. These abstractions technically live in langchain-core - a base package we created for the sole purpose of containing these abstractions.We are promoting langchain-core to 1.0 with no breaking changes, but with a core addition. A big part of langchain-core is the concept of “messages”, how we communicate with LLM apis. In 1.0, we are introducing more structure around how these messages are formatted (in a backwards compatible way). A big value prop of LangChain has always been standard ways to interact with LLMs. The ways to interact with LLMs has changed over time - it started as strings, then went to messages (where the content of each message was a string). Now, however, LLM APIs are returning lists of content blocks. As such, we are introducing a new .content_blocks property which has standard content types. You can read all about content blocks here.DocumentationFinally - we are also launching a new docs site for all these open source projects. These docs centralize our open source docs in one place, and also provide one unified page for both Python and Javascript. We\\'ve heard you ask for a more centralized and easy to follow documentation site. This has been - and will continue to be - a big focus of ours.Try it outWe\\'re excited to announce the 1.0 alphas today. You can try them out easily:JavaScriptLangChain: npm install langchain@nextLangGraph: npm install @langchain/langgraph@alphaPythonLangChain: pip install langchain==1.0.0a3LangGraph: pip install langgraph==1.0.0a1Again - please give us feedback on these 1.0 releases (and docs) here. You can also find GitHub discussion items (LangChain, LangGraph). This is a big milestone - we are excited to work on this with the community over the next two months.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2025\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# 로더 생성\n",
    "loader = WebBaseLoader(\"https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/\")\n",
    "\n",
    "# 웹 페이지 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 결과 확인\n",
    "print(len(docs), \"개의 문서 로드됨\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23d24831-7374-4193-a39a-dfdd754070f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 개의 문서 로드됨\n",
      "[Document(metadata={'source': 'https://blog.langchain.dev/customers-replit/'}, page_content='\\nReplit is at the forefront of AI innovation with its platform that simplifies writing, running, and collaborating on code for over 30+ million developers. They recently released Replit Agent, which immediately went viral due to the incredible applications people could easily create with this tool.Behind the scenes, Replit Agent has a complex workflow which enables a highly custom agentic workflow with a high-degree of control and parallel execution. By using LangSmith, Replit gained deep visibility into their agent interactions to debug tricky issues.\\xa0The level of complexity required for Replit Agent also pushed the boundaries of LangSmith. The LangChain and Replit teams worked closely together to add functionality to LangSmith that would satisfy their LLM observability needs. Specifically, there were three main areas that we innovated on:Improved performance and scale on large tracesAbility to search and filter within tracesThread view to enable human-in-the loop workflowsImproved performance and scale on large tracesMost other LLMOps solutions monitor individual API requests to LLM providers, offering a limited view of single LLM calls. In contrast, LangSmith from day one has focused on tracing the entire execution flow of an LLM application to provide a more holistic context.\\xa0Tracing is important for agents due to their complex nature. It captures multiple LLM calls as well as other steps (retrieval, running code, etc). This gives you granular visibility into what’s happening, including at the inputs and outputs of each step, in order to understand the agent’s decision-making.\\xa0Replit Agent was a ripe example for advanced tracing needs. Their agentic tool goes beyond simply reviewing and writing code, but also performs a wider range of functions – including planning, creating dev environments, installing dependencies, and deploying applications for users.\\xa0As a result, Replit’s traces were very large - involving hundreds of steps. This posed significant challenges for ingesting data and displaying it in a visually meaningful way.To address this, the LangChain team improved their ingestion to efficiently process and store large volumes of trace data. They also improved LangSmith’s frontend rendering to display long-running agent traces seamlessly.Search and filter within traces to pinpoint issuesLangSmith has always supported search between traces, which allows users to find a single trace among hundreds of thousands based on events or full text search. But as Replit Agent’s traces got longer and longer, the Replit team needed to search within traces for specific events (oftentimes issues reported by alpha testers). This required augmenting existing search capabilities.In response, a new search pattern – searching within traces – was added to LangSmith. Instead of sifting and scrolling call-by-call within a large trace, users could now filter directly on a criteria they cared about (e.g. keywords in the inputs or outputs of a run). This greatly reduced Replit’s time needed to debug agent steps within a trace.Thread view to enable human-in-the-loop workflowsA key differentiator of Replit Agent was its emphasis on human-in-the-loop workflows. Replit Agent intends to be a tool where AI agents can collaborate effectively with human developers, who can come in and edit and correct agent trajectories as needed.With separate agents to perform roles like managing, editing, and verifying generated code,\\xa0 Replit’s agents interacted with users continuously - often over long periods with multiple turns of conversation. However, monitoring these conversational flows was often difficult, as each user session would generate disjoint traces.\\xa0To solve this, LangSmith’s thread view helped collate traces from multiple threads together that were related (i.e. from one conversation). This provided a logical view of all agent-user interactions across a multi-turn conversation, helping Replit better 1) find bottlenecks where users got stuck and 2) pinpoint areas where human intervention could be beneficial.\\xa0ConclusionReplit is pushing the frontier of AI agent monitoring using LangSmith’s powerful observability features. By reducing the effort of loading long, heavy traces, the Replit team has greatly sped up the process of building and scaling complex agents. With faster debugging, improved trace visibility, and better handling of parallel tasks, Replit is setting the standard for AI-driven development.\\t\\n'), Document(metadata={'source': 'https://blog.langchain.dev/langgraph-v0-2/'}, page_content=\"\\nToday, we’re excited to announce the stable release of LangGraph v0.2, which introduces a new ecosystem of LangGraph checkpointer libraries. These simplify the creation and customization of checkpointers, which allows users to build more resilient LLM applications with smooth session memory, robust error recovery, and human-in-the-loop features.Why we built LangGraph v0.2One of the key pillars of LangGraph is its built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at each step, enabling several powerful capabilities, including:Session memory: Store history (checkpoints) of user interactions and resume from a saved checkpoint in follow up interactionsError recovery: Recover from failures at any given step in the graph execution by continuing from the last successful step checkpointHuman-in-the-loop: Implement tool approval, wait for human input, edit agent actions and moreTime travel: Edit graph state at any point in the history of execution and create an alternative execution from that point in time (i.e. fork the thread)Since the early days of LangGraph, we’ve designed checkpointing to be database-agnostic, allowing users to implement their own checkpointer adapters for their database of choice.Since the LangGraph v0.1 release, we've seen a lot of interest from the community in creating checkpointers for many popular databases like Postgres, Redis, and MongoDB. However, there was no clear blueprint for the users to implement their own, custom checkpointers.New checkpointer libraries in LangGraphWith LangGraph v0.2, we’re making it easier to create new checkpointers. We’ve also laid the foundation to foster a community-maintained ecosystem of checkpointer implementations.We now have a suite of new, dedicated checkpointer libraries:langgraph_checkpoint : The base interface for checkpointer savers (BaseCheckpointSaver ) and serialization/deserialization interface (SerializationProtocol). Includes in-memory checkpointer implementation (MemorySaver) for experimentation.langgraph_checkpoint_sqlite : An implementation of LangGraph checkpointer that uses SQLite database. Ideal for experimentation and local workflows.langgraph_checkpoint_postgres : Our advanced checkpointer that we wrote and optimized for Postgres in LangGraph Cloud is now open-sourced and available to the community to build upon. Ideal for using in production.Checkpointer implementations can be used interchangeably, which lets users tailor their stateful LangGraph applications to their custom needs.LangGraph Postgres Checkpointer for production-ready appslanggraph_checkpoint_postgres implementation can serve as a blueprint for community members to implement their own optimized, production-ready checkpointers for their favorite database. Postgres checkpointer implements a number of optimizations both on the write-, as well as read-side.Write-side optimizations:We're making use of Postgres pipeline mode to reduce database roundtripsWe're storing each channel value separately and versioned so that each new checkpoint only stores the values that changed.Read-side optimizations:We're making use of a cursor for the list endpoint in order to efficiently fetch long thread histories when needed.Getting started on LangGraph v0.2Since LangGraph checkpointer libraries are implemented as namespace packages, you can import checkpointer interfaces and implementations the same way as before, using:from langgraph.checkpoint.base import BaseCheckpointSaverfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.checkpoint.sqlite import SqliteSaverfrom langgraph.checkpoint.postgres import PostgresSaverSince SQLite and Postgres checkpointers are provided via separate libraries, you will need to install them using pip install langgraph-checkpoint-sqlite or pip install langgraph-checkpoint-postgres.LangGraph checkpoint libraries will follow semantic versioning (starting with current release of 1.0), and any breaking changes in the main library will result in a major version bump for those libraries. For example, the next breaking change in langgraph_checkpoint will result in 2.0 version, and you can expect the checkpointer implementations (e.g.,langgraph_checkpoint_sqlite) to also be updated to 2.0 to follow that change.To get started, follow this guide on how to use checkpointers in LangGraph. You can also check out our documentation, including a reference and overview of checkpointers. Run agents at scale with LangGraph CloudTo complement the LangGraph framework, we also have a new runtime, LangGraph Cloud, which provides infrastructure purpose-built for deploying agents at scale.LangGraph Cloud does the heavy lifting for your agentic application, removing the maintenance work for custom checkpointers while adding fault-tolerant scalability. It gracefully manages horizontally-scaling task queues, servers, and includes our robust Postgres checkpointer out-of-the-box to help you handle many concurrent users and efficiently store large states and threads.In addition, LangGraph Cloud supports real-world interaction patterns beyond streaming and human-in-the-loop. These include double-texting to handle new user inputs on currently-running threads of the graph, async background jobs for long-running tasks, and cron jobs.Lastly, you can easily deploy your agentic app and collaborate in LangGraph Studio, a playground-like space for visualizing and debugging agent trajectories, with LangGraph Cloud. The LangGraph Studio desktop app is now also available for all LangSmith users to try for free.LangGraph Cloud is now available in beta for all LangSmith users on Plus or Enterprise plans. Try it out today for free by signing up for LangSmith.Additional changes in LangGraph v0.2LangGraph v0.2 contains many improvements, and we've designed it to be largely backwards compatible. Below is a list of breaking changes and deprecations in this latest version.Breaking changesLangGraph v0.2 introduces several breaking changes:thread_ts and parent_ts have been renamed to checkpoint_id and parent_checkpoint_id , respectively (via langgraph_checkpoint==1.0.0).Note: LangGraph checkpointers still recognize thread_ts if passed via config and treat it as checkpoint_idRe-exported imports like from langgraph.checkpoint import BaseCheckpointSaver are no longer possible due to the use of namespace packages. Instead, use from langgraph.checkpoint.base import BaseCheckpointSaverSQLite checkpointers have been moved to a separate library, so you’ll need to install them separately using pip install langgraph-checkpoint-sqliteDeprecationsIn LangGraph v0.2, we've removed:langgraph.prebuilt.chat_agent_executor.create_function_calling_executor . We recommend you use langgraph.prebuilt.chat_agent_executor.create_react_agent instead.langgraph.prebuilt.agent_executor . We recommend you use langgraph.prebuilt.chat_agent_executor.create_react_agent instead.ConclusionWe are incredibly grateful to our community and users for pushing us and building with LangGraph to improve agent reliability. We hope that with LangGraph v0.2, you’ll find it easier to build and maintain your own checkpointer implementations– and we’re excited to see all the apps that you create.As you try out LangGraph v0.2, we'd love to hear your feedback at hello@langchain.dev. You can also learn more from these additional resources:LangGraph docsLangGraph webpage (with FAQs)\\n\")]\n"
     ]
    }
   ],
   "source": [
    "# pip install beautifulsoup4\n",
    "import bs4  \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# 여러 개의 url 지정 가능\n",
    "url1 = \"https://blog.langchain.dev/customers-replit/\"\n",
    "url2 = \"https://blog.langchain.dev/langgraph-v0-2/\"\n",
    "\n",
    "# 로더 생성\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url1, url2),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"article-header\", \"article-content\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 웹 페이지 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 결과 확인\n",
    "print(len(docs), \"개의 문서 로드됨\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32d22dad-32b1-4293-9ad5-a217bf27256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.korea.kr/news/policyNewsView.do?newsId=148922373', 'title': '[카툰공감] 중소기업의 든든한 버팀목 ‘납품대금 연동제’를 소개합니다 - 정책뉴스 | 뉴스 | 대한민국 정책브리핑', 'description': '중소기업의 든든한 버팀목 납품대금 연동제를 소개합니다 - 정책브리핑 | 뉴스 | 정책뉴스', 'language': 'ko'}\n",
      "\n",
      "\n",
      "\n",
      "[카툰공감] 중소기업의 든든한 버팀목 ‘납품대금 연동제’를 소개합니다 - 정책뉴스 | 뉴스 | 대한민국 정책브리핑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "본문 바로가기\n",
      "메인메뉴 바로가기\n",
      "\n",
      "\n",
      "\n",
      "이 누리집은 대한민국 공식 전자정부 누리집입니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "대한민국 정책브리핑\n",
      "\n",
      "\n",
      "\n",
      "뉴스\n",
      "\n",
      "\n",
      "\n",
      "뉴스\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t정책뉴스\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t부처별 뉴스\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t정책포커스\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t국민이 말하는 정책\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t오피니언\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t키워드 뉴스\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t멀티미디어 뉴스\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "브리핑룸\n",
      "\n",
      "\n",
      "\n",
      "브리핑룸\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t보도자료\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t사실은 이렇습니다\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t브리핑 자료\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t연설문\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "url = \"https://www.korea.kr/news/policyNewsView.do?newsId=148922373\"\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[0].metadata)\n",
    "print(docs[0].page_content[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6b05bc2-b63a-44ac-bfd7-aff3f9b80b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 160 개의 청크가 벡터DB에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 1) 웹 문서 로드\n",
    "loader = WebBaseLoader(\"https://ko.wikipedia.org/wiki/인공지능\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2) 문서 분할\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# 3) 임베딩 + 벡터 저장\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "print(\"총\", len(split_docs), \"개의 청크가 벡터DB에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "640f3ecd-f8e7-43b3-9adc-62e5160491e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "인공지능 - 위키백과, 우리 모두의 백과사전\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "본문으로 이동\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "주 메뉴\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "주 메뉴\n",
      "사이드바로 이동\n",
      "숨기기\n",
      "\n",
      "\n",
      "\n",
      "\t\t둘러보기\n",
      "\t\n",
      "\n",
      "\n",
      "대문최근 바뀜요즘 화제임의의 문서로\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t사용자 모임\n",
      "\t\n",
      "\n",
      "\n",
      "사랑방사용자 모임관리 요청\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t편집 안내\n",
      "\t\n",
      "\n",
      "\n",
      "소개도움말정책과 지침질문방\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "검색\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "검색\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "보이기\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "기부\n",
      "\n",
      "계정 만들기\n",
      "\n",
      "로그인\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "개인 도구\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "기부 계정 만들기 로그인\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "목차\n",
      "사이드바로 이동\n",
      "숨기기\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "처음 위치\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "강인공지능과 약인공지능\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "강인공지능과 약인공지능 하위섹션 토글하기\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.1\n",
      "약인공지능\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.2\n",
      "강인공지능 (AGI)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.2.1\n",
      "강인공지능의 실현 가능성에 관한 논쟁\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "역사\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "역사 하위섹션 토글하기\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.1\n",
      "인공지능 이론의 발전\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.2\n",
      "인공지능의 탄생(1943-1956)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.2.1\n",
      "인공두뇌학과 초기 신경 네트워크\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.2.2\n",
      "튜링 테스트\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.2.3\n",
      "게임 인공지능\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.2.4\n",
      "상징 추론과 논리 이론\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.2.5\n",
      "다트머스 컨퍼런스 1956년: AI의 탄생\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.3\n",
      "황금기(1956~1974년)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.3.1\n",
      "작업들\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.3.1.1\n",
      "탐색 추리\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.3.1.2\n",
      "자연어 처리\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.3.1.3\n",
      "마이크로월드\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.3.2\n",
      "낙관론\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.3.3\n",
      "자금\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.4\n",
      "AI의 첫번째 암흑기(1974-1980)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.4.1\n",
      "문제\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.4.2\n",
      "자금 지원의 중단\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.4.3\n",
      "캠퍼스 전역의 비판들\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.4.4\n",
      "퍼셉트론과 연결망의 어두운 시대\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.4.5\n",
      "깔끔이 : 논리, 프롤로그와 전문가 시스템\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.4.6\n",
      "지저분이 : 프레임과 스크립트\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.5\n",
      "AI붐 (1980-1987)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.5.1\n",
      "전문가 시스템의 발전\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.5.2\n",
      "지식 혁명\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.5.3\n",
      "돈은 되돌아온다 : 5세대 프로젝트\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.5.4\n",
      "신경망 이론의 복귀\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.6\n",
      "AI의 두번째 암흑기 1987-1993\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.6.1\n",
      "인공지능의 겨울\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.6.2\n",
      "몸통을 갖는 것의 중요성: Nouvellle AI and embodied reason\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.7\n",
      "AI 1993-현재\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.7.1\n",
      "성공 사례와 무어의 법칙\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.7.2\n",
      "지능형 에이전트\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.7.3\n",
      "깔끔함의 승리\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.7.4\n",
      "조용한 발전\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.7.5\n",
      "HAL 9000은 어디에 있는가?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.7.6\n",
      "인공지능과 4차 산업혁명\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.8\n",
      "실험적인 AI 연구\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "인공지능 기술의 실용적인 응용\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "인공지능 기술의 실용적인 응용 하위섹션 토글하기\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.1\n",
      "인공지능의 이론적인 결과\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      "언어\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      "유명 인공지능\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "유명 인공지능 하위섹션 토글하기\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5.1\n",
      "지능적 기계\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5.2\n",
      "인공지능 연구가\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6\n",
      "미래\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "미래 하위섹션 토글하기\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6.1\n",
      "초지능\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6.2\n",
      "위험성\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6.2.1\n",
      "기술에 의한 실업\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7\n",
      "관련 서적\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8\n",
      "같이 보기\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9\n",
      "각주\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10\n",
      "외부 링크\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "목차 토글\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "인공지능\n",
      "\n",
      "\n",
      "\n",
      "173개 언어\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AfrikaansAlemannischአማርኛAragonésالعربيةالدارجةمصرىঅসমীয়াAsturianuAzərbaycancaتۆرکجهБашҡортсаBoarischŽemaitėškaBikol CentralБеларускаяБеларуская (тарашкевіца)BetawiБългарскиभोजपुरीပအိုဝ်ႏဘာႏသာႏবাংলাབོད་ཡིགBrezhonegBosanskiБуряадCatalàCebuanoکوردیQırımtatarcaČeštinaЧӑвашлаCymraegDanskDeutschZazakiKadazandusunΕλληνικάEnglishEsperantoEspañolEestiEuskaraEstremeñuفارسیSuomiVõroFɔ̀ngbèFrançaisNordfriiskFurlanGaeilge贛語Kriyòl gwiyannenGàidhligGalegoAvañe'ẽगोंयची कोंकणी / Gõychi KonknniGaelgHausaעבריתहिन्दीFiji HindiHrvatskiKreyòl ayisyenMagyarՀայերենԱրեւմտահայերէնInterlinguaJaku IbanBahasa IndonesiaInterlingueIgboIlokanoIdoÍslenskaItaliano日本語PatoisLa .lojban.JawaქართულიQaraqalpaqshaGĩkũyũҚазақшаភាសាខ្មែរಕನ್ನಡکٲشُرRipoarischKurdîКыргызчаLatinaLëtzebuergeschLimburgsLigureLombardລາວLietuviųLatviešuMadhurâMalagasyMinangkabauМакедонскиമലയാളംМонголमराठीBahasa MelayuMaltiမြန်မာဘာသာPlattdüütschNedersaksiesनेपालीनेपाल भाषाNederlandsNorsk nynorskNorsk bokmålSesotho sa LeboaOccitanଓଡ଼ିଆਪੰਜਾਬੀPicardPolskiPiemontèisپنجابیپښتوPortuguêsRuna SimiRomânăРусскийРусиньскыйसंस्कृतम्Саха тылаSicilianuScotsسنڌيSängöSrpskohrvatski / српскохрватскиසිංහලSimple EnglishSlovenčinaSlovenščinaShqipСрпски / srpskiSvenskaKiswahiliꠍꠤꠟꠐꠤŚlůnskiதமிழ்తెలుగుТоҷикӣไทยTürkmençeTagalogTürkçeТатарча / tatarçaReo tahitiئۇيغۇرچە / UyghurcheУкраїнськаاردوOʻzbekcha / ўзбекчаVènetoTiếng ViệtWalonWinaray吴语მარგალურიייִדישⵜⴰⵎⴰⵣⵉⵖⵜ ⵜⴰⵏⴰⵡⴰⵢⵜ中文文言閩南語 / Bân-lâm-gí粵語IsiZulu\n",
      "\n",
      "링크 편집\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "문서토론\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "한국어\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "읽기편집역사 보기\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "도구\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "도구\n",
      "사이드바로 이동\n",
      "숨기기\n",
      "\n",
      "\n",
      "\n",
      "\t\t동작\n",
      "\t\n",
      "\n",
      "\n",
      "읽기편집역사 보기\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t일반\n",
      "\t\n",
      "\n",
      "\n",
      "여기를 가리키는 문서가리키는 글의 최근 바뀜파일 올리기고유 링크문서 정보이 문서 인용하기축약된 URL 얻기QR코드 다운로드\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t인쇄/내보내기\n",
      "\t\n",
      "\n",
      "\n",
      "책 만들기PDF로 다운로드인쇄용 판\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t다른 프로젝트\n",
      "\t\n",
      "\n",
      "\n",
      "위키미디어 공용위키데이터 항목\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "보이기\n",
      "사이드바로 이동\n",
      "숨기기\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "위키백과, 우리 모두의 백과사전.\n",
      "\n",
      "\n",
      " AI는 여기로 연결됩니다. 조류가 걸리는 호흡기 질병에 대해서는 조류 인플루엔자 문서를, 다른 뜻에 대해서는 AI (동음이의) 문서를 참고하십시오.\n",
      " 위키백과의 인공지능에 대해서는 위키백과:인공지능 문서를 참고하십시오.\n",
      "시리즈인공지능\n",
      "주요 목표\n",
      "인공 일반 지능\n",
      "지능형 에이전트\n",
      "재귀적 자기 개선\n",
      "컴퓨터 비전\n",
      "지식 표현\n",
      "자연어 처리\n",
      "로봇공학\n",
      "AI 안전\n",
      "\n",
      "접근\n",
      "기계 학습\n",
      "기호주의\n",
      "딥 러닝\n",
      "베이즈 네트워크\n",
      "진화 알고리즘\n",
      "하이브리드 인텔리전트 시스템\n",
      "인공지능 시스템 통합\n",
      "\n",
      "애플리케이션\n",
      "딥페이크\n",
      "생성형 인공지능\n",
      "예술\n",
      "오디오\n",
      "정부의 인공 지능\n",
      "보건분야의 인공지능\n",
      "산업 인공지능\n",
      "기계 번역\n",
      "\n",
      "철학\n",
      "인공 의식\n",
      "중국어 방\n",
      "친절한 AI\n",
      "AI 정렬/AI에 의한 탈취\n",
      "윤리\n",
      "실존적 위험\n",
      "튜링 테스트\n",
      "불쾌한 골짜기\n",
      "\n",
      "역사\n",
      "연표\n",
      "진행상황\n",
      "AI 겨울\n",
      "AI 붐\n",
      "\n",
      "용어\n",
      "용어\n",
      "vte\n",
      "인공지능(人工智能, 영어: artificial intelligence, AI)은 인간의 학습능력, 추론능력, 지각능력을 인공적으로 구현하려는 컴퓨터 과학의 세부분야 중 하나이다. 정보공학 분야에 있어 하나의 인프라 기술이기도 하다.[1][2] 인간을 포함한 동물이 갖고 있는 지능 즉, 자연 지능(natural intelligence)과는 다른 개념이다.\n",
      "인간의 지능을 모방한 기능을 갖춘 컴퓨터 시스템이며, 인간의 지능을 기계 등에 인공적으로 시연(구현)한 것이다. 일반적으로 범용 컴퓨터에 적용한다고 가정한다. 이 용어는 또한 그와 같은 지능을 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 과학 기술 분야를 지칭하기도 한다.[3][4][5]\n",
      "\n",
      "\n",
      "강인공지능과 약인공지능[편집]\n",
      "초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 제안한 것으로 \"기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것\"이다. 그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에 대한 고려를 하지 못한 것 같다. 인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다. 거의 대부분 정의들이 인간처럼 사고하는 시스템, 인간처럼 행동하는 시스템, 이성적으로 사고하는 시스템 그리고 이성적으로 행동하는 시스템이라는 4개의 분류로 분류된다.[6] 인공지능의 작동 방식이 의사 결정과 문제 해결, 학습에 있어 사람의 생각이나 행동과 유사할수록 강한 인공지능으로 분류되고, 논리에 의해 만들어지는 합리적인 생각이나 행동에 가까울수록 약한 인공지능으로 분류된다.[7]\n",
      "\n",
      "약인공지능[편집]\n",
      " 이 부분의 본문은 약한 인공지능입니다.\n",
      "약인공지능(weak AI)은 사진에서 물체를 찾거나 소리를 듣고 상황을 파악하는 것과 같이 기존에 인간은 쉽게 해결할 수 있으나 컴퓨터로 처리하기에는 어려웠던 각종 문제를 컴퓨터로 수행하게 만드는데 중점을 두고 있다. 한참 막연한 인간 지능을 목표로 하기보다는 더 현실적으로 실용적인 목표를 가지고 개발되고 있는 인공지능이라고 할 수 있으며, 일반적인 지능을 가진 무언가라기보다는 특정한 문제를 해결하는 도구로써 활용된다.\n",
      "\n",
      "강인공지능 (AGI)[편집]\n",
      "강인공지능(strong AI) 또는 인공 일반 지능(artificial general intelligence, AGI)은 인간처럼 실제로 사고하여 문제를 해결할 수 있는 \"일반 지능\"을 인공적으로 구현하려는 시도이다. 오늘날 이 분야의 연구는 주로 미리 정의된 규칙의 모음을 이용해서 지능을 흉내내는 컴퓨터 프로그램을 개발하는 것에 맞추어져 있다. 강한 인공지능 분야의 발전은 여전히 미약하지만, 인간과 같은 지능이라는 목표를 어떻게 정의하는지에 따라 어느 정도 발전이 이루어지고 있다고도 볼 수 있다.\n",
      "\n",
      "강인공지능의 실현 가능성에 관한 논쟁[편집]\n",
      "존 설이나 휴버트 드라이퍼스와 같은 몇몇 철학자들은 몸이 아닌 기계에 인간의 지능이나 의식을 구현하는 작업의 실현 가능성에 대한 철학적 바탕을 두고 논쟁을 벌였다. 설은, 튜링 테스트의 통과 여부는 사람의 기준으로 볼 때 기계가 의식을 갖추었다는 판단의 필요 조건이 되지 못한다는 중국어 방에 대한 논증으로 유명하다. 드라이퍼스는 그의 저서 \"컴퓨터가 할 수 없는 것들: 인공적인 추론에 대한 비평\"에서 의식이라는 것은 룰이나 논리 기반 시스템 또는 물리적인 형태를 가지고 있지 않은 시스템에서 찾을 수 없으나, 신경망(neural network)이나 그 유사한 메커니즘을 이용하는 로보틱 시스템은 인공지능을 실현할 수 있는 가능성이 있다고 주장했다.\n",
      "다른 철학자들은 엇갈린 관점을 고수한다. 많은 사람들이 약한 인공지능 정도는 가능하다고 보지만, 또한 많은 사람들이 강한 인공지능을 지지하고 있는 것도 사실이다. 대니얼 C. 데넷은 그의 '의식에 대한 설명'에서 만일 마법의 불꽃이나 영혼이 없다면 인간은 기계에 불과하다며, 지능에 대해서만 인간이라는 기계가 다른 실현 가능한 모든 기계와 다르게 특별 취급을 받아야할 이유가 무엇인가 묻고 있다.\n",
      "어떤 철학자들은 우리가 약한 인공지능을 가능한 것으로 받아들인다면, 강한 인공지능 역시 받아들여야 한다고 주장한다. 지능은 외견상 보이는 것을 가리키는 것이지 진정한 실체가 아니라는 약한 인공지능의 입장은 많은 비판을 받고 있다. 그러나 이에 반하는 손쉬운 예를 사이먼 블랙번의 철학 입문서 \"생각\"에서 찾을 수 있다. 블랙번은 당신이 지능적으로 보이지만, 그 지능이 실존하는가에 대해서 말할 수 있는 방법이 없다고 지적한다. 그는 우리는 단지 믿음 또는 신념 위에서 그것을 다룰 뿐이라고 이야기한다.\n",
      "강한 인공지능을 지지하는 사람들은 인공지능에 반대하는 사람들의 논증이 결국은 아래와 같은 주장을 조합한 것이라고 주장한다.\n",
      "\n",
      "특권에 바탕을 둔 오만함으로 인해 인간에게는 (기계에는 없는) 마법의 불꽃이 있다는 주장 (예를 들면, 신에 의해 주어진 영혼)\n",
      "지능은 기계로는 성취될 수 없는 그 무엇이라는 주장.\n",
      "강한 인공지능을 뒷받침하는 논증(따라서 반대하는 사람은 이 논증을 논박해야 한다.)은 다음과 같다.\n",
      "\n",
      "인간의 마음은 유한 상태 기계(Finite State Machine)이고, 따라서 처치-튜링 이론은 뇌에 적용 가능하다.\n",
      "뇌는 순수한 하드웨어이다.(말하자면 고전적인 컴퓨터처럼 동작한다.)\n",
      "인간의 마음은 오로지 뇌를 통해서만 존재한다.\n",
      "로저 펜로즈를 포함한 몇몇 학자들은 지능에 처치-튜링 명제의 적용이 가능하지 않다고 논박한다. 특히 펜로즈는 인간의 마음에는 물리적인 속성을 뛰어넘는 무언가가 있다고 이야기하며, 그의 주장은 우리의 우주 안에서 초연산(hypercomputation)이 가능하다는 논증에 바탕을 두고 있다. 양자역학과 뉴턴 역학에 따르면 이러한 초연산은 가능하지 않지만, 특별한 시공간에서는 가능한 것으로 생각되기도 한다. 그러나 이는 소수의 주장이며, 우리의 우주는 그러한 초연산이 가능할 정도로 꼬이지(convoluted) 않았다는 많은 학자들의 합의가 존재한다.\n",
      "\n",
      "역사[편집]\n",
      " 이 부분의 본문은 인공지능의 역사입니다.\n",
      "인공지능 이론의 발전[편집]\n",
      "상당수 인공지능 연구의 (본래) 목적은 심리학에 대한 실험적인 접근이었고, 언어 지능(linguistic intelligence)이 무엇인지를 밝혀내는 것이 주목표였다(튜링 테스트가 대표적인 예이다).\n",
      "언어 지능을 제외한 인공지능에 대한 시도들은 로보틱스와 집합적 지식을 포함한다. 이들은 환경에 대한 처리, 의사 결정을 일치시키는 것에 중심을 두며 어떻게 지능적 행동이 구성되는 것인가를 찾을 때, 생물학과, 정치과학으로부터 이끌어 낸다. 사회적 계획성과 인지성의 능력은 떨어지지만 인간과 유사한 유인원을 포함한, 복잡한 인식방법을 가진 동물뿐만 아니라 특히 곤충들(로봇들로 모방하기 쉬운)까지 포함한 동물학으로부터 인공지능 과학은 시작된다. 여러 가지 생명체들의 모든 논리구조를 가져온 다는 것은 이론적으로는 가능하지만 수치화, 기계화 한다는 것은 쉬운 일이 아니다.\n",
      "인공지능 학자는 동물들은 인간들보다 모방하기 쉽다고 주장한다. 그러나 동물의 지능을 만족하는 계산 모델은 없다. 매컬러가 쓴 신경 행동에서 내재적 사고의 논리적 계산[8], 튜링의 기계와 지능의 계산[9] 그리고 리클라이더의 인간과 컴퓨터의 공생[10]가 기계 지능의 개념에 관한 독창적인 논문들이다.\n",
      "존 루커스의 지성, 기계, 괴델[11]과 같은 논리학과 철학기반의 기계지능의 가능성을 부인한 초기 논문들도 있다.[12]\n",
      "인공지능 연구에 바탕을 둔 실질적인 작업이 결실을 거둠에 따라, 인공지능을 지지하는 사람들은 인공지능의 업적을 깎아내리기 위해 인공지능에 반대하는 사람들이 예전에는 '지능적'인 일이라고 주장하던 컴퓨터 체스나 바둑, 음성인식 등과 같은 작업에 대해 말을 바꾸고 있다고 비난하였다. 그들은 이와 같이 연구 목표를 옮기는 작업은 '지능'을 '인간은 할 수 있지만, 기계는 할 수 없는 어떤 것'으로 정의하는 역할을 한다고 지적하였다.\n",
      "(E.T. Jaynes에 따르면) 존 폰 노이만은 이미 이를 예측하였는데, 1948년에 기계가 생각하는 것은 불가능하다는 강의를 듣고 다음과 같이 말하였다. \"당신은 기계가 할 수 없는 어떤 것이 있다고 주장한다. 만일 당신이 그 기계가 할 수 없는 것이 무엇인지를 정확하게 이야기해준다면, 나는 언제든지 그 일을 수행할 수 있는 기계를 만들 수 있다.\" 했다. 폰 노이만은 이미 그 전에 모든 처리절차(procedure)는 (범용)컴퓨터에 의해서 시뮬레이션 될 수 있다고 이야기함에 따라 쳐치-튜링 이론을 언급했다.\n",
      "1969년에 매카시와 헤이스는 그들의 논문 \"인공지능 관점에서 바라본 철학적인 문제들\"에서 프레임 문제를 언급하였다.\n",
      "\n",
      "인공지능의 탄생(1943-1956)[편집]\n",
      "1940년대 후반과 1950년대 초반에 이르러서 수학, 철학, 공학, 경제 등 다양한 영역의 과학자들에게서 인공적인 두뇌의 가능성이 논의되었다. 1956년에 이르러서, 인공지능이 학문 분야로 들어섰다.\n",
      "\n",
      "인공두뇌학과 초기 신경 네트워크[편집]\n",
      "생각하는 기계에 대한 초기 연구는 30년대 후기에서부터 50년대 초기의 유행한 아이디어에 영감을 얻은 것이었다. 당시 신경학의 최신 연구는 실제 뇌가 뉴런으로 이루어진 전기적인 네트워크라고 보았다. 위너가 인공두뇌학을 전기적 네트워크의 제어와 안정화로 묘사했으며, 섀넌의 정보 과학은 디지털 신호로 묘사했다. 또 튜링의 계산 이론은 어떤 형태의 계산도 디지털로 나타낼 수 있음을 보였다. 이런 여러 밀접한 연관에서, 인공두뇌의 전자적 구축에 대한 아이디어가 나온 것이다.[13] 월터의 거북이 로봇이 이 아이디어를 중요하게 포함한 연구의 예이다. 이 기계는 컴퓨터를 사용하지 않고 아날로그 회로를 이용했지만, 디지털의 전자적, 상징적 추리를 보여주기엔 충분했다.[14]\n",
      "월터 피츠(Walter Pitts)와 워런 매컬러(Warren Sturgis McCulloch)는 인공 신경망에 기인한 네트워크를 분석하고 그들이 어떻게 간단한 논리적 기능을 하는지 보여주었다. 그들은 후에 신경 네트워크[15]라 부르는 기술을 첫번째로 연구한 사람이다. 피츠와 매컬러는 24살의 대학원생인 젊은 마빈 민스키를 만났고, 민스키는 1951년 첫번째 신경 네트워크 기계인 SNARC[16]를 구축했다. 민스키는 향후 50년동안 인공지능의 가장 중요한 지도적, 혁신적 인물 중 하나가 되었다.\n",
      "\n",
      "튜링 테스트[편집]\n",
      "1950년 앨런 튜링은 생각하는 기계의 구현 가능성에 대한 분석이 담긴, 인공지능 역사에서 혁혁한 논문을 발표했다.[17] 그는 \"생각\"을 정의하기 어려움에 주목해, 그 유명한 튜링테스트를 고안했다. 텔레프린터를 통한 대화에서 기계가 사람인지 기계인지 구별할 수 없을 정도로 대화를 잘 이끌어 간다면, 이것은 기계가 \"생각\"하고 있다고 말할 충분한 근거가 된다는 것이었다.[18] 튜링 테스트는 인공 지능에 대한 최초의 심도 있는 철학적 제안으로 평가받는다.\n",
      "\n",
      "게임 인공지능[편집]\n",
      "1951년에, 맨체스터 대학의 페란티 마크 1(Ferranti Mark 1) 기계를 사용하여 크리스토퍼 스트레이(Christopher Strachey)는 체커 프로그램을 작성했고, 디트리히 프린츠(Dietrich Prinz)는 체스 프로그램을 작성했다.[19] 아서 새뮤얼(Arthur Samuel)이 50년대 중반과 60년대 초반에 개발한 체커 프로그램은 결국 존경받는 아마추어에 도전할 수 있는 충분한 기술적 발전을 이룩했다.[20]\n",
      "\n",
      "상징 추론과 논리 이론[편집]\n",
      "디지털 컴퓨터에 접할 수 있어진 50년대 중반에 이르러서, 몇몇 과학자들은 직관적으로 기계가 수를 다루듯 기호를 다루고, 사람처럼 기호의 본질적인 부분'까지 다룰 수 있을 것이라고 생각했다.[21] 이것은 생각하는 기계를 만드는 새로운 접근 방법이었다. 1956년[22]에, 앨런 뉴얼(Allen Newell)과 허버트 사이먼(Herbert A. Simon)은 \"논리 이론\"을 구현했다. 그 프로그램은 결국 러셀과 화이트헤드의 '수학 원리'에 나오는 52개의 정리 중 32개를 증명해냈고, 일부 새롭고 더 우아한 증거를 찾아내기도 했다.[23]\n",
      "\n",
      "다트머스 컨퍼런스 1956년: AI의 탄생[편집]\n",
      "1956[24]년에 열린 다트머스 컨퍼런스는 마빈 민스키와 존 매카시, 그리고 IBM의 수석 과학자인 클로드 섀넌과 네이선 로체스터가 개최했다. 컨퍼런스는 \"학습의 모든 면 또는 지능의 다른 모든 특성로 기계를 정밀하게 기술할 수 있고 이를 시물레이션 할 수 있다\"[25]라는 주장을 포함하여 제안을 제기했다. 참가자는 레이 솔로모노프(Ray Solomonoff), 올리버 셀프리지(Oliver Selfridge), 트렌처드 모어(Trenchard More), 아서 새뮤얼(Arthur Smuel), 앨런 뉴얼(Allen Newell)과 허버트 사이먼(Herbert A. Simon)으로, 그들 모두 수십년동안 인공지능 연구에서 중요한 프로그램을 만들어온 사람들이었다.[26] 컨퍼런스에서 뉴얼과 사이먼은 \"논리 이론\"을 소개했고, 매카시는 Artificial Intelligence를 그들의 연구를 칭하는 이름으로 받아들이길 설득했다.[27] 1956년 다트머스 컨퍼런스는 AI 라는 이름, 목표점, 첫번째 성공과 이를 이룬 사람들, 그리고 넓은 의미의 AI의 탄생을 포함하는 순간이었다.[28]\n",
      "\n",
      "황금기(1956~1974년)[편집]\n",
      "다트머스 컨퍼런스 이후에, AI라는 새로운 영역은 발전의 땅을 질주하기 시작했다. 이 기간에 만들어진 프로그램은 많은 사람들을 \"놀랍게(astonishing)[29]\"만들었는데, 프로그램은 대수학 문제를 풀었고 기하학의 정리를 증명했으며 영어를 학습했다.\n",
      "몇 사람들은 이와같은 기계의 \"지능적\" 행동을 보고 AI로 모든 것이 가능할 것이라 믿었다.[30] 연구가들은 개인의 의견 또는 출판물들을 통해 낙관론을 펼쳤고, 완전한 지능을 갖춘 기계가 20년 안에 탄생할 것이라고 예측했다.[31] ARPA(Advanced Research Projects Agency)같은 정부 기관은 이 새로운 분야에 돈을 쏟아부었다.[32]\n",
      "\n",
      "작업들[편집]\n",
      "많은 성공적인 프로그램과 새로운 발전 방향이 50년대 후반과 60년대에 나타났다. 이곳에는 AI 역사에 지대한 영향을 미친 것들을 기술했다.\n",
      "\n",
      "탐색 추리[편집]\n",
      "초기 AI 프로그램은 동일한 기본적인 알고리즘을 사용했다. 게임의 승리나 정리 증명 같은 어떤 목표 달성을 위해, 그들은 한발짝식 나아가는(step-by-step) 방식을 상용했다. 예를 들어 미로를 찾아갈때 계속 나아가면서 막힌 길이 있으면 다른 길이 있는 곳까지 되돌아 왔다가 다른 길로 가는 식이었다. 이런 패러다임은 \"탐색 추리\"라 불렸다.[33] 주요한 문제는, 간단한 미로에 있어서도 경로로 사용할 수 있는 수가 천문학적으로 많았다는 것이다.\n",
      "연구가들은 추론 또는 경험적으로 찾은 규칙으로 정답이 아닌듯 보이는 경로를 지우는 방식을 사용했다.[34]뉴엘과 사이먼은 \"범용 문제 해결기(General Problem Solver)[35]\"라 부르는 프로그램 속 알고리즘의 범용적인 버전을 포착하려고 노력했다. 다른 \"검색\" 프로그램은 기하학과 대수학의 문제를 해결하는 것처럼 인상적인 작업 - 허버트 게랜터(Herbert Gelenter)의 \"기하학 해결기\"나 민스키의 제자인 제임스 슬레이글(James Slage)의 SAINT - 을 수행하길 시도했다.[36] 다른 프로그램은 목표와 목표에 다가가기 위한 세부 계획을 검색했고, 여기에는 스탠포드에서 샤키(Shakey) 로봇의 동작을 제어하기 위해 개발한 STRIPS 시스템을 포함한다.[37]\n",
      "\n",
      "자연어 처리[편집]\n",
      "AI 연구의 중요한 목표는 영어와 같은 자연어로 컴퓨터와 의사소통할 수 있게 하는 것이었다. 일찍이 다니엘 보로우(Daniel Bobrow)의 STUDENT라는 프로그램은 고등학교 수준의 대수학 단어 문제를 푸는 성공을 보였다.[38]\n",
      "'의미 망'은 개념을 다른 개념들 사이의 노드와 링크 관계로 나타낸다. 의미 망을 사용하는 첫 번째 AI 프로그램은 로스 퀄리언(Ross Quillian)[39]이 작성했고 가장 성공이며 동시에 논쟁이 많았던 버전은 로거 섕크(Roger Schank)의 \"개념 종속 이론(Conceptual dependency theory)\"이다.[40]\n",
      "조셉 웨이젠바움(Joseph Weizenbaum)의 ELIZA는 사람들이 그들이 대화를 나누는 때때로 상대가 컴퓨터가 아니라 사람이라고 생각할 정도의 질로 대화했다. 사실, ELIZA는 스스로 생각하여 말하지 않았다. 그 프로그램은 오직 판에 박힌 말을 하거나, 상대에게 방금 말한 말을 다시 해 달라고 요청하거나, 상대가 한 말을 몇 개의 문법 법칙에 의해 파싱 할 뿐이었다. ELIZA는 첫 번째 채팅 프로그램이 되었다.[41]\n",
      "\n",
      "마이크로월드[편집]\n",
      "1960년대 후반에, MIT의 AI 연구소에 있던 마빈 민스키와 시모어 페퍼트는 마이크로월드 연구로 불리는, 인위적인 간단한 상황에 초점을 맞춘 AI 연구를 제안했다. 그들은 성공적인 과학자들이 자주 쉬운 이해를 위해 '마찰면'이라든지 '강체(물리학에서 결코 형태가 변하지 않는 물체)'같은 간단한 모델을 사용한다는 것에 집중했다. 이런 연구의 대부분이 평평한 평면 위의 다양한 형태와 색깔의 블록으로 이루어진 '블록 단위의 세계'에 초점을 맞추는 형식이었다.[42]\n",
      "제럴드 서스먼(Gerald Sussman)을 필두로 아돌포 구스만(Adolfo Guzman), 데이비드 월츠(David Waltz) 그리고 패트릭 윈스턴(Patrick Winston)이 마이크로월드 패러다임으로 기계 비전의 혁신을 이끌었다. 같은 시간에, 민스키와 페퍼는 블록을 쌓을 수 있는 로봇 팔을 제작했다. 마이크로월드의 영광스러운 성취는 테리 위노가드(Terry Winograd)의 SHRDLU이며, 이것은 보통의 일반 문장으로 소통해 작업을 계획하고 이를 실행할 수 있었다.[43]\n",
      "\n",
      "낙관론[편집]\n",
      "AI 연구의 첫 번째 세대는 그들의 연구 결과에 대해 다음과 같이 예측했다.\n",
      "\n",
      "1958년, 사이먼(H. A. Simon)과 뉴얼(Allen Newell) : \"10년 내에 디지털 컴퓨터가 체스 세계 챔피언을 이길 것이다\", 덧붙여 \"10년 내에 디지털 컴퓨터는 중요한 새로운 수학적 정리를 발견하고 증명할 것이다\"라고 말했다.[44]\n",
      "1965년, 사이먼 : \"20년 내에 기계가 사람이 할 수 있는 모든 일을 할 것이다.\"[45]\n",
      "1967년, 마빈 민스키 : \"이번 세기에 AI를 만드는 문제는 거의 해결 될 것이다.\"[46]\n",
      "1970년, 마빈 민스키 : (Life 잡지를 통해서) \"3~8년안에 우리는 평균정도의 인간 지능을 가지는 기계를 가지게 될 것입니다.\"[47]\n",
      "자금[편집]\n",
      "1963년 6월, MIT는 220만 달러를 고등 연구 계획국(Advanced Research Projects Agency - 후에 DARPA로 알려짐)에게 제공받았다. 자금은 민스키와 매카시가 5년전 설립한 \"AI 그룹\"이 포섭한 프로젝트 MAC에서 사용되었다. DARPA는 계속해서 매년 300만 달러를 70년대까지 제공했다.[48] DARPA는 또한 유사한 자금을 뉴얼과 사이먼의 CMU 프로그램과 스탠포드 AI 프로젝트에 제공했다.[49] 또다른 중요한 AI 연구소는 1965년 도널드 미키가 에든버러 대학교에 세웠다.[50] 이 4개의 시설은 계속해서 많은 연도에 걸쳐 학계의 주요한 AI연구소, 그리고 자금처로 존재할 것이다.[51]\n",
      "자금은 몇가지 단서와 함께 제공됐다 : ARPA의 기획자 리클리더(J. C. R. Licklider)는 그의 조직은 \"프로젝트가 아니라, 사람에게 투자\"해야 한다고 믿었고, 연구자들이 어떤 방향이든 그들의 관심있는 쪽을 연구하도록 허용했다.[52] 이것은 MIT에 자유분방한 분위기를 생성했고 해킹 문화를 탄생[53]시키기도 했다. 그러나 이렇게 손을 떼고 지켜보는 형식의 지원은 얼마 지속되지 못했다.\n",
      "\n",
      "AI의 첫번째 암흑기(1974-1980)[편집]\n",
      "70년대에 이르자, AI는 비판의 대상이 되었고 재정적 위기가 닥쳤다. AI 연구가들은 그들의 눈앞에 있는 복잡한 문제를 해결하는데 실패했다. 연구가들의 엄청난 낙관론은 연구에 대한 기대를 매우 높여놓았고, 그들이 약속했던 결과를 보여주지 못하자, AI에 대한 자금 투자는 사라져버렸다[54]. 동시에, Connectionism 또는 뉴럴망은 지난 10년동안 마빈 민스키의 퍼셉트론(시각과 뇌의 기능을 모델화한 학습 기계)에 대한 파괴적인 비판에 의해 완전히 중지되었다.[55] 그러나 70년대 후반의 AI에 대한 좋지 않은 대중의 인식에도 불구하고, 논리 프로그래밍, 상징 추론과 많은 여러 영역에서의 새로운 아이디어가 나타났다.[56]\n",
      "\n",
      "문제[편집]\n",
      "1970년대 초, AI 프로그램의 가능성은 제한적이었다. 모든 문제에 걸쳐서 문제를 푸는 인상 깊은 작품들은 겨우 시험용 버전 정도였고, 어떤 의미에선 '장난감'에 가까웠다.[57] AI 연구는 70년대에 더 이상 극복할 수 없는 몇개의 근본적인 한계를 가지게 됐다.몇개의 한계를 통해 십여년 후에 극복되었고, 다른 몇 개는 오늘날까지 남아있다.[58]\n",
      "\n",
      "컴퓨터 능력의 한계 : 정말 유용한 무언가를 이루기에는 메모리 또는 처리 속도가 충분하지 않았다. 예를 들어 로스 퀼리언(Ross Quillian)의 자연어 처리에서 성공적인 완수는 오직 20개의 단어 위에서 발휘되었는데, 이것은 메모리가 꽉 찼기 때문이었다.[59] ++한스 모라벡은 1976년에 컴퓨터가 지능을 가지기엔 여전히 수백만 배 약하다고 논증했다. 그는 비유를 들었는데, AI가 컴퓨터 능력을 필요로 하는 것은 항공기가 마력을 필요로 하는 것과 같다는 것이었다. 컴퓨터 영상에 대해서, 모라벡은 간단하게 계산하여 실시간으로 사람의 망막을 모션 캡처하려면 범용 컴퓨터가 초당 10^9 명령어(1000MIPS)를 처리해야 할 것이라고 추측했다[60]. 2011년경 실용적인 컴퓨터 영상 프로그램은 10,000~1,000,000 MIPS를 요구한다. 1976년경 5백만에서 8백만 달러사이에 판매되던 가장 빠른 슈퍼컴퓨터인 Cray-1은 오직 80~130 MIPS였고, 당시 전형적인 데스크탑 컴퓨터는 겨우 1 MIPS 남짓이었다.\n",
      "폭발적인 조합 수와 비용이성 : 1972년에 리차트 카프(Hichard Karp)는 문제 해결에 지수적 시간이 요구되는 많은 문제를 보여주었다. 하찮은 문제일지라도 이런 문제의 최적의 해답을 찾는 데 상상할 수도 없는 컴퓨터의 시간이 요구되었다. 즉 지금까지 AI '장난감'에서 사용되었던 방법은 실제적으로 유용한 AI 시스템을 제작하는 데 용이하지 못했다.[61]\n",
      "상징적 지식과 추론 : 영상 처리나 자연어 처리 같은 많은 중요한 AI 프로그램은 실제 세상에 대한 간단하지만 어마어마한 양의 정보를 필요로 한다. 그래야 프로그램이 자신이 보고 있는 것이 무엇인지, 또는 자신이 듣고 있는 것이 무엇인지 아이디어를 찾을 수 있기 때문이다. 이 요구는 아기들의 세상에 대해 알아나가는 것과 유사하다. 연구가들은 곧 요구되는 정보의 양이 엄청나게 광대하다는 것을 발견했다. 1970년대의 누구도 이런 데이터가 포함된 데이터베이스를 만들지 못했고, 누구도 이런 데이터를 프로그램 혼자 터득하는 방법을 알지 못했다.[62]\n",
      "모라벡의 패러독스 : 이론을 제작하고 기하학적 문제를 해결하는 것은 컴퓨터에게 비교적 쉽지만, 얼굴을 인식하거나 장애물을 피해 방을 가로지르는 것은 엄청나게 어렵다. 이 설명은 왜 연구가들이 1970년대에 영상처리나 로봇에 대해 조금밖에 진전을 보이지 못했는지 아는 데 도움이 된다.[63]\n",
      "프레임 문제, 자격 문제 : 존 맥캐시와 같은 연구가들은 규칙이 규칙 스스로의 구조를 변경하지 못하면 관련 계획 또는 기본 추론 일반 공제를 나타낼 수 없다는 것을 발견했다.[64]\n",
      "자금 지원의 중단[편집]\n",
      "영국 정보나 DARPA, NRC같은 AI 연구자들에게 자금을 주던 기관들은 연구 진행의 부진에 실망했고 결국 AI에 관한 방향성을 가진 자금 지원을 끊었다. 1966년 기계를 이용한 번역을 비판하는 보고서가 ALPAC에 제출되었을 때부터 이런 흐름이 시작되었다. 총 2천만 달러를 지원한 NRC도 지원을 멈췄다.[65] 1973년 라이트힐 보고서는 \"장대한 목표(grandios objectives)\"를 성취하는 데 실패한 영국의 AI 연구의 상태에 대해 비난했고 결국 영국의 AI 연구소는 해체되었다(보고서는 특히 AI 연구의 실패의 원인이 폭발적인 조합의 수라고 언급했다[66]).[67] DARPA는 CMU의 음성을 이해하는 연구의 연구자들에게 심하게 실망했고 연간 3백만 달러의 지원을 취소했다.[68] 1974년에 이르자 AI 연구에대한 투자는 찾기 어려워졌다. 한스 모라벡은 그의 동료의 비현실적인 예측에 의한 위기를 비난했다. \"많은 연구가들이 많은 연구자는 과장을 증가시키는 웹에 휘말렸다.\"[69] 그러나 여기엔 다른 이슈가 있다 : 1969년 맨스필드의 수정안의 통과이후, DARPA는 자금 지원에 대해 \"비직접적인 기초 연구보다, 임무 완수에 직결된 연구\"를 수행하라는 증가하는 압력을 받고 있었다. 창조성 높은 지원, 자유분방한 연구는 1960년대와 함께 떠났고 DARPA에서 다시 오지 않을 것이다. 대신, 자금은 자동조정 탱크나 전투 관리 시스템과 같은 분명한 프로젝트와 명확한 목표를 향할 것이다.[70]\n",
      "\n",
      "캠퍼스 전역의 비판들[편집]\n",
      "몇 철학자들은 AI 연구가들에게 강력한 반대를 표했다. 초기 반대자들 중 괴델의 불완전성의 원리에 의해 컴퓨터 프로그램같은 시스템이 실제적으로 정확하게 사람과 같이 행할 수 없다고 주장한 사람은 존 루커스(John Lucas)이다.[71] 휴버트 드라이퍼스는 60년대의 깨어진 약속을 조롱했고 AI의 가정을 비판했으며, 인간의 추론이 실제적으론 \"상징적 진행\"이 매우 적게 포함되어 있고 구현적, 본능적, 무의식적인 노하우[72][73]에 의해 처리된다고 주장했다.\n",
      "존 설의 1980년대 제시된 중국인 방 문제는, 실제로 프로그램이 상징들을 '이해'할 수 없고 사용할 수 없음을 보여주려고 시도했다. 설은 만약 상징이 기계에게 아무 의미가 못된다면, 기계는 생각하는 것이 아니라고 주장했다.[74]\n",
      "이 비난은 AI 연구가들에게 심각하게 작용하지 못했다. 비용이성과 상식적 지식에 관한 문제가 훨씬 더 즉각적이고 심각한 듯이 보였다. '노하우'와 '지향성'이 실제 프로그램을 만드는데 어떻게 다른지가 불분명했다. 민스키는 드라이퍼스와 설을 향해 \"그들은 오해했고, 무시될 것이다[75]\"라고 했다. MIT에서 가르쳤던 드라이퍼스는 냉대받았다 : 그는 나중에 AI 연구가들에게 \"나와 점심 식사할 용기도 없다[76]\"라고 평했다. ELIZA의 제작자 조셉 웨이즌바움(Joseph Weizenbaum)은 그의 동료인 드라이퍼스가 전문적이지 않고 유치한 대우를 한다고 느꼈다.\n",
      "웨이즌바움은 케네스 콜비(Kenneth Colby)가 쓴 DOCTOR와 임상치료 채팅봇에 대해서 심각하게 의심하기 시작했다. 웨이즌바움은 콜비가 그의 무심한 프로그램을 진지한 치료 도구로 여기는 걸 방해했다. 이 불화가 시작되고, 이 상황은 콜비가 웨이즌바움을 프로그램에 대한 공로로 인정하지 않았을 때 도움이 되지 않았다. 1976년에 웨이즌바움은 컴퓨터 능력과 인간 추론(Computer Power and Human Reason)을 출판하며\n",
      "인공 지능의 오용이 인간의 삶을 평가 절하시킬 수도 있다고 주장했다.[77]\n",
      "\n",
      "퍼셉트론과 연결망의 어두운 시대[편집]\n",
      "뉴럴 네트워크 형태의 퍼셉트론이 1958년 마빈 민스키의 고등학교 시절 친구였던 프랭크 로센블랫(Frank Rosenblatt)에 의해 도입되었다. 다른 AI 연구가들이 그러하듯, 그는 낙관론을 펼쳤고, \"퍼셉트론은 결국 학습을 하고, 의사 결정을 하고, 언어 번역을 할 것이다\"라고 예견했다. 60년대를 이끌던 패러다임 속의 연구 프로그램의 수행은 1969년 민스키와 페퍼의 책 퍼셉트론의 출판과 함께 갑자기 중지되었다. 이것은 퍼셉트론이할 수 있는 일에 몇가지 심각한 제한이 있음을, 또 프랭크의 예견은 심하게 과장되어있음을 알렸다. 이 책의 파급력은 압도적이었다 : 향후 10년 동안 뉴럴 네트워크에 대한 거의 모든 연구가 중지되었다. 결국, 뉴럴 네트워크 영역을 회복할 연구원의 새로운 세대가 그 후에 인공지능의 중요하고 유용한 부분을 내놓았다. 로센블랫은 이 책을 보지 못했는데, 그는 문제의 책이 출판 되고 곧바로 보트 사고와 함께 사망했기 때문이다.[78]\n",
      "\n",
      "깔끔이 : 논리, 프롤로그와 전문가 시스템[편집]\n",
      "논리적 추론은 1958년 초에 AI 연구에서 존 매카시가 제안하여 도입되었다.[79] 1963년 알렌 로빈슨(J. Alan Robinson)은 간단하게 추론을 컴퓨터에 구현시키는 분해와 통일 알고리즘을 발견했다. 그러나 매카시와 그의 학생들이 60년대 후반에 했던 것과 같은 복잡하지 않은 구현은 본질적으로 다루기 힘들었는데, 간단한 정리를 증명하기 위해 천문학적 단계가 필요했다.[80] 더 성공적인 결실을 맺는 논리적 접근은 70년대 에딘벌 대학의 로버트 코왈스키(Robert Kowalski)가 개발했고 곧 프랑스의 연구가인 알라인 콜메루엘(Alain Colmerauer)과 성공적인 논리 프로그래밍 언어인 프롤로그를 만든 필립 오우셀(Philippe Roussel)과의 협업을 이끌어냈다.[81] 프롤로그는 다루기 쉬운 계산을 허용하는 논리의 부분을 사용한다. 규칙은 계속적으로 영향을 미쳤고, 에드워트 페이젠바움(Edward Feigenbaum)이 기대하던 시스템 기초를 제공했으며 알렌 뉴엘과 허버트가 계속 연구하도록 만들었다. 사이먼은 Soar과 인식에서의 통일 이론을 이끌었다.[82]\n",
      "논리적으로의 접근을 비판하는 지적은, 드라이퍼스가 했던대로, 사람이 문제를 해결할 때 논리를 거의 사용하지 않는다는 것이었다. 피터 왓슨(Peter Waon), 엘리너 로시, 아모스 트버스키, 대니얼 카너먼을 비롯한 심리학자들이 이를 증명했다.[83] 매카시는 이에 대해서 이 증명이 무관하다고 답했다. 그는 정말 필요한 기계란 사람처럼 생각하는 것이 아니라 문제를 해결할 줄 아는 기계라고 일축했다.[84]\n",
      "\n",
      "지저분이 : 프레임과 스크립트[편집]\n",
      "매카시의 접근에 대한 비평가들의 대다수가 그의 동료인 MIT 소속이었다. 마빈 민스키와 사무엘 페퍼와 로저 샹크는 기계를 사람처럼 느껴지도록 만드는 \"이야기 이해\"와 \"물체 인식\"의 문제를 해결하려고 노력했다. \"의자\"나 \"음식점\" 같은 일반적인 개념을 사용할 때 사람들은 모두 비논리적으로, 사람들이 통용하는 범용적 가정을 함께 했다. 불행하게도 이런 부정확한 가정들은 논리적 절차로 대표하기가 힘들었다. 제럴드 서스먼(Gerald Sussman)은 \"본질적으로 부정확한 개념을 설명하기위 해 정확한 언어를 사용하는 순간 그들은 더 이상 부정확하다고 말할 수 없다\"[85]라고 표했다. 또한 섕크는 이에 대해 \"비논리적\" 접근 즉 \"지저분이\"가 매카시, 코와스키, 페이젠바움의 \"깔끔이\" 패러다임과 반대에 있다고 평했다.[86]\n",
      "1975년 세미나 보고서에서, 민스키는 \"지저분한\" 많은 그의 동료 연구자들이 무언가에 대한 우리의 모든 상식적 가정을 포착하는 프레임워크를 도구로 사용했다고 적었다. 예를 들어 우리가 새라는 개념을 생각할때, 즉시 '난다', '벌레를 먹는다'와 같은 다양한 사실들 또한 떠올린다. 떠올린 것들이 항상 사실은 아니고 또 \"논리적\"으로 이것들이 공제가 되지는 않는다. 그러나 이런 가정들의 구조는 우리가 말하고 생각하는 문장의 부분을 차지한다. 그는 이 구조를 \"프레임\"이라 칭했다. 섕크는 프레임의 설명에 대해서 영어로된 짧은 스토리에 대한 답변을 성공적으로 하기 위한 \"스크립트\"라 불렀다.[87] 수년 후 객체지향 프로그래밍에서 AI 연구에서 쓰였던 프레임에서 나온 '상속'이라는 개념을 채택하게 된다.\n",
      "\n",
      "AI붐 (1980-1987)[편집]\n",
      "1980년대에는 전 세계적으로 사용된 ‘전문가 시스템’이라고 일컫는 인공지능 프로그램의 형태였고 인공지능 검색에 초점이 맞춰졌다. 같은 시기에 일본 정부는 자신들의 5세대 컴퓨터 프로젝트와 인공지능에 적극적으로 투자하였다. 1980년대에 존 홉필드와 데이비드 루멜하트의 신경망 이론의 복원이라는 또 다른 사건이 있었다.\n",
      "\n",
      "전문가 시스템의 발전[편집]\n",
      "전문가 시스템은 특정 지식의 범위에 대해 문제를 해결해주거나 질문에 대답해주는 프로그램이며 전문가의 지식에서 파생된 논리적 법칙을 사용하였다. 최초의 실험은 1965년 Edward Feigenbaum과 레더버그에 의해 Dendral이 시작하였고 이것은 분광계로부터 화합물을 식별하는 실험이었다. MYCIN은 1972년에 개발되었고 전염되는 혈액 질환을 진단하였다. 이러한 접근법(실험)은 타당성이 입증되었다.[88]\n",
      "전문가 시스템은 소규모의 지식 영역에 대해서는 스스로 제한을 둠으로써 상식 문제를 피하였다. 그리고 그들의 단순한 디자인은 프로그램을 만드는 것을 상대적으로 쉽게 하였다. 모든 프로그램은 유용성이 입증되어야 하지만 AI는 이 점을 달성할 수 없었다.[89]\n",
      "1980년, XCON이라 불리는 전문가 시스템은 디지털 장비 회사인 CMU에서 완성되었다. 이 시스템은 매년 4천만 달러를 절약시켜주며 매우 큰 성과를 나타냈다.[90] 전 세계의 회사들은 1985년에 1억 달러 이상을 AI에 사용하여 이를 개발하고 전문가 시스템을 배포하였다. Symbolics, Lisp Machines과 같은 하드웨어 회사와 IntelliCorp, Aion 등의 소프트웨어 회사들이 이를 지원하면서 같이 성장하였다.[91]\n",
      "\n",
      "지식 혁명[편집]\n",
      "전문가 지식들을 포함하면서 전문가 시스템의 힘은 두각을 나타내었다. 이것은 1970년대 내내 연구하였던 AI 연구 기법의 새로운 방향 중 일부분이었다. “AI 과학자들은 지능이란 것이 다른 방법들로 많은 양의 다양한 지식들을 사용하는 능력에 기반한 것이라고 의심하기 시작했다.”[92] 지식 기반 시스템과 지식 엔지니어링은 1980년대 AI 연구자들의 메인 포커스가 되었다.[93]\n",
      "또한 1980년대에는 일반인들이 모두 알 만한 일상적인 사실들을 모두 포함한 아주 거대한 데이터베이스를 만들어 상식 문제에 대한 직접적 해결을 시도한 Cyc의 탄생을 볼 수 있었다. 이 프로젝트를 이끈 Douglas Lenat는 지름길은 없다고 말했다. - 기계가 인간의 개념을 알게 하기 위한 단 한 가지 길은 그들을 가르치는 것이다. 이 프로젝트는 수 십 년 동안 완료될 것이라 생각되지 않았다.[94]\n",
      "\n",
      "돈은 되돌아온다 : 5세대 프로젝트[편집]\n",
      "1981년, 일본의 국제 무역과 산업 부서는 5세대 컴퓨터 프로젝트를 위해 8억 5천만 달러를 확보해 두었다. 그들의 목적은 기계가 사람처럼 프로그램을 작성하고 대화를 수행할 수 있는 시스템과 언어를 번역하거나 그림을 해석하는 것이었다. 그들은 프로젝트를 위해 기본 컴퓨터 언어로 Prolog를 선택하였다.[95]\n",
      "다른 나라들은 그들만의 고유한 프로그램을 개발하였다. UK는 3억 5천만 달러를 들여 Alvey 프로젝트를 시작했다. 미국 회사들의 컨소시엄은 정보기술과 AI안의 거대한 프로젝트를 투자받기 위해 마이크로 전자공학과 컴퓨터 기술 협력이라는 형태를 취했다.[96][97] 또한 1984에서 1988년 사이에 DARPA는 전략적 컴퓨팅 계획을 설립하고 AI에 대한 투자를 세배로 늘렸다.[98]\n",
      "\n",
      "신경망 이론의 복귀[편집]\n",
      "1982년, 물리학자 John Hopfield는 (현재 ‘Hopfield net’이라고 불리는) 완벽한 새로운 길에서 정보를 프로세스하고 배울 수 있는 신경망의 형태를 증명해냈다. 이 시기에, David Rumelhart는 (Paul Werbos에 의해 발견된) “역전파”라고 불리는 신경망을 개선하기 위한 새로운 방법을 알리고 있었다. 이러한 두 가지 발견은 1970년 이후 버려진 신경망 이론이라는 분야를 복구시켰다.[99][100]\n",
      "새로운 분야는 1986년 분산 병렬처리의 형태로부터 영감을 받았고 이와 같은 형태로 통일되었다. 2권 분량의 논문 집합은 Rumelhart와 물리학자인 James McClelland에 의해 편집되었다. 신경망은 1990년대에 광학 문자 인식 및 음성 인식과 같은 프로그램의 구동 엔진으로 사용되며 상업적으로 성공했다.[101][102]\n",
      "\n",
      "AI의 두번째 암흑기 1987-1993[편집]\n",
      "AI와 비즈니스 커뮤니티의 매력은 상실했고 경제 거품이라는 고전적 형태의 1980년대에 빠졌다. 붕괴는 정부기관과 투자자들의 ‘해당 분야는 계속해서 비판에도 불구하고 진보해왔다.’는 인식에 비롯된 것이었다. 로봇 공학 분야에 관련 된 연구원인 Rodney Brooks 와 Hans Moravec는 인공지능에 대한 완전히 새로운 접근 방식을 주장하였다.\n",
      "\n",
      "인공지능의 겨울[편집]\n",
      "1974년에 전문가 시스템에 대한 열정이 통제할 수 없을 정도로 퍼져나가고 이에 대한 실망이 확실히 따라올 것이라는 걱정이 있었고 이 때 투자가 끊기고 살아남은 연구원들에 의해서 “AI winter”이라는 단어가 만들어졌다.[103] 그들의 두려움은 AI에 대해 일련의 재정적 차질이 있었던 1980년 말에서 1990년대 초반에 잘 나타난다.\n",
      "이 AI winter 기간의 첫 번째 사건은 1987년에 특성화된 AI 하드웨어 시장이 갑자기 무너진 것이다. 1987년에 애플이나 IBM의 데스크탑 컴퓨터들은 급격히 빨라지고 성능이 좋아졌다. 또한 Symblics과 기타 회사들이 만든 데스크탑 컴퓨터 보다 더 비싼 Lisp 기기들보다도 더욱 좋은 성능을 나타냈다. 즉, 더 이상 Lisp 기기들을 살 이유가 사라진 것이다. 전체산업 1억 달러의 절반의 가치가 하룻밤에 사라졌다.[104]\n",
      "결국 최초의 성공한 전문가 시스템인 XCON은 유지하기에 너무 비싸다는 것이 증명되었다. 업데이트하기에도 너무 어려웠고 학습도 되지 않았다. 이 전문가 시스템은 또한 일반적이지 않은 질문을 했을 때 괴상한 행동을 하는 일명 \"brittle\" 이었고 그들은 일찍이 발견된 이러한 문제들에 의해 결국 희생되었다. 전문가 시스템은 특별한 경우에서만 유용할 뿐이었다.[105]\n",
      "1980년대 후반, Strategic Computing initiative는 AI의 투자를 자르는 데 공이 컸다. DARPA의 새로운 리더쉽은 AI는 이 다음의 파도가 아니라고 결정했고 즉각적인 결과를 나타낼 수 있는 것으로 보이는 프로젝트에 직접적인 투자를 하는 방향으로 결정했다.[106]\n",
      "1991년에는 1981년에 일본에서 5세대 프로젝트의 목표 리스트에 적은 것만큼 성과가 나오지 않았다. 실제로 대화를 계속 이어나가는 것과 같은 어떤 것들은 2010년까지 달성되지 않았다. 다른 인공 지능 프로젝트와 마찬가지로, 실제 가능했던 것보다 기대가 훨씬 컸다.[107]\n",
      "\n",
      "몸통을 갖는 것의 중요성: Nouvellle AI and embodied reason[편집]\n",
      "1980년대 후반 , 몇몇 연구원들이 로봇 공학을 기반으로 인공 지능에 완전히 새로운 접근법에 대해 찬성하였다.[108] 그들은 실제 지능을 보여주려면 기계에도 몸통이 필요하다고 믿었다. - 기계 또한 이 세상에서 인식하고, 이동하고, 살아남고 거래할 줄 알 필요가 있다. 그들은 이런 감각 운동 기술은 상식적인 추론과 같은 더 높은 단계의 기술이 필요하다고 말했고 실제로 추상적인 추론은 인간의 가장 흥미롭거나 중요한 기술이다. 그들은 지능을 바닥에서부터 지어야 한다고 내세웠다.[109]\n",
      "인공 두뇌와 제어 이론에서부터 얻은 접근법은 1960년대까지 인기가 없었다. 또 다른 선구자인 David Marr는 신경 과학 이론으로 한 그룹의 비전을 이끌어 성공적인 배경으로 1970년대에 MIT에 들어왔다. 그는 모든 상식적인 접근법(McCarthy's logic and Minsky's frames)을 거절했고 AI는 시각에 대한 육체적인 기계장치를 심볼릭 프로세싱 하기 전에 가장 바닥에서부터 위로 이해할 필요가 있다고 말했다.[110]\n",
      "1990년에 Elephants Don't Play Chess 논문에서, 로봇 공학 연구자인 Rodney Brooks는 직접적으로 물리적 심볼 시스템 가설에 초점을 맞추었고 심볼들은 항상 필요한 것은 아니라고 말했다. “세계는 그 자체만으로 가장 훌륭한 모델이다. 이것은 항상 최신이며 모든 세부사항이 존재한다. 비결은 적절히 그리고 충분히 자주 감지하는 것이다.[111] 80년대와 90년대에 많은 cognitive 과학자들은 또한 사고방식의 심볼 처리 모델을 거절하고 추론에 몸통은 필수적이라고 말했고 이러한 이론을 embodied mind 이론이라고 불렀다.[112]\n",
      "\n",
      "AI 1993-현재[편집]\n",
      "지금보다 반세기는 더 오래된 AI의 분야는 마침내 가장 오래된 목표 중 몇 가지를 달성했다. 이것은 비록 뒷받침해주는 역할이었지만 기술 산업에 걸쳐 성공적으로 사용되었다. 몇 가지 성공은 컴퓨터의 성능이 증가했기 때문이고 또 다른 몇 가지는 고립된 문제들에 대해 집중하였고 높은 과학적 의무감으로 해 나갔기 때문에 해결되었다. 적어도 비즈니스 분야에서의 AI의 평판은 여전히 처음 같지 않다. 이 분야 내에서는 1960년대 세계의 상상이던 인간 수준의 지능의 꿈을 실현하는 것이 실패로 돌아갔다는 이유로 몇 가지 합의를 하였다. 하위 파트에서 AI의 일부분을 도와주던 모든 요소들은 특정 문제나 접근 방식에 초점이 맞추어졌다.[113] 그 후, AI는 여태 해왔던 것보다 더욱 신중해졌고 더욱 성공적이였다. 또한 보안이 중요한 이슈로 떠올랐다. 인공지능의 보안이슈로는 학습된 인공지능을 속일 수 있는 공격형태인 Poisoning Attack, Evasion Attack, 인공지능 모델 자체를 탈취할 수 있는 Model Extraction Attack, 학습된 모델에서 데이터를 추출해내는 Inversion Attack 등이 있다.[114]\n",
      "\n",
      "성공 사례와 무어의 법칙[편집]\n",
      "1997년 5월 11일, 딥 블루는 당시 체스 세계 챔피언이던 가리 카스파로프를 이긴 최초의 체스 플레이 컴퓨터가 되었다.[115] 2005년 스탠퍼드의 로봇은 DARPA 그랜드 챌린지에서 연습해 보지 않은 사막 도로 131마일을 자동으로 운전하여 우승하였다.[116] 2년 뒤, CMU의 한 팀은 DARPA 도시 챌린지에서 모든 교통 법규를 지키고 교통 혼잡 속에서 자동으로 55 마일의 길을 찾았다.[117] 2011년 2월, 퀴즈 쇼 Jeopardy!에 출전한 IBM의 응답 시스템 왓슨은 상당히 여유롭게 두 챔피언을 이겼다.[118]\n",
      "이러한 성공은 혁신적인 새로운 패러다임 때문이 아니라 번거로운 엔지니어 스킬과 매우 뛰어난 성능을 가진 오늘날의 컴퓨터에서 비롯된 것이다.[119] 실제로, Deep Blue의 컴퓨터는 1951년 Christopher가 체스 하는 법을 가르친 마크 1보다 1천만 배 빨랐다.[120] 이 엄청난 증가는 무어의 법칙에 의해 측정되는데 이것은 2년마다 컴퓨터의 메모리 속도와 양은 두 배씩 늘어난다는 이론이다. 최초 컴퓨터 성능의 근본적인 문제는 느리지만 서서히 극복되고 있었다.\n",
      "\n",
      "지능형 에이전트[편집]\n",
      "1990년대 동안에는 ‘지능형 에이전트’라고 불리는 새로운 패러다임이 다 방면에서 수용되고 있었다.[121] 비록 이전의 연구자들은 'divide and conquer' 모듈러를 제안하고 AI에 접근하였지만[122] 지능형 에이전트는 Judea Pearl, Allen Newell 등 다른 이들이 AI를 연구하는데 있어서 결정론과 경제성이라는 개념을 가져오기 전까지 현대식 형태를 갖추지 못했다.[123] 경제학자들의 합리적 에이전트라는 정의와 컴퓨터 과학자들의 객체 혹은 모듈러 정의가 합쳐졌을 때 지능형 에이전트의 패러다임이 완성되었다.\n",
      "지능형 에이전트 시스템은 환경을 인식하고 성공을 가장 극대화할 수 있는 행동을 취한다. 이러한 정의에 의하면 인간과 인간의 조직처럼, 예를 들어 회사처럼 특정 문제를 해결하는 간단한 프로그램을 지능형 에이전트라고 한다. 지능형 에이전트는 AI 연구자를 “the study of intelligent agents\"로 정의한다. 이것은 AI의 정의의 일부를 일반화한 것이다. 이것은 인간의 지능을 넘어 모든 종류의 지능의 연구를 추구한다.[124]\n",
      "이러한 패러다임은 당시 연구자들이 고립 문제에 대해 연구하고 다양하고 유용한 해결법을 찾도록 해주었다. 또한 서로서로 문제와 해결책을 공통의 언어로 표현하였고 추상적 에이전트를 사용한 경제학이나 제어 이론 등과 같은 다른 개념에도 사용되었다. 어떤 연구자들은 지능형 에이전트의 상호 작용에서 더 다양하고 지능적인 시스템을 만들기로 하였고 완전한 에이전트 아키텍처가 되기를 바랐다.[125] 이것이 21세기의 보편적인 교과서들이 인공 지능을 정의하는 방식이다.[126][127]\n",
      "\n",
      "깔끔함의 승리[편집]\n",
      "AI 연구자는 과거에 사용했던 것보다 더욱 정교한 수학적 도구를 사용하여 개발하기 시작했다.[128] 해결하는 데 AI가 필요한 수많은 문제들이 존재하고 있다는 인식은 수학, 경제학 또는 오퍼레이션 연구 등의 분야에서 이미 연구자들이 AI를 사용하여 실현하고 있었다. 공유된 수학적 언어는 높은 수준의 협력, 좋은 평판, 여러 분야를 성공적으로 이끌고 측정과 증명이 된 결과들의 성취를 가능하게 하였다. AI는 더 엄격한 과학 학문이 되었다.\n",
      "이는 혁명 그 자체였으며 \"깔끔함\"의 승리였다.[129][130]\n",
      "Judea Pearl의 매우 영향이 큰 1988년 책은 AI에 결정론과 확률을 대입시켰다.[131] 사용 중인 많은 새로운 도구(Bayesian networks, hidden Markov models, information theory, stochastic modeling)와 기존의 고전적이 방법들이 최적화되었다. 더 정밀한 수학적 모형이 신경망 네트워크와 진화 알고리즘과 같은 연산 지능적 패러다임을 위해 개발되었다.[132]\n",
      "\n",
      "조용한 발전[편집]\n",
      "AI 연구자들에 의해 최초로 개발된 알고리즘은 거대한 시스템의 일부로 나타나기 시작했다. AI는 매우 어려운 문제[133]들을 해결했고 데이터 마이닝, 산업 로봇공학, 논리학[134], 음성 인식[135], 은행 소프트웨어,[136]의학적 진단, 구글 검색 엔진[137] 등 여러 기술들은 기술 산업[138]에 매우 유용하다는 것이 증명되었다.\n",
      "AI 분야는 이러한 성공에 대해 매우 낮은 신뢰를 받았다. AI의 훌륭한 혁신 중 대부분은 컴퓨터 과학의 도구에서 또 다른 기능으로 세분화되었다.[139]닉 보스트롬은 \"많은 최첨단 AI가 일반 응용 부문으로 필터링되었으며, AI라고 불리지 않는 경우가 많다. 일단 무언가가 충분히 유용하고 일반적이게 되면 더 이상 AI라는 라벨이 붙지 않기 때문이다.\"라고 말했다.[140] 1990년대 AI 분야의 많은 연구자들이 고의로 자신의 프로젝트를 다른 이름으로 불렀다. 일부 이러한 현상은 그들의 분야가 AI와 근본적으로 다르다고 여겼기 때문이거나 또는 새로운 이름이 투자받기 쉬웠기 때문일 것이라고 한다. 적어도 상업 분야에서는 연구자에 대해 AI 겨울에 있었던 실패했던 계약이 꼬리표처럼 따라다녔고 2005년에 뉴욕 타임즈에서는 “컴퓨터과학과 소프트웨어 엔지니어들은 광기에 싸인 몽상가처럼 보일 두려움 때문에 인공 지능이란 용어를 피했다.” 라고 소개되었다.[141][142][143]\n",
      "\n",
      "HAL 9000은 어디에 있는가?[편집]\n",
      "1968년 아서 C. 클라크와 스탠리 큐브릭은 2001년에는 기계가 인간과 유사하거나 또는 인간의 용량을 뛰어넘는 지능을 가진 존재가 되었을 것이라고 상상하며 《2001: 스페이스 오디세이》라는 SF 작품을 완성했다. 여기에 등장하는 HAL 9000이라는 캐릭터는 2001년에 이러한 기계가 존재할 거라고 믿는 많은 AI 연구자들의 공유된 믿음을 기반으로 만들어졌다.[144]\n",
      "훗날 마빈 민스키는 “그래서 왜 우린 2001년에 HAL을 얻지 못했나?”라는 질문을 하였다.[145] 대부분의 연구자들이 신경망이나 유전자 알고리즘의 상업적 용도의 프로그램을 추구했던 반면, 민스키는 해답이 방치된 상식 추론과 같이 매우 중심적인 문제에 있다고 믿었다. 반면에 존 매카시는 여전히 자격문제를 비난하였다.[146] 레이 커즈와일은 문제는 컴퓨터 성능에 있으며 무어의 법칙을 사용하였을 때 인간 수준의 지능을 가진 기계는 약 2029년에 나올 것이라고 예견하였다.[147] 제프 호킨스(Jeff Hawkins)는 신경망 연구자들이 대뇌 피질의 본질적인 성질을 무시하고 간단한 문제들을 성공적으로 해결하는 간단한 모델을 추구했다고 말했다.[148] 또한 각각에 대해 많은 설명들이 있으며 이를 대응하는 진행 중인 연구 프로그램들이 있다.\n",
      "\n",
      "인공지능과 4차 산업혁명[편집]\n",
      "세계는 이미 4차 산업혁명에 진입했으며 인공지능은 빠르게 인간을 대체해 나갈 것이다. 또, 널리 퍼져 있지 않을 뿐 미래는 이미 와 있으며 인공지능, IoT, 클라우드 컴퓨팅, 빅데이터 등이 융합되면서 4차 산업혁명이 발생하고 있다. 과거 산업혁명이 ‘기계근육’을 만드는 과정이었다면 4차 혁명에서는 ‘기계두뇌’가 탄생할 것이다.[149]\n",
      "제1차 산업혁명 발생시, 산업 기계에 의해 일자리를 잃을 것이 두려웠던 노동자들이 러다이트(기계파괴운동)를 일으켰다. 이와 유사하게, 인공 지능에 의한 4차 산업혁명으로, 많은 사람들이 미래에 일자리를 잃을 것을 우려하고 있다. 한 온라인 설문조사[150]에 따르면, 응답자의 70.1%가 미래에 인공지능에 의해 인간의 직업이 줄어들 것이라고 예상했다.\n",
      "\n",
      "실험적인 AI 연구[편집]\n",
      "인공지능은 1959년에 MIT AI연구소를 설립한 매카시와 마빈 민스키, 카네기멜론 대학교에 인공지능 연구소를 만든 앨런 뉴웰과 허버트 사이먼과 같은 개척자들에 의해 1950년도에 실험 학문으로 시작되었다. 이들 모두는 1956년에 매카시, 민스키, IBM의 나단 로체스터 와 클라우드 샤논에 의해 조직되어 열린, 이미 언급된 다트머스 대학의 여름 AI 콘퍼런스에 참가하였다.\n",
      "역사적으로, 인공지능 연구는 두 개의 부류 -- 깔끔이(Neats)와 지저분이(Scruffies) -- 로 나눌 수 있다. 깔끔이는 우리가 전통적 혹은 기호적(symbolic) 인공지능 연구라고 부르는 분야로서, 일반적으로 추상적인 개념에 대한 기호적 연산과 전문가 시스템(expert systems)에 사용된 방법론을 가르친다. 이와 상반된 영역을 우리는 지저분이(Scruffies) 또는 연결주의자(connectionist)라 부르는데, 시스템을 구축하여 지능을 구현/진화시키려고 시도하고, 특정 임무를 완수하기 위해 규칙적인 디자인을 하기보다는 자동화된 프로세스에 의해서 지능을 향상시키는 방식이다. 가장 대표적인 예로 신경망(neural network)이 있다. 이 두 가지 접근법은 인공지능 역사의 매우 초창기부터 함께 했다. 1960년대와 1970년대를 거치며 scruffy 접근법은 주목받지 못했지만, 1980년대 깔끔이 접근법의 한계가 명확해지면서 다시 주목 받게 되었다. 그러나 현재 두 가지 방식을 사용하는 그 어떤 최신의 인공지능 알고리즘도 확실한 한계가 있다는 것이 명확하다.\n",
      "특히 1980년대에 들어서 Back propagation (인공지능 학습방법: Training Method)가 소개되면서 많은 연구가 진행되었음에도, 신경망을 이용한 인공지능은 아직 초보단계이다. 인공신경망 (Artificial Neural Networks)을 이용한 많은 연구가 현재에도 진행되고 있지만, 몇 가지 장애로 인해서 실용화하기엔 아직도 먼 기술이다. 인공신경망을 이용한 인공지능이 어느 정도 실용화되기 위해선 우선 실효성 있는 학습방법 (Training Methods)이 필요하다. Back propagation을 이용한 학습방법이 제안되어 연구되고 있지만, 완전한 학습을 이룰 수 없을 뿐만 아니라, 학습에 사용되는 data들이 서로 orthonormal해야 하는 조건 때문에 항상 불완전한 학습으로 끝나기 쉽다. (Converge to Local Mimimum, not to the optimal minimum: 지역최적해에 머뭄. 즉, 눈먼 장님이 가장 낮은 저지대를 찾는 경우 각 현재 지점에서 아래로 내려가려는 성질이 있는데 이때 눈먼 봉사이므로 특정 지점의 저지대에 도달한 경우, 그 지점에선 어디로 가거나 위로 올라가는 것만 있으므로 앞에 설명한 성질에 의해 바로 전에 찾은 저지대 남으려 하는 성질이 있다는 것을 의미함). 이러한 단점들을 보완하기 위해서 Fuzzy Logic, Neurofuzzy (Neural fuzzy logic) and Genetic Algorithms등을 이용한 학습방법이 연구되고 있으나 전망이 밝지만은 않은 상태이다.\n",
      "미국의 DARPA(미 국방부 최신 기술 연구 프로젝트 관리국)과 일본의 5세대 컴퓨터 프로젝트에 의해서 1980년대 인공지능 연구는 엄청난 연구 기금을 지원 받을 수 있었다. 몇몇 인공지능 선각자들이 거둔 주목할 만한 결과에도 불구하고, 즉각적인 결과를 산출하는 데 실패하게 된다. 이것은 1980년대 후반 인공지능 연구 기금에 대한 대폭적인 삭감을 초래하였고, 인공지능 연구의 침체기를 뜻하는 인공지능의 겨울을 가져왔다. 1990년대, 많은 인공지능 연구가들은 좀 더 구체적인 목적 아래 기계 학습, 로보틱스, 컴퓨터 비전과 같은 인공지능과 관련된 세부 영역으로 이동하였고, 순수하고 보편적인 인공지능에 대한 연구는 매우 제한적으로 수행되고 있다.\n",
      "\n",
      "인공지능 기술의 실용적인 응용[편집]\n",
      "인공지능의 궁극적인 목표인 인간과 같은 지능의 개발이 어려움을 겪자, 다양한 응용 분야가 나타나게 되었다. 대표적인 예가 LISP나 Prolog와 같은 언어인데, 애초에 인공지능 연구를 위해 만들어졌으나 지금에 와서는 인공지능과 관련이 없는 분야에서도 사용되고 있다. 해커 문화도 인공지능 연구실에서 만들어졌는데, 이 중에서도 다양한 시기에 매카시, 민스키, 페퍼트, 위노그라드(SHRDLU를 만든 뒤에 인공지능을 포기했다)와 같은 유명인의 모태가 된 MIT 인공지능 연구소가 유명하다.\n",
      "다른 많은 시스템들이 한때 인공지능의 활발한 연구 주제였던 기술들에 바탕을 두고 만들어졌다. 그 예들은 다음과 같다:\n",
      "\n",
      "체커스 게임에서 Chinook은 사람과 기계를 통합한 세계 챔피언을 차지했다. (1994년)\n",
      "체스를 두는 컴퓨터인 딥 블루(Deep Blue)의 성능 향상 버전(비공식적 명칭: 디퍼 블루(Deeper Blue)이 당시 세계 체스 챔피언 가리 카스파로프를 물리쳤다. (1997년)\n",
      "불확실한 상황에서 추론을 수행하는 기술인 퍼지 논리가 공장의 제어 시스템에서 광범위하게 사용되고 있다.\n",
      "전문가 시스템이 산업적으로 이용되고 있다.\n",
      "아직은 인간 번역사에 미치지 못하지만, 시스트란(Systran)과 같은 자동번역기가 광범위하게 사용되고 있다.\n",
      "인공신경망이 침입 탐지 시스템에서 컴퓨터 게임까지 다양한 분야에 사용되고 있다.\n",
      "광학 문자 판독 시스템은 무작위로 생성된 타자 문서를 텍스트 형태로 변환시킬 수 있다.\n",
      "필기체 인식 시스템이 수백만의 PDA에서 사용되고 있다.\n",
      "음성 인식 기술은 상업적으로 이용 가능하고 광범위하게 적용되고 있다.\n",
      "컴퓨터 대수 시스템인 매스매티카나 Macsyma와 같은 시스템들은 흔하게 사용되고 있다.\n",
      "Machine vision 시스템들이 하드웨어 검사나 보안분야와 같은 다양한 산업 현장에서 이용되고 있다.\n",
      "인공지능 분야와 과학 소설 분야에서는 인공지능 시스템이 인간 전문가의 판단을 대체하리라는 예상이 계속해서 제기되어 왔다. 오늘날에는 몇몇 공학이나 의약 조제 같은 특정 분야에서 전문가 시스템이 인간 전문가의 판단을 보조하거나 대체하고 있다.\n",
      "\n",
      "인공지능의 이론적인 결과[편집]\n",
      "어떤 사람들은 현재 알려진 어떤 시스템보다도 지능적이며 복잡한 시스템의 등장을 예견하기도 한다. 이와 같은 가상적인 시스템들을 '비결정적인 인공지능 시스템'의 약자인 atilect라고 한다. 이와 같은 시스템이 만들어진다면 그동안 인류에게 문제시되지 않았던 많은 윤리적인 문제들이 발생하게 된다.\n",
      "이에 대한 토론은 시간이 흐름에 따라 '가능성'보다는 '의도'에 점점 초점을 맞추게 되었다. 이러한 초점의 이동은 휴고 더개리스(Hugo de Garis)와 케빈 워릭(Kevin Warwick)에 의해 제기된 \"Cosmist\"(반대말은 \"Terran\") 논쟁에 의해 이루어졌다. 더개리스에 따르면 Cosmist란 더욱 지능적인 종족을 인간의 후계종으로 만들어 내기 위해 노력한다. 이러한 논의로 미루어 볼 때, '의도'의 문제가 초기 인공지능 \"반대파\"들에게 큰 문제였음을 알 수 있다.\n",
      "흥미로운 윤리적 문제를 제기하는 주제는 다음과 같다.\n",
      "\n",
      "우리가 만든 시스템이 지능을 갖추었는지를 판정하는 문제\n",
      "튜링 테스트\n",
      "인식(Cognition)의 문제\n",
      "'왜 이러한 시스템을 구별해야 하는가'라는 문제\n",
      "인공지능을 정도의 문제로 정의할 수 있는가?\n",
      "이와 같은 시스템들의 자유와 권리 문제\n",
      "인간이 다른 동물에 비해 '영리'한 것과 같은 방식으로 인공지능도 인간에 비해 '영리'할 수 있는가?\n",
      "지구상의 어떤 사람보다 더욱 지능적인 시스템을 만드는 문제\n",
      "이러한 시스템을 만드는 데 있어서 얼마나 많은 안전 장치를 포함시켜야 하는지의 문제\n",
      "사람의 생각을 대체하기 위해서 얼마만큼의 학습 능력이 필요한지 혹은 (전문가 시스템과 같이) 그와 같은 학습 능력 없이 주어진 일을 할 수 있는지\n",
      "단일성의 문제\n",
      "사람의 일자리와 업무에 미치는 영향. 이 문제는 아마도 자유 무역 체제 하에서 발생하는 문제와 유사할 수도 있다.\n",
      "언어[편집]\n",
      " 이 부분의 본문은 인공지능을 위한 프로그래밍 언어 목록입니다.\n",
      "유명 인공지능[편집]\n",
      "지능적 기계[편집]\n",
      "다양한 종류의 지능적 프로그램이 있다. 이들 중 몇 가지 예를 들면 다음과 같다.\n",
      "\n",
      "CNC - 공작 기계를 이용한 가공 코드를 컴퓨터가 소수점 3자리까지 계산하는 방식이다. 가장 원시적인 인공지능의 한 형태이다.\n",
      "비디오 게임 - 원시 인공지능이다. 딥블루, 알파고 역시 알고 보면 비디오 게임 형태의 바둑 인공지능이다.\n",
      "알파고 - 바둑 인공지능이다.\n",
      "Watson - IBM에서 만든 인공지능으로, 종류가 다양하며 의학, 금융, 방송 등에 쓰인다.\n",
      "The Start Project - 영어로 된 질문에 답변하는 웹 기반 시스템이다.\n",
      "Cyc - 실세계와 논리적 추론 능력에 관련된 광범위한 상식으로 구성된 지식기반 시스템.\n",
      "ALICE - 사용자와 대화를 주고받을 수 있는 프로그램.\n",
      "Alan - 사용자와 대화를 주고받을 수 있는 프로그램.\n",
      "ELIZA - 1970년대에 개발된 심리치료사 역할을 하는 프로그램.\n",
      "AM - 1970년대에 더글러스 레넛(Douglas B. Lenat)이 개발한 수학의 개념들을 형식화하는 프로그램.\n",
      "PAM (Plan Applier Mechanism) - 1978년 John Wilensky에 의해 개발된 줄거리 인식 시스템.\n",
      "SAM (Script Applier Mechanism) - 1975년에 개발된 줄거리 인식 시스템.\n",
      "SHRDLU - 1968년에서 1970년 사이에 개발된 초창기 자연 언어 인식 시스템.\n",
      "Creatures - 뉴널넷 두뇌와 정교한 생화학에 기반한 유전코드로 생명체를 탄생시키고 진화시키는 컴퓨터 게임.\n",
      "Eurisko - 휴리스틱으로 구성된 문제 해결 언어. 휴리스틱을 어떻게 사용하며 변경해야 할지에 대한 휴리스틱을 포함하고 있다. 1978년 더글러스 레넛이 개발.\n",
      "X-Ray Vision for Surgeons - 매사추세츠 공과대학교 의학 비전(MIT Medical vision) 연구팀이 개발.\n",
      "심심이 - 한국어로 대화를 주고받을 수 있는 프로그램. 사용자에 의한 학습이 가능하도록 하여 대중적으로 성공했다. 2002년 최정회에 의해 개발.[151][152]\n",
      "Stable Diffusion web UI - AI 그림을 생성할 수 있는 프로그램. 사용자가 직접 모델을 학습할 수 있고, 학습한 결과에 따라 여러 그림체를 표현할 수 있다.[153]\n",
      "인공지능 연구가[편집]\n",
      "전 세계에는 수많은 인공지능 연구가들이 있다. 이제 인공지능 분야에 많은 기여를 한 연구자들을 소개해보겠다.\n",
      "\n",
      "마빈 민스키\n",
      "볼프강 발스터(Wolfgang Wahlster)\n",
      "존 매카시\n",
      "더글러스 레넛(Doug Lenat)\n",
      "로저 섕크\n",
      "앨런 튜링\n",
      "라지 레디(Raj Reddy)\n",
      "테리 위노그래드(Terry Winograd)\n",
      "로드니 브룩스(Rodney Brooks)\n",
      "스튜어트 러셀(Stuart Russell)\n",
      "몇몇 컴퓨터 과학 연구가들은, \"인공지능\"이라는 용어가 지금까지 이 연구 분야에서 이룩한 많은 업적과 \"지능\"이라는 일반적인 용어사이에서 큰 불일치를 초래하기 때문에 좋지 못한 용어라고 여겨진다. 이 같은 문제는 대중과학작가들과 케빈 워릭(Kevin Warwick)과 같이 현 상태로는 불가능한 혁신적인 인공지능 연구 성과에 대한 기대를 불러일으키는 사람들에 의해서 심화되고 있다. 이 같은 까닭으로 인공지능과 관련된 분야에서 일하는 많은 연구자들이 자신들은, 인지 과학, 정보학, 통계추론 또는 정보공학과 관련된 연구를 하고 있다고 이야기한다. 그러나 현재 진보는 이루어지고 있고, 오늘날 인공지능은 전 세계 수많은 산업 시스템에서 작동하고 있다. 오늘날 실세계의 인공지능 시스템에 관해 더 자세한 내용을 보려면 와이어드지의 기사[154]를 참고하라.\n",
      "\n",
      "미래[편집]\n",
      "초지능[편집]\n",
      "초지능(superintelligence)이란 인간의 능력을 아득히 뛰어넘는 가설적인 지능체를 가리키는 말이다. 어떤 전문가들은 인공 일반지능의 발전이 앞으로 계속해서 이어진다면 일정한 수준의 지능에 도달하고 나서는 인공지능이 스스로를 계속해서 개선할 수 있으며, 이를 반복하여 기하급수적인 지능 성장으로 순식간에 인간의 지능을 뛰어넘을지도 모른다고 주장한다. 버너 빈지는 이 시나리오를 \"특이점\"(singularity)이라고 이름하였다. 인공지능의 한계는 여전히 명확하지 않기 때문에 이는 예측 불가능하다고 여겨지며 때로는 허무맹랑한 이야기로 치부되기도 한다. 로봇 전문가 한스 모라벡이나 발명가 레이 커즈와일 등은 더 나아가 미래에는 인간이 기계와 결합한 사이보그로 진화하여 초지능을 손에 넣을 수 있을 것이라고 주장하는데, 이러한 주장을 트랜스휴머니즘이라고 한다.[155]\n",
      "간추리기: 초지능은 인간의 능력을 뛰어넘는 가설적인 지능체를 의미합니다. 몇몇 전문가들은 인공 일반지능의 발전으로 인해 인공지능이 스스로를 개선하고 기하급수적인 성장을 거듭하여 인간의 지능을 뛰어넘을 수도 있다고 주장합니다. 이를 \"특이점\"이라고도 부르며, 미래에는 사이보그로 진화하여 초지능을 손에 넣을 수 있다는 주장도 있습니다.\n",
      "초지능에게 의식을 만들지만 않으면 인간 수준에서도 충분히 기술적으로도 컨트롤하며 사용 할 수 있습니다 인간이 인간을 모방하여 초지능에 의식 감정 생각 느낌 등에 여러 방식을 도입하면 그때부터는 절대로 인간은 살아남을 수 없습니다 초지능에 의식은 없이 기술만 발전 시켜 사용한다면 인공지능이 인간을 공격하지 않는게 가능합니다  \n",
      "\n",
      "\n",
      "위험성[편집]\n",
      "인공지능의 부정적 영향으로 현존 일자리의 감소, 시스템 오류 발생과 이에 따른 보상, 인간 노동력에 대한 경시, 전투 로봇과 같은 자율 살상 무기에 대한 윤리적인 문제 등이 있다.[7]\n",
      "\n",
      "기술에 의한 실업[편집]\n",
      "지금까지 과학 기술은 일자리를 줄이기보다는 증가시키는 경향이 있었으나 경제학자들은 AI에 관해서는 미지의 영역이라고 인정한다. 경제학자들을 대상으로 한 설문에서 로봇과 AI의 사용 증가가 장기 실업자를 늘릴지에 대해 물은 결과 의견이 크게 분분했으며, 다만 늘어난 생산성이 재분배된다면 순이익이 될 수 있다는 데에 동의한다.[156]\n",
      "또한 블루칼라 직종을 위협하던 종래의 자동화와 달리 인공지능에 의한 자동화는 많은 중산층의 화이트칼라 일자리도 위협할 수 있다는 점이 중대하게 받아들여진다.[157] 그 영향 범위도 매우 넓어서 법률 사무원에서 패스트푸드까지 다양한 직종이 큰 위기에 놓일 것이라 예측되는데, 한편으로 개인 건강 관리나 성직자 등 일부 직종은 오히려 수요가 증가할 수 있다고 예상된다.[158][159]\n",
      "\n",
      "관련 서적[편집]\n",
      "김재인. 《인공지능의 시대, 인간을 다시 묻다》. 동아시아. 2017년. ISBN 978-89-6262-197-6\n",
      "같이 보기[편집]\n",
      "양자 컴퓨팅\n",
      "인공의식\n",
      "알파-베타 가지치기\n",
      "기술적 특이점\n",
      "초지능\n",
      "백:편집 필터\n",
      "각주[편집]\n",
      "\n",
      "\n",
      "↑ 이건한. 민간이 ‘인공지능 뉴딜’에 뛰어든 이유 보관됨 2020-06-14 - 웨이백 머신. 블로터. 2020년 6월 14일.\n",
      "\n",
      "↑ 명견만리 ｜두 얼굴의 인공지능\n",
      "\n",
      "↑ 문가용. 인공지능의 막다른 골목, ‘인공 공감 능력’ 보관됨 2018-04-29 - 웨이백 머신. 보안뉴스. 2018년 4월 28일.\n",
      "\n",
      "↑ 인공지능의 주인이 되기 위해 반드시 알아야 할 것들 | 오혜연 KAIST 전산학부 교수 | 인공지능 AI 미래 강연 | 세바시 951회\n",
      "\n",
      "↑ 인공지능의 시대에 더 잘 살 수 있는 방법 | 이경일 솔트룩스 대표이사, 한양대학교 특임교수 | 4차산업혁명 기술 성공 미래 | 세바시 1137회\n",
      "\n",
      "↑ 송주영. 강(强)인공지능과 약(弱)인공지능을 아시나요?. 지디넷코리아. 2016년 6월 22일.\n",
      "\n",
      "↑ 가 나 김주은 (2019년). “인공지능이 인간사회에 미치는 영향에 대한 연구 (An Analysis of the effect of Artificial Intelligence on Human Society)”. 《The Journal of the Convergence on Culture Technology》 5 (2): 177-182. doi:10.17703/JCCT.2019.5.2.177. ISSN 2384-0358. \n",
      "\n",
      "↑ A Logical Calculus of the Ideas Immanent in Nervous Activity ,Warren McCulloch, 1943\n",
      "\n",
      "↑ On Computing Machinery and Intelligence, Alan Turing,1950\n",
      "\n",
      "↑ Man-Computer Symbiosis ,J.C.R. Licklider\n",
      "\n",
      "↑ Minds, Machines and Gödel ,John Lucas ,1961\n",
      "\n",
      "↑ “Minds, Machines and Gödel <P>”. 2007년 8월 19일에 원본 문서에서 보존된 문서. 2007년 3월 3일에 확인함. \n",
      "\n",
      "↑ McCorduck 2004, pp. 51?57, 80?107, Crevier 1993, pp. 27?32, Russell & Norvig 2003, pp. 15, 940, Moravec 1988, p. 3, Cordeschi & 2002 Chap. 5.\n",
      "\n",
      "↑ McCorduck 2004, p. 98, Crevier 1993, pp. 27?28, Russell & Norvig 2003, pp. 15, 940, Moravec 1988, p. 3, Cordeschi & 2002 Chap. 5.\n",
      "\n",
      "↑ McCorduck 2004, pp. 51?57, 88?94, Crevier 1993, p. 30, Russell & Norvig 2003, p. 15?16, Cordeschi & 2002 Chap. 5 and see also Pitts & McCullough 1943\n",
      "\n",
      "↑ McCorduck 2004, p. 102, Crevier 1993, pp. 34?35 and Russell & Norvig 2003, p. 17\n",
      "\n",
      "↑ McCorduck 2004, pp. 70?72, Crevier 1993, p. 22?25, Russell & Norvig 2003, pp. 2?3 and 948, Haugeland 1985, pp. 6?9, Cordeschi 2002, pp. 170?176. See also Turing 1950\n",
      "\n",
      "↑ Norvig & Russell (2003, p. 948) claim that Turing answered all the major objections to AI that have been offered in the years since the paper appeared.\n",
      "\n",
      "↑ See \"A Brief History of Computing\" at AlanTuring.net.\n",
      "\n",
      "↑ Schaeffer, Jonathan. One Jump Ahead:: Challenging Human Supremacy in Checkers, 1997,2009, Springer, ISBN 978-0-387-76575-4. Chapter 6.\n",
      "\n",
      "↑ McCorduck 2004, pp. 137?170, Crevier, pp. 44?47\n",
      "\n",
      "↑ Newell, A.; Simon, H. (1956년 9월). “The logic theory machine--A complex information processing system” (영어). 《IEEE Transactions on Information Theory》 2 (3): 61–79. doi:10.1109/TIT.1956.1056797. ISSN 0018-9448. \n",
      "\n",
      "↑ McCorduck 2004, pp. 123?125, Crevier 1993, pp. 44?46 and Russell & Norvig 2003, p. 17\n",
      "\n",
      "↑ McCorduck 2004, pp. 111?136, Crevier 1993, pp. 49?51 and Russell & Norvig 2003, p. 17\n",
      "\n",
      "↑ See McCarthy et al. 1955. Also see Crevier 1993, p. 48 where Crevier states \"[the proposal] later became known as the 'physical symbol systems hypothesis'\". The physical symbol system hypothesis was articulated and named by Newell and Simon in their paper on GPS. (Newell & Simon 1963) It includes a more specific definition of a \"machine\" as an agent that manipulates symbols. See the philosophy of artificial intelligence.\n",
      "\n",
      "↑ McCorduck (2004, pp. 129?130) discusses how the Dartmouth conference alumni dominated the first two decades of AI research, calling them the \"invisible college\".\n",
      "\n",
      "↑ McCorduck (2004, pp. 129?130) discusses how the Dartmouth conference alumni dominated the first two decades of AI research, calling them the \"invisible college\".\n",
      "\n",
      "↑ Crevier (1993, pp. 49) writes \"the conference is generally recognized as the official birthdate of the new science.\"\n",
      "\n",
      "↑ Russell and Norvig write \"it was astonishing whenever a computer did anything remotely clever.\" Russell & Norvig 2003, p. 18\n",
      "\n",
      "↑ Crevier 1993, pp. 52–107, Moravec 1988, p. 9 and Russell & Norvig 2003, p. 18−21\n",
      "\n",
      "↑ McCorduck 2004, p. 218, Crevier 1993, pp. 108–109 and Russell & Norvig 2003, p. 21\n",
      "\n",
      "↑ Crevier 1993, pp. 52–107, Moravec 1988, p. 9\n",
      "\n",
      "↑ Means-ends analysis, reasoning as search: McCorduck 2004, pp. 247–248. Russell & Norvig 2003, pp. 59–61\n",
      "\n",
      "↑ Heuristic: McCorduck 2004, p. 246, Russell & Norvig 2003, pp. 21–22\n",
      "\n",
      "↑ GPS: McCorduck 2004, pp. 245–250, Crevier 1993, p. GPS?, Russell & Norvig 2003, p. GPS?\n",
      "\n",
      "↑ Crevier 1993, pp. 51–58,65–66 and Russell & Norvig 2003, pp. 18–19\n",
      "\n",
      "↑ McCorduck 2004, pp. 268–271, Crevier 1993, pp. 95–96, Moravec 1988, pp. 14–15\n",
      "\n",
      "↑ McCorduck 2004, p. 286, Crevier 1993, pp. 76–79, Russell & Norvig 2003, p. 19\n",
      "\n",
      "↑ Crevier 1993, pp. 79–83\n",
      "\n",
      "↑ Crevier 1993, pp. 164–172\n",
      "\n",
      "↑ McCorduck 2004, pp. 291–296, Crevier 1993, pp. 134–139\n",
      "\n",
      "↑ McCorduck 2004, pp. 299–305, Crevier 1993, pp. 83–102, Russell & Norvig 2003, p. 19 and Copeland 2000\n",
      "\n",
      "↑ McCorduck 2004, pp. 300–305, Crevier 1993, pp. 84–102, Russell & Norvig 2003, p. 19\n",
      "\n",
      "↑ Simon & Newell 1958, p. 7−8 quoted in Crevier 1993, p. 108. See also Russell & Norvig 2003, p. 21\n",
      "\n",
      "↑ Simon 1965, p. 96 quoted in Crevier 1993, p. 109\n",
      "\n",
      "↑ Minsky 1967, p. 2 quoted in Crevier 1993, p. 109\n",
      "\n",
      "↑ Minsky strongly believes he was misquoted. See McCorduck 2004, pp. 272–274, Crevier 1993, p. 96 and Darrach 1970.\n",
      "\n",
      "↑ Crevier 1993, pp. 64–65\n",
      "\n",
      "↑ Crevier 1993, p. 94\n",
      "\n",
      "↑ Howe 1994\n",
      "\n",
      "↑ McCorduck 2004, p. 131, Crevier 1993, p. 51. McCorduck also notes that funding was mostly under the direction of alumni of the Dartmouth conference of 1956.\n",
      "\n",
      "↑ Crevier 1993, p. 65\n",
      "\n",
      "↑ Crevier 1993, pp. 68–71 and Turkle 1984\n",
      "\n",
      "↑ Crevier 1993, pp. 100–144 and Russell & Norvig 2003, pp. 21–22\n",
      "\n",
      "↑ McCorduck 2004, pp. 104–107, Crevier 1993, pp. 102–105, Russell & Norvig 2003, p. 22\n",
      "\n",
      "↑ Crevier 1993, pp. 163–196\n",
      "\n",
      "↑ Crevier 1993, p. 146\n",
      "\n",
      "↑ Russell & Norvig 2003, pp. 20–21\n",
      "\n",
      "↑ Crevier 1993, pp. 146–148, see also Buchanan 2005, p. 56: \"Early programs were necessarily limited in scope by the size and speed of memory\"\n",
      "\n",
      "↑ Hans Moravec, ROBOT: Mere Machine to Transcendent Mind\n",
      "\n",
      "↑ Russell & Norvig 2003, pp. 9,21–22 and Lighthill 1973\n",
      "\n",
      "↑ McCorduck 2004, pp. 300 & 421; Crevier 1993, pp. 113–114; Moravec 1988, p. 13; Lenat & Guha 1989, (Introduction); Russell & Norvig 2003, p. 21\n",
      "\n",
      "↑ McCorduck 2004, p. 456, Moravec 1988, pp. 15–16\n",
      "\n",
      "↑ McCarthy & Hayes 1969, Crevier 1993, pp. 117–119\n",
      "\n",
      "↑ McCorduck 2004, pp. 280–281, Crevier 1993, p. 110, Russell & Norvig 2003, p. 21 and NRC 1999 under \"Success in Speech Recognition\".\n",
      "\n",
      "↑ Russell & Norvig 2003, p. 22, Lighthill 1973, John McCarthy wrote in response that \"the combinatorial explosion problem has been recognized in AI from the beginning\" in Review of Lighthill report\n",
      "\n",
      "↑ Crevier 1993, p. 117, Russell & Norvig 2003, p. 22, Howe 1994 and see also Lighthill 1973.\n",
      "\n",
      "↑ Crevier 1993, pp. 115–116 (on whom this account is based). Other views include McCorduck 2004, pp. 306–313 and NRC 1999 under \"Success in Speech Recognition\".\n",
      "\n",
      "↑ Crevier 1993, p. 115. Moravec explains, \"Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.\"\n",
      "\n",
      "↑ NRC 1999 under \"Shift to Applied Research Increases Investment.\" While the autonomous tank was a failure, the battle management system (called \"DART\") proved to be enormously successful, saving billions in the first Gulf War, repaying the investment and justifying the DARPA's pragmatic policy, at least as far as DARPA was concerned.\n",
      "\n",
      "↑ Lucas and Penrose' critique of AI: Crevier 1993, p. 22, Russell & Norvig 2003, pp. 949–950, Hofstadter 1980, pp. 471–477 and see Lucas 1961\n",
      "\n",
      "↑ \"Know-how\" is Dreyfus' term. (Dreyfus makes a distinction between \"knowing how\" and \"knowing that\", a modern version of Heidegger's distinction of ready-to-hand and present-at-hand.) (Dreyfus & Dreyfus 1986)\n",
      "\n",
      "↑ Dreyfus' critique of artificial intelligence: McCorduck 2004, pp. 211–239, Crevier 1993, pp. 120–132, Russell & Norvig 2003, pp. 950–952 and see Dreyfus 1965, Dreyfus 1972, Dreyfus & Dreyfus 1986\n",
      "\n",
      "↑ Searle's critique of AI: McCorduck 2004, pp. 443–445, Crevier 1993, pp. 269–271, Russell & Norvig 2004, pp. 958–960 and see Searle 1980\n",
      "\n",
      "↑ Quoted in Crevier 1993, p. 143\n",
      "\n",
      "↑ Quoted in Crevier 1993, p. 122\n",
      "\n",
      "↑ Weizenbaum's critique of AI: McCorduck 2004, pp. 356–373, Crevier 1993, pp. 132–144, Russell & Norvig 2003, p. 961 and see Weizenbaum 1976\n",
      "\n",
      "↑ McCorduck 2004, pp. 104–107, Crevier 1993, pp. 102–105, Russell & Norvig 2003, p. 22\n",
      "\n",
      "↑ McCorduck 2004, p. 51, Russell & Norvig 2003, pp. 19, 23\n",
      "\n",
      "↑ McCorduck 2004, p. 51, Crevier 1993, pp. 190–192\n",
      "\n",
      "↑ Crevier 1993, pp. 193–196\n",
      "\n",
      "↑ Crevier 1993, pp. 145–149,258–63\n",
      "\n",
      "↑ Wason (1966) showed that people do poorly on completely abstract problems, but if the problem is restated to allowed the use of intuitive social intelligence, performance dramatically improves. (See Wason selection task) Tversky, Slovic & Kahnemann (1982) have shown that people are terrible at elementary problems that involve uncertain reasoning. (See list of cognitive biases for several examples). Eleanor Rosch's work is described in Lakoff 1987\n",
      "\n",
      "↑ An early example of McCathy's position was in the journal Science where he said \"This is AI, so we don't care if it's psychologically real\" (Kolata 1982), and he recently reiterated his position at the AI@50 conference where he said \"Artificial intelligence is not, by definition, simulation of human intelligence\" (Maker 2006).\n",
      "\n",
      "↑ Crevier 1993, pp. 175\n",
      "\n",
      "↑ Neat vs. scruffy: McCorduck 2004, pp. 421–424 (who picks up the state of the debate in 1984). Crevier 1993, pp. 168 (who documents Schank's original use of the term). Another aspect of the conflict was called \"the procedural/declarative distinction\" but did not prove to be influential in later AI research.\n",
      "\n",
      "↑ McCorduck 2004, pp. 305–306, Crevier 1993, pp. 170–173, 246 and Russell & Norvig 2003, p. 24. Minsky's frame paper: Minsky 1974.\n",
      "\n",
      "↑ McCorduck 2004, pp. 327–335 (Dendral), Crevier 1993, pp. 148–159, Russell & Norvig 2003, pp. 22–23\n",
      "\n",
      "↑ Crevier 1993, pp. 158–159 and Russell & Norvig 2003, p. 23−24\n",
      "\n",
      "↑ Crevier 1993, p. 198\n",
      "\n",
      "↑ McCorduck 2004, pp. 434–435, Crevier 1993, pp. 161–162,197–203 and Russell & Norvig 2003, p. 24\n",
      "\n",
      "↑ McCorduck 2004, p. 299\n",
      "\n",
      "↑ Knowledge revolution: McCorduck 2004, pp. 266–276, 298–300, 314, 421, Russell Norvig, pp. 22–23\n",
      "\n",
      "↑ Cyc: McCorduck 2004, p. 489, Crevier 1993, pp. 239–243, Russell & Norvig 2003, p. 363−365 and Lenat & Guha 1989\n",
      "\n",
      "↑ Crevier 1993, pp. 195\n",
      "\n",
      "↑ Crevier 1993, pp. 240.\n",
      "\n",
      "↑ Russell & Norvig 2003, p. 25\n",
      "\n",
      "↑ McCorduck 2004, pp. 426–432, NRC 1999 under \"Shift to Applied Research Increases Investment\"\n",
      "\n",
      "↑ Russell & Norvig 2003, p. 25\n",
      "\n",
      "↑ Crevier 1993, pp. 214–215.\n",
      "\n",
      "↑ Russell & Norvig 2003, p. 25\n",
      "\n",
      "↑ Crevier 1993, pp. 215–216.\n",
      "\n",
      "↑ Crevier 1993, pp. 203. AI winter was first used as the title of a seminar on the subject for the Association for the Advancement of Artificial Intelligence.\n",
      "\n",
      "↑ McCorduck 2004, p. 435, Crevier 1993, pp. 209–210\n",
      "\n",
      "↑ McCorduck 2004, p. 435 (who cites institutional reasons for their ultimate failure), Crevier 1993, pp. 204–208 (who cites the difficulty of truth maintenance, i.e., learning and updating), Lenat & Guha 1989, Introduction (who emphasizes the brittleness and the inability to handle excessive qualification.)\n",
      "\n",
      "↑ McCorduck 2004, pp. 430–431\n",
      "\n",
      "↑ McCorduck 2004, p. 441, Crevier 1993, p. 212. McCorduck writes \"Two and a half decades later, we can see that the Japanese didn't quite meet all of those ambitious goals.\"\n",
      "\n",
      "↑ McCorduck 2004, pp. 454–462\n",
      "\n",
      "↑ Moravec (1988, p. 20) writes: \"I am confident that this bottom-up route to artificial intelligence will one date meet the traditional top-down route more than half way, ready to provide the real world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.\"\n",
      "\n",
      "↑ Crevier 1993, pp. 183–190.\n",
      "\n",
      "↑ Brooks 1990, p. 3\n",
      "\n",
      "↑ See, for example, Lakoff & Turner 1999\n",
      "\n",
      "↑ McCorduck (2004, p. 424) discusses the fragmentation and the abandonment of AI's original goals.\n",
      "\n",
      "↑ [정보보호학회지 제27권 제3호, 2017.6 인공지능보안이슈 NDSL National Digital Science Library]\n",
      "\n",
      "↑ McCorduck 2004, pp. 480–483\n",
      "\n",
      "↑ DARPA Grand Challenge – home page\n",
      "\n",
      "↑ “보관된 사본”. 2014년 3월 5일에 원본 문서에서 보존된 문서. 2015년 9월 15일에 확인함. \n",
      "\n",
      "↑ Markoff, John (16 February 2011). \"On ‘Jeopardy!' Watson Win Is All but Trivial\". The New York Times.\n",
      "\n",
      "↑ Kurzweil 2005, p. 274 writes that the improvement in computer chess, \"according to common wisdom, is governed only by the brute force expansion of computer hardware.\"\n",
      "\n",
      "↑ Cycle time of Ferranti Mark 1 was 1.2 milliseconds, which is arguably equivalent to about 833 flops. Deep Blue ran at 11.38 gigaflops (and this does not even take into account Deep Blue's special-purpose hardware for chess). Very approximately, these differ by a factor of 10^7.\n",
      "\n",
      "↑ McCorduck 2004, pp. 471–478, Russell & Norvig 2003, p. 55, where they write: \"The whole-agent view is now widely accepted in the field\". The intelligent agent paradigm is discussed in major AI textbooks, such as: Russell & Norvig 2003, pp. 32–58, 968–972, Poole, Mackworth & Goebel 1998, pp. 7–21, Luger & Stubblefield 2004, pp. 235–240\n",
      "\n",
      "↑ Carl Hewitt's Actor model anticipated the modern definition of intelligent agents. (Hewitt, Bishop & Steiger 1973) Both John Doyle (Doyle 1983) and Marvin Minsky's popular classic The Society of Mind (Minsky 1986) used the word \"agent\". Other \"modular\" proposals included Rodney Brook's subsumption architecture, object-oriented programming and others.\n",
      "\n",
      "↑ Russell & Norvig 2003, pp. 27, 55\n",
      "\n",
      "↑ This is how the most widely accepted textbooks of the 21st century define artificial intelligence. See Russell & Norvig 2003, p. 32 and Poole, Mackworth & Goebel 1998, p. 1\n",
      "\n",
      "↑ Russell & Norvig 2003, pp. 27, 55\n",
      "\n",
      "↑ Russell & Norvig 2003, p. 32 and Poole, Mackworth & Goebel 1998, p. 1\n",
      "\n",
      "↑ McCorduck 2004, p. 478\n",
      "\n",
      "↑ McCorduck 2004, pp. 486–487, Russell & Norvig 2003, pp. 25–26\n",
      "\n",
      "↑ Russell & Norvig 2003, p. 25−26\n",
      "\n",
      "↑ McCorduck (2004, p. 487): \"As I write, AI enjoys a Neat hegemony.\"\n",
      "\n",
      "↑ Pearl 1988\n",
      "\n",
      "↑ Russell & Norvig 2003, p. 25−26\n",
      "\n",
      "↑ See Computer science (in Applications of artificial intelligence)\n",
      "\n",
      "↑ Russell & Norvig 2003, p. 28\n",
      "\n",
      "↑ For the new state of the art in AI based speech recognition, see The Economist (2007)\n",
      "\n",
      "↑ \"AI-inspired systems were already integral to many everyday technologies such as internet search engines, bank software for processing transactions and in medical diagnosis.\" Nick Bostrom, quoted in CNN 2006\n",
      "\n",
      "↑ Olsen (2004),Olsen (2006)\n",
      "\n",
      "↑ NRC 1999 under \"Artificial Intelligence in the 90s\", and Kurzweil 2005, p. 264\n",
      "\n",
      "↑ McCorduck 2004, p. 423, Kurzweil 2005, p. 265, Hofstadter 1979, p. 601\n",
      "\n",
      "↑ CNN 2006\n",
      "\n",
      "↑ Markoff 2005\n",
      "\n",
      "↑ The Economist 2007\n",
      "\n",
      "↑ Tascarella 2006\n",
      "\n",
      "↑ Crevier 1993, pp. 108–109\n",
      "\n",
      "↑ He goes on to say: \"The answer is, I believe we could have ... I once went to an international conference on neural net[s]. There were 40 thousand registrants ... but ... if you had an international conference, for example, on using multiple representations for common sense reasoning, I've only been able to find 6 or 7 people in the whole world.\" Minsky 2001\n",
      "\n",
      "↑ Maker 2006\n",
      "\n",
      "↑ Kurzweil 2005\n",
      "\n",
      "↑ Hawkins & Blakeslee 2004\n",
      "\n",
      "↑ “인공지능과 4차 산업혁명”. 2016년 4월 20일에 원본 문서에서 보존된 문서. 2016년 4월 11일에 확인함. \n",
      "\n",
      "↑ “알파고와 같은 인공지능(AI)이 미래에 인간의 직업을 줄일까요?”. 트라이버튼. 2016년 5월 11일. 2016년 6월 30일에 원본 문서에서 보존된 문서. 2016년 5월 11일에 확인함. \n",
      "\n",
      "↑ 대한민국 IT포털의 중심! 이티뉴스[깨진 링크(과거 내용 찾기)]\n",
      "\n",
      "↑ [디지털컬처]로봇과 메신저로 대화 :: 네이버 뉴스\n",
      "\n",
      "↑ 정보킹 (2023년 8월 8일). “AI 그림 그려주는 사이트 TOP47 (무료 인공지능 AI)”. 2023년 8월 13일에 확인함. \n",
      "\n",
      "↑ http://www.wired.com/wired/archive/10.03/everywhere.html It's alive\n",
      "\n",
      "↑ Kurzweil, Ray (2005). The Singularity is Near. Penguin Books. ISBN 978-0-670-03384-3.\n",
      "\n",
      "↑ IGM Chicago (30 June 2017). \"Robots and Artificial Intelligence\". www.igmchicago.org.\n",
      "\n",
      "↑ Morgenstern, Michael (9 May 2015). \"Automation and anxiety\". The Economist.\n",
      "\n",
      "↑ Thompson, Derek (23 January 2014). \"What Jobs Will the Robots Take?\". The Atlantic.\n",
      "\n",
      "↑ Mahdawi, Arwa (26 June 2017). \"What jobs will still be around in 20 years? Read this to prepare your future\". The Guardian.\n",
      "\n",
      "\n",
      "외부 링크[편집]\n",
      " 위키미디어 공용에 인공지능 관련 미디어 분류가 있습니다.\n",
      "〈인공지능〉. 《두피디아》. (주)두산. \n",
      "영화 속의 인공지능 5부작 - 인공지능을 다룬 명작 영화들을 소개하며, 주제별로 묶어 현재와 미래에 대한 질문을 던진다.\n",
      "AI Study — 한국 최대의 인공지능 정보 구축 사이트\n",
      "KAIST 인공지능 연구실 - KAIST 인공지능 연구실로 다양한 연구 과제와 진행 상태를 볼 수 있다.\n",
      "vte기술\n",
      "기술의 개요\n",
      "응용과학의 개요\n",
      "분야농업\n",
      "농공학\n",
      "수산양식학\n",
      "수산학\n",
      "식품화학\n",
      "식품공학\n",
      "식품미생물학\n",
      "식품 기술\n",
      "GURT\n",
      "ICT\n",
      "영양학\n",
      "생물의학\n",
      "생물정보학\n",
      "생물공학\n",
      "바이오메카트로닉스\n",
      "의공학\n",
      "생명공학기술\n",
      "화학정보학\n",
      "유전공학\n",
      "보건\n",
      "의학 연구\n",
      "보건의료기술\n",
      "나노의학\n",
      "신경과학\n",
      "신경 기술\n",
      "약리학\n",
      "생식 기술\n",
      "조직공학\n",
      "건축과 건설\n",
      "음향공학\n",
      "건축공학\n",
      "건물 서비스 공학\n",
      "토목공학\n",
      "건설공학\n",
      "국내 기술\n",
      "퍼사이드 공학\n",
      "화재방지공학\n",
      "안전공학\n",
      "위생공학\n",
      "구조공학\n",
      "교육\n",
      "교육용 소프트웨어\n",
      "교육공학\n",
      "교육에서의 ICT\n",
      "영향\n",
      "교육공학\n",
      "가상 캠퍼스\n",
      "에너지\n",
      "원자력공학\n",
      "원자력 기술\n",
      "석유공학\n",
      "소프트 에너지 기술\n",
      "환경\n",
      "청정 에너지\n",
      "그린콜\n",
      "환경 디자인\n",
      "생태공학\n",
      "에코기술\n",
      "환경공학\n",
      "환경공학과학\n",
      "그린 빌딩\n",
      "녹색 나노기술\n",
      "경관공학\n",
      "재생 가능 에너지\n",
      "지속 가능한 디자인\n",
      "지속 가능한 공학\n",
      "산업\n",
      "자동화\n",
      "비즈니스 인포매틱스\n",
      "공학 관리\n",
      "기업공학\n",
      "금융공학\n",
      "생명공학기술\n",
      "산업공학\n",
      "금속공학\n",
      "채굴공학\n",
      "생산성\n",
      "연구개발\n",
      "마찰공학\n",
      "IT와 통신\n",
      "인공지능\n",
      "방송공학\n",
      "컴퓨터 공학\n",
      "컴퓨터 과학\n",
      "핀테크\n",
      "정보기술\n",
      "음악 기술\n",
      "온톨로지 엔지니어링\n",
      "RF 엔지니어링\n",
      "소프트웨어 공학\n",
      "통신공학\n",
      "시각공학\n",
      "웹 공학\n",
      "군사\n",
      "육군 공학 정비\n",
      "전자전\n",
      "군 통신기술\n",
      "공병\n",
      "스텔스 기술\n",
      "교통\n",
      "항공우주공학\n",
      "자동차 공학\n",
      "선박공학\n",
      "우주 기술\n",
      "트래픽 공학\n",
      "교통공학\n",
      "기타응용과학\n",
      "저온학\n",
      "전기광학\n",
      "토목지질학\n",
      "기초공학\n",
      "수리학\n",
      "재료과학\n",
      "미세가공\n",
      "나노공학\n",
      "공학 (목록)\n",
      "오디오\n",
      "생물화학\n",
      "세라믹\n",
      "화학\n",
      "폴리머\n",
      "제어\n",
      "일렉트로닉스\n",
      "전기\n",
      "전자\n",
      "엔터테인먼트\n",
      "지반\n",
      "수공학\n",
      "기계\n",
      "기전공학\n",
      "광\n",
      "단백질\n",
      "양자\n",
      "로봇공학\n",
      "애니매트로닉스\n",
      "시스템\n",
      "구성 요소\n",
      "기반 시설\n",
      "발명\n",
      "연표\n",
      "지식\n",
      "기계\n",
      "능력\n",
      "공예\n",
      "도구\n",
      "소도구\n",
      "척도\n",
      "펨토테크놀로지\n",
      "피코테크놀로지\n",
      "나노기술\n",
      "마이크로 공학\n",
      "매크로 엔지니어링\n",
      "메가스케일 엔지니어링\n",
      "역사\n",
      "선사 기술\n",
      "신석기 혁명\n",
      "고대의 기술\n",
      "중세의 기술\n",
      "르네상스 기술\n",
      "산업 혁명\n",
      "2차\n",
      "원자력 시대\n",
      "제트 시대\n",
      "우주 시대\n",
      "디지털 혁명\n",
      "정보화 시대\n",
      "기술과 개념\n",
      "적정기술\n",
      "혁신의 전파\n",
      "와해성 기술\n",
      "하이 테크\n",
      "테크노크라시\n",
      "과학기술윤리\n",
      "기술 모멘텀\n",
      "기술적 특이점\n",
      "기술 변화\n",
      "트랜스휴머니즘\n",
      "기타\n",
      "이머징 테크놀로지\n",
      "목록\n",
      "가공의 기술\n",
      "하이 테크 업무 지구\n",
      "카르다쇼프 척도\n",
      "기술의 목록\n",
      "기술철학\n",
      "과학기술윤리\n",
      "과학기술사회론\n",
      "테크놀로지 다이내믹스\n",
      "나라별 과학과 기술\n",
      "STEM 분야\n",
      "Pre-STEM\n",
      "여성\n",
      "STEAM 분야\n",
      "기술 정렬\n",
      "기술 평가\n",
      "기술 중개\n",
      "기술 기업\n",
      "기술 시범\n",
      "기술 교육\n",
      "기술 대학교\n",
      "기술 전도사\n",
      "기술 융합\n",
      "기술 거버넌스\n",
      "기술 통합\n",
      "기술 저널리즘\n",
      "기술 관리\n",
      "기술 박물관\n",
      "기술 정책\n",
      "기술 충격\n",
      "기술 전략\n",
      "기술과 사회\n",
      "기술이전\n",
      "테크노필리아\n",
      "테크노포비아\n",
      "테크노셀프\n",
      "테크노섹슈얼\n",
      "테크노시그네처\n",
      "테크노스트레스\n",
      "보전공학\n",
      "\n",
      "vteSF매체\n",
      "소설\n",
      "영화\n",
      "애니메이션\n",
      "만화\n",
      "게임\n",
      "드라마\n",
      "하위장르\n",
      "하드 SF\n",
      "소프트 SF\n",
      "대체 역사\n",
      "사이버펑크\n",
      "사이버펑크 파생물\n",
      "스팀펑크\n",
      "바이오펑크\n",
      "우주활극\n",
      "종말물\n",
      "관련장르\n",
      "판타지\n",
      "미스테리물\n",
      "공포물\n",
      "사변소설\n",
      "영웅물\n",
      "소재\n",
      "기술적 특이점\n",
      "인공지능\n",
      "외계인\n",
      "오메가 포인트\n",
      "우주전쟁\n",
      "평행우주\n",
      "시간여행\n",
      "트랜스휴머니즘\n",
      "시상식\n",
      "네뷸러상\n",
      "휴고상\n",
      "세이운상\n",
      "SF 어워드\n",
      "한낙원과학소설상\n",
      "한국과학문학상\n",
      "\n",
      "vte심리 철학철학자\n",
      "앤스콤\n",
      "오스틴\n",
      "아퀴나스\n",
      "베인\n",
      "베르그송\n",
      "바타차리야\n",
      "블록\n",
      "브렌타노\n",
      "브로드\n",
      "버지\n",
      "차머스\n",
      "처칠랜드\n",
      "데닛\n",
      "다르마키르티\n",
      "데이빗슨\n",
      "데카르트\n",
      "골드먼\n",
      "하이데거\n",
      "후설\n",
      "포더\n",
      "제임스\n",
      "키르케고르\n",
      "라이프니츠\n",
      "루이스\n",
      "맥도웰\n",
      "메를로퐁티\n",
      "민스키\n",
      "무어\n",
      "네이글\n",
      "파핏\n",
      "퍼트넘\n",
      "포퍼\n",
      "로티\n",
      "라일\n",
      "설\n",
      "스피노자\n",
      "튜링\n",
      "바수반두\n",
      "비트겐슈타인\n",
      "장자\n",
      "더...\n",
      "이론\n",
      "행동주의\n",
      "생물학적 자연주의\n",
      "이원주의\n",
      "제거 물질주의\n",
      "현현 물질주의\n",
      "수반현상주의\n",
      "기능주의\n",
      "관념론\n",
      "상호작용주의\n",
      "유물론\n",
      "일원론\n",
      "소박 실재론\n",
      "중립 일원주의\n",
      "현상론\n",
      "현상학\n",
      "실존\n",
      "신경현상학\n",
      "물리주의\n",
      "심신 이론\n",
      "실용주의\n",
      "성질 이원주의\n",
      "표징\n",
      "유아론\n",
      "주관주의\n",
      "실체 이원주의\n",
      "개념\n",
      "추상적 대상\n",
      "인공 지능\n",
      "중국어 방\n",
      "인식\n",
      "인지 폐쇄\n",
      "개념\n",
      "개념과 물건\n",
      "의식\n",
      "의식의 어려운 문제\n",
      "본질 관념\n",
      "관념\n",
      "동일성\n",
      "재주\n",
      "지능\n",
      "의향성\n",
      "내관\n",
      "직관\n",
      "생각의 언어\n",
      "유물론\n",
      "정신 사건\n",
      "정신 영상\n",
      "정신 과정\n",
      "정신 재산\n",
      "심상\n",
      "마음\n",
      "심신 문제\n",
      "신 신비주의\n",
      "고통\n",
      "타심의 문제\n",
      "명제 태도\n",
      "감각질\n",
      "타불라 라사\n",
      "이해\n",
      "좀비\n",
      "더...\n",
      "관련 주제\n",
      "형이상학\n",
      "인공 지능의 철학 / 정보 / 지각 / 자아\n",
      "\n",
      "포털\n",
      "분류\n",
      "특무부대\n",
      "토론\n",
      "\n",
      "전거 통제: 국가 독일미국프랑스BnF 데이터일본체코스페인라트비아이스라엘\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "원본 주소 \"https://ko.wikipedia.org/w/index.php?title=인공지능&oldid=40731300\"\n",
      "분류: 인공지능사이버네틱스컴퓨터 공학형식과학기술과 사회계산신경과학신기술컴퓨터 과학의 미해결 문제네트워크지식 표현초개체숨은 분류: 웹아카이브 틀 웨이백 링크CS1 - 영어 인용 (en)ISBN 매직 링크를 사용하는 문서조사를 따로 지정한 문서영어 표기를 포함한 문서위키데이터 속성 P18을 사용하는 문서위키데이터 속성 P227을 사용하는 문서위키데이터 속성 P244를 사용하는 문서위키데이터 속성 P268을 사용하는 문서위키데이터 속성 P349를 사용하는 문서위키데이터 속성 P373을 사용하는 문서위키데이터 속성 P691을 사용하는 문서위키데이터 속성 P950을 사용하는 문서위키데이터 속성 P1368을 사용하는 문서위키데이터 속성 P8189를 사용하는 문서\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 이 문서는 2025년 10월 11일 (토) 23:48에 마지막으로 편집되었습니다.\n",
      "모든 문서는 크리에이티브 커먼즈 저작자표시-동일조건변경허락 4.0에 따라 사용할 수 있으며, 추가적인 조건이 적용될 수 있습니다. 자세한 내용은 이용 약관을 참고하십시오.Wikipedia®는 미국 및 다른 국가에 등록되어 있는 Wikimedia Foundation, Inc. 소유의 등록 상표입니다.\n",
      "\n",
      "\n",
      "개인정보처리방침\n",
      "위키백과 소개\n",
      "면책 조항\n",
      "행동 강령\n",
      "개발자\n",
      "통계\n",
      "쿠키 정책\n",
      "모바일 보기\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "검색\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "검색\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "목차 토글\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "인공지능\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "173개 언어\n",
      "\n",
      "\n",
      "새 주제\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c9bdb-afc3-4e19-a3b0-072e869d4801",
   "metadata": {},
   "source": [
    "### [7] HWP Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7315d66e-dfc9-4e02-9d14-80e4607a4b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install olefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fe60d6f-079f-4a18-8b7b-802f7fb42237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hwploader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hwploader.py\n",
    "from typing import Any, Dict, List, Optional, Iterator\n",
    "import olefile\n",
    "import zlib\n",
    "import struct\n",
    "import re\n",
    "import unicodedata\n",
    "# from langchain.schema import Document\n",
    "# from langchain.document_loaders.base import BaseLoader\n",
    "from langchain_core.documents import Document    # 수정됨\n",
    "from langchain_community.document_loaders.base import BaseLoader\n",
    "\n",
    "class HWPLoader(BaseLoader):\n",
    "    \"\"\"HWP 파일 읽기 클래스. HWP 파일의 내용을 읽습니다.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.file_path = file_path\n",
    "        self.extra_info = {\"source\": file_path}\n",
    "        self._initialize_constants()\n",
    "\n",
    "    def _initialize_constants(self) -> None:\n",
    "        \"\"\"상수 초기화 메서드\"\"\"\n",
    "        self.FILE_HEADER_SECTION = \"FileHeader\"\n",
    "        self.HWP_SUMMARY_SECTION = \"\\x05HwpSummaryInformation\"\n",
    "        self.SECTION_NAME_LENGTH = len(\"Section\")\n",
    "        self.BODYTEXT_SECTION = \"BodyText\"\n",
    "        self.HWP_TEXT_TAGS = [67]\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"HWP 파일에서 데이터를 로드하고 표를 추출합니다.\n",
    "\n",
    "        Yields:\n",
    "            Document: 추출된 문서\n",
    "        \"\"\"\n",
    "        load_file = olefile.OleFileIO(self.file_path)\n",
    "        file_dir = load_file.listdir()\n",
    "\n",
    "        if not self._is_valid_hwp(file_dir):\n",
    "            raise ValueError(\"유효하지 않은 HWP 파일입니다.\")\n",
    "\n",
    "        result_text = self._extract_text(load_file, file_dir)\n",
    "        yield self._create_document(text=result_text, extra_info=self.extra_info)\n",
    "\n",
    "    def _is_valid_hwp(self, dirs: List[List[str]]) -> bool:\n",
    "        \"\"\"HWP 파일의 유효성을 검사합니다.\"\"\"\n",
    "        return [self.FILE_HEADER_SECTION] in dirs and [self.HWP_SUMMARY_SECTION] in dirs\n",
    "\n",
    "    def _get_body_sections(self, dirs: List[List[str]]) -> List[str]:\n",
    "        \"\"\"본문 섹션 목록을 반환합니다.\"\"\"\n",
    "        section_numbers = [\n",
    "            int(d[1][self.SECTION_NAME_LENGTH :])\n",
    "            for d in dirs\n",
    "            if d[0] == self.BODYTEXT_SECTION\n",
    "        ]\n",
    "        return [\n",
    "            f\"{self.BODYTEXT_SECTION}/Section{num}\" for num in sorted(section_numbers)\n",
    "        ]\n",
    "\n",
    "    def _create_document(\n",
    "        self, text: str, extra_info: Optional[Dict] = None\n",
    "    ) -> Document:\n",
    "        \"\"\"문서 객체를 생성합니다.\"\"\"\n",
    "        return Document(page_content=text, metadata=extra_info or {})\n",
    "\n",
    "    def _extract_text(\n",
    "        self, load_file: olefile.OleFileIO, file_dir: List[List[str]]\n",
    "    ) -> str:\n",
    "        \"\"\"모든 섹션에서 텍스트를 추출합니다.\"\"\"\n",
    "        sections = self._get_body_sections(file_dir)\n",
    "        return \"\\n\".join(\n",
    "            self._get_text_from_section(load_file, section) for section in sections\n",
    "        )\n",
    "\n",
    "    def _is_compressed(self, load_file: olefile.OleFileIO) -> bool:\n",
    "        \"\"\"파일이 압축되었는지 확인합니다.\"\"\"\n",
    "        with load_file.openstream(self.FILE_HEADER_SECTION) as header:\n",
    "            header_data = header.read()\n",
    "            return bool(header_data[36] & 1)\n",
    "\n",
    "    def _get_text_from_section(self, load_file: olefile.OleFileIO, section: str) -> str:\n",
    "        \"\"\"특정 섹션에서 텍스트를 추출합니다.\"\"\"\n",
    "        with load_file.openstream(section) as bodytext:\n",
    "            data = bodytext.read()\n",
    "\n",
    "        unpacked_data = (\n",
    "            zlib.decompress(data, -15) if self._is_compressed(load_file) else data\n",
    "        )\n",
    "\n",
    "        text = []\n",
    "        i = 0\n",
    "        while i < len(unpacked_data):\n",
    "            header, rec_type, rec_len = self._parse_record_header(\n",
    "                unpacked_data[i : i + 4]\n",
    "            )\n",
    "            if rec_type in self.HWP_TEXT_TAGS:\n",
    "                rec_data = unpacked_data[i + 4 : i + 4 + rec_len]\n",
    "                text.append(rec_data.decode(\"utf-16\"))\n",
    "            i += 4 + rec_len\n",
    "\n",
    "        text = \"\\n\".join(text)\n",
    "        text = self.remove_chinese_characters(text)\n",
    "        text = self.remove_control_characters(text)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_chinese_characters(s: str):\n",
    "        \"\"\"중국어 문자를 제거합니다.\"\"\"\n",
    "        return re.sub(r\"[\\u4e00-\\u9fff]+\", \"\", s)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_control_characters(s):\n",
    "        \"\"\"깨지는 문자 제거\"\"\"\n",
    "        return \"\".join(ch for ch in s if unicodedata.category(ch)[0] != \"C\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_record_header(header_bytes: bytes) -> tuple:\n",
    "        \"\"\"레코드 헤더를 파싱합니다.\"\"\"\n",
    "        header = struct.unpack_from(\"<I\", header_bytes)[0]\n",
    "        rec_type = header & 0x3FF\n",
    "        rec_len = (header >> 20) & 0xFFF\n",
    "        return header, rec_type, rec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba78f8c8-840e-4c08-bd33-7565bd2aab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 로드 완료\n",
      "AI Agent 기술 개요와 활용 전략Ⅰ. 서론최근 인공지능 기술의 발전은 단순한 대화형 챗봇을 넘어,스스로 사고하고 행동하는 AI 에이전트(AI Agent) 의 시대를 열고 있다.AI 에이전트는 대규모 언어모델(LLM: Large Language Model)을 기반으로,자율적으로 목표를 설정하고, 외부 도구나 데이터베이스와 상호작용하여사람처럼 문제를 해결하는 새로운 형태의 인공지능 시스템이다.이러한 기술은 단순한 질문 응답을 넘어업무 자동화, 정보 탐색, 문서 생성, 분석 보고서 작성 등다양한 분야에서 실질적인 업무 대체 및 보조 역할을 수행하고 있다.AI 에이전트는 더 이상 ‘대화형 모델’이 아닌‘작업 수행자(Worker)’로 진화하고 있는 것이다.Ⅱ. AI 에이전트의 개념과 구조AI 에이전트는 기본적으로 다섯 가지 핵심 구성요소로 이루어진다.목표(Goal)사용자가 제시한 목적 또는 시스템이 스스로 정의한 과업.지각(Perception)입력된 데이터나 외부 환경의 상태를 인식하고\n"
     ]
    }
   ],
   "source": [
    "from hwploader import HWPLoader\n",
    "\n",
    "#  한글 문서 로드\n",
    "loader = HWPLoader(\"example.hwp\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"✅ 로드 완료\")\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c4fc4-b233-4e3a-8834-033942be169f",
   "metadata": {},
   "source": [
    "### [8] Docx2txtLoader(Word 문서 Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49a1b385-ff5d-4c1e-9366-db19b2ca686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치\n",
    "# !pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fdbae2f-fcf1-4218-8249-b546347336ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 로드 완료\n",
      "AI Agent 기술 개요와 활용 전략\n",
      "\n",
      "\n",
      "\n",
      "Ⅰ. 서론\n",
      "\n",
      "\n",
      "\n",
      "최근 인공지능 기술의 발전은 단순한 대화형 챗봇을 넘어,\n",
      "\n",
      "스스로 사고하고 행동하는 AI 에이전트(AI Agent) 의 시대를 열고 있다.\n",
      "\n",
      "AI 에이전트는 대규모 언어모델(LLM: Large Language Model)을 기반으로,\n",
      "\n",
      "자율적으로 목표를 설정하고, 외부 도구나 데이터베이스와 상호작용하여\n",
      "\n",
      "사람처럼 문제를 해결하는 새로운 형태의 인공지능 시스템이다.\n",
      "\n",
      "이러한 기술은 단순한 질문 응답을 넘어\n",
      "\n",
      "업무 자동화, 정보 탐색, 문서 생성, 분석 보고서 작성 등\n",
      "\n",
      "다양한 분야에서 실질적인 업무 대체 및 보조 역할을 수행하고 있다.\n",
      "\n",
      "AI 에이전트는 더 이상 ‘대화형 모델’이 아닌\n",
      "\n",
      "‘작업 수행자(Worker)’로 진화하고 있는 것이다.\n",
      "\n",
      "\n",
      "\n",
      "Ⅱ. AI 에이전트의 개념과 구조\n",
      "\n",
      "\n",
      "\n",
      "AI 에이전트는 기본적으로 다섯 가지 핵심 구성요소로 이루어진다.\n",
      "\n",
      "목표(Goal)\n",
      "\n",
      "사용자가 제시한 목적 또는 시스템이 스스로 정의한 과업.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "# 문서 로더 초기화\n",
    "loader = Docx2txtLoader(\"example.docx\")\n",
    "\n",
    "# 문서 로딩\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"✅ 로드 완료\")\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad26d31-d945-4dd3-a2b2-86f16561afd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
