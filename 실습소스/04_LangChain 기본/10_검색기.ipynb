{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58cd42a5-b1f7-40de-947c-4943f05e7931",
   "metadata": {},
   "source": [
    "## 검색기(Retriever)\n",
    "LangChain의 검색기(Retriever) 는 RAG(Retrieval-Augmented Generation) 구조에서 질문과 관련된 문서나 정보 조각을 찾아주는 핵심 모듈이다. <br>\n",
    "즉, “지식 검색 엔진” 역할을 하며, LLM이 답변을 생성하기 전에 참고할 문맥(context)을 찾아준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88472069-3fc1-4eb6-b7b6-29137c255af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일의 내용 불러오기\n",
    "load_dotenv(\"C:/env/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f7f66-2601-457a-874b-48000f0b6e3e",
   "metadata": {},
   "source": [
    "### [1] VectorStoreRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf1d69c-91c1-4679-b5e6-9d5d8cd66181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[결과 1] Retriever는 문서에서 관련 정보를 검색합니다.\n",
      "[결과 2] FAISS는 Facebook AI가 만든 벡터 검색 라이브러리입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 1) 문서 준비\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain은 LLM 애플리케이션 개발 프레임워크입니다.\"),\n",
    "    Document(page_content=\"Retriever는 문서에서 관련 정보를 검색합니다.\"),\n",
    "    Document(page_content=\"FAISS는 Facebook AI가 만든 벡터 검색 라이브러리입니다.\")\n",
    "]\n",
    "\n",
    "# 2) 임베딩 모델\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 3) FAISS 벡터 DB 생성\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 4) VectorStoreRetriever 생성\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# 5) 질의 수행\n",
    "query = \"문서를 검색하는 모듈은?\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "# 6) 결과 출력\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"[결과 {i}] {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1858ea-613f-4389-90b6-509501ad647d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS는 벡터 검색 라이브러리로, 문서나 데이터베이스에서 유사한 벡터를 효율적으로 검색하는 역할을 합니다.\n"
     ]
    }
   ],
   "source": [
    "# VectorStoreRetriever 기반 RAG 파이프라인\n",
    "\n",
    "#  1. 필수 모듈 임포트\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#  2. LLM 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "#  3. Retriever (예: FAISS, Chroma 등)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "#  4. 프롬프트 정의\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "아래 문서를 참고하여 질문에 답하세요.\n",
    "\n",
    "문서 내용:\n",
    "{context}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "# 5. RAG 파이프라인 구성\n",
    "rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "#  6. 질의 실행\n",
    "response = rag_chain.invoke(\"FAISS는 어떤 역할을 하나요?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7efb3-9b60-482d-a63e-ba6302704ad5",
   "metadata": {},
   "source": [
    "### MMR(Maximal Marginal Relevance) 검색\n",
    ": MMR(Maximal Marginal Relevance) 검색은\n",
    "LangChain의 VectorStoreRetriever 등에서 검색 결과의 “다양성(Diversity)”을 높이기 위한 검색 방식이다.<br>\n",
    "MMR 검색은 “질문과 비슷하면서도 서로 다른 내용의 문서”를 골라주는,\n",
    "중복 최소화 + 다양성 보장형 벡터 검색 방식이다. <br>\n",
    "lambda_mult : 유사성과 다양성의 비율 (기본 0.5, 높을수록 유사성 우선,낮을수록 다양성 우선)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc793c6-13e8-4c19-8955-4b412f7b9ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='cf29d58e-5f5d-44f1-b975-735b4c8669ef', metadata={}, page_content='Retriever는 문서에서 관련 정보를 검색합니다.'),\n",
       " Document(id='678dae07-121d-4cd1-8dc3-815ed395d752', metadata={}, page_content='LangChain은 LLM 애플리케이션 개발 프레임워크입니다.'),\n",
       " Document(id='27a97f6a-c7af-42da-813b-4fca12f61c1a', metadata={}, page_content='FAISS는 Facebook AI가 만든 벡터 검색 라이브러리입니다.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MMR - 다양성 고려 (lambda_mult 작을수록 더 다양하게 추출)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={'k': 3, 'lambda_mult': 0.15}  # lambda_mult가 0.15로 설정되어 있으므로, 관련성보다 다양성을 더 우선\n",
    ")\n",
    "\n",
    "# docs = retriever.get_relevant_documents(query)  # 1.0에서 변경됨\n",
    "docs = retriever.invoke(query)\n",
    "print(len(docs))\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53693679-3b20-47fb-a432-c38154521e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 16 페이지 로드됨\n",
      "\n",
      "질문: 이 논문에서 리뷰 이상 탐지를 수행하는 주요 방법은 무엇인가요?\n",
      "\n",
      "답변: 이 논문에서 리뷰 이상 탐지를 수행하는 주요 방법은 다음과 같이 여러 가지 기법으로 분류됩니다:\n",
      "\n",
      "1. **비지도 학습**: F-통계를 통해 향상된 K-평균을 사용하여 리뷰를 적응적으로 클러스터링하고 비정상 그룹을 감지하여 가짜 리뷰 식별 기능을 강화합니다.\n",
      "\n",
      "2. **지도 학습**: 리뷰 텍스트, 사용자 행동, 판매자 데이터의 기능을 사용하여 가짜 리뷰 탐지의 정확성과 효율성을 균형 있게 유지하는 강력한 성능을 보이는 DDAG-SVM 모델을 적용합니다.\n",
      "\n",
      "3. **반지도 학습**: 일반 베이지안 모델과 수동 데이터 어노테이션 결과를 기반으로 여러 특징 조합의 효과를 평가하고, Co-training과 Tri-training이라는 두 가지 반지도 학습 전략을 도입하여 탐지 성능을 크게 향상시킵니다.\n",
      "\n",
      "4. **텍스트 기반 분석**: 댓글의 언어적 내용과 의미적 특징을 분석하여 허위 댓글을 구별하는 주요 수단으로 사용됩니다.\n",
      "\n",
      "5. **비판 기반 분석**: 가짜 리뷰어들이 보이는 비정상적인 평가 패턴을 분석하여 실제 사용자와의 차이를 발견하고, 이것이 탐지의 핵심 단서로 작용합니다.\n",
      "\n",
      "6. **시간 기반 분석**: 가짜 리뷰와 실제 사용자 리뷰의 시간적 차이를 분석하여, 가짜 리뷰는 짧은 시간 내에 집중적으로 게시되는 경향이 있다는 것을 발견합니다.\n",
      "\n",
      "7. **차트 구조 기반 분석**: 시간적 밀집성과 그래프 구조를 결합해 리뷰 그룹을 정의합니다.\n",
      "\n",
      "8. **PU 학습 알고리즘 및 행동 밀도**: 최소한의 라벨링된 데이터를 사용하여 가짜 댓글을 감지하는 데 높은 효과를 보임.\n",
      "\n",
      "이 기법들은 서로 보완적인 역할을 하며, 일반적으로 가짜 리뷰 또는 비정상적인 리뷰를 탐지하기 위해 활용됩니다.\n"
     ]
    }
   ],
   "source": [
    "#  1. 모듈 임포트\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "#  2. PDF 문서 로드\n",
    "loader = PyPDFLoader(\"유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지.pdf\")\n",
    "docs = loader.load()\n",
    "print(f\"총 {len(docs)} 페이지 로드됨\")\n",
    "\n",
    "#  3. 임베딩 모델 설정\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "#  4. FAISS 벡터스토어 생성\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "#  5. MMR(Maximal Marginal Relevance) 기반 Retriever 생성\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  \n",
    "    search_kwargs={\"k\": 3, \"lambda_mult\": 0.5} \n",
    ")\n",
    "\n",
    "#  6. 프롬프트 템플릿 정의\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "아래의 문서를 참고하여 질문에 답하세요.\n",
    "\n",
    "문서 내용:\n",
    "{context}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "#  7. LLM 및 파서 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "#  8. RAG 파이프라인 구성\n",
    "rag_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "#  9. 질의 실행\n",
    "query = \"이 논문에서 리뷰 이상 탐지를 수행하는 주요 방법은 무엇인가요?\"\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "#  10. 결과 출력\n",
    "print(\"\\n질문:\", query)\n",
    "print(\"\\n답변:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbeca7f1-fc55-4043-b54d-c1dafa5cc1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(id='cae7c6ec-3bff-45a0-aaf5-47bae361a6e3', metadata={'producer': 'iText 2.1.7 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-10-07T19:23:49+09:00', 'moddate': '2025-10-07T19:26:28+09:00', 'source': '유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지   51\\n폐된 조작된 리뷰 클러스터를 효과적으로 분리\\n할 수 있으며, 탐지 시스템의 신뢰성과 견고성\\n을 크게 향상시키는 데 기여한다. 따라서, 이전 \\n연구에서 사용한 리뷰 분류기법의 주요 유형들\\n을 고찰하였으며, 서술된 모든 방법은 <표 1>에 \\n제시했다.\\n2.2 극단적 및 조작된 리뷰의 식별\\n온라인 사용자 리뷰는 소비자 의사결정에 강\\n력한 영향을 미치는 요소로 작용하며, 그 신뢰\\n성과 진정성 확보를 위한 탐지 기술의 중요성이 \\n점차 강조되고 있다\\n. [22]는 형용사의 극성을 판\\n별하기 위해 코퍼스 기반의 비지도 학습 접근 \\n방식을 제안하였다.\\n이들의 방법은 텍스트 내에서 접속사(예: \\n“and”, “but”)로 연결된 형용사들이 일반적으로 \\n유사한 감성 극성을 지닌다는 언어학적 가정을 \\n기반으로 한다. 이러한 원칙은 이후 감성어 사\\n전 구축 및 감성 분석 자동화의 핵심 기초가 되\\n었으며\\n[23] 규칙 기반 감성 분류에서 텍스트의 \\n감정 성향을 결정하는 데 널리 활용되었다. 다\\n만, 해당 방식은 비속어나 신조어, 반어법 등 문\\n맥에 따라 감정이 변하는 표현을 정교하게 처리\\n하는 데는 한계가 존재한다.\\n문서 단위에서의 감성 분류는 특히 극단적인 \\n리뷰 탐지에 효과적인 접근법으로 여겨진다. \\n[1]은 극단적 견해와 비극단적 견해를 이진 분\\n류하는 연구를 통해, 감정적으로 과장된 콘텐츠가 \\n온라인 담론의 진정성을 왜곡할 수 있다는 점을 \\n지적하였다. 이는 조작적이거나 편향된 정보가 소\\n비자 판단에 미치는 영향을 최소화하기 위한 기\\n분류 분류기법 주요 발견 저자\\n1 비지도 학습 F-통계를 통해 향상된 K-평균을 사용하여 리뷰를 적응적으로 클러스터링하\\n고 비정상 그룹을 감지하여 가짜 리뷰 식별 기능을 향상 [39]\\n2 지도 학습 \\n리뷰 텍스트, 사용자 행동, 판매자 데이터의 기능을 사용하여 가짜 리뷰 탐지\\n의 정확성과 효율성을 균형 있게 유지하는 데 강력한 성능을 보인 \\nDDAG-SVM 모델 적용\\n[40]\\n3 반지도 학습\\n먼저 일반 베이지안 모델과 수동 데이터 어노테이션 결과를 기반으로 여러 \\n특징 조합의 효과를 평가하고, 이를 바탕으로 다수의 어노테이션이 없는 \\n텍스트를 최대한 활용하기 위해 Co-training과 Tri-training이라는 두 가지 반\\n지도 학습 전략을 도입하여 탐지 성능을 효과적으로 향상\\n[20]\\n4 텍스트 기반 분석\\n댓글 텍스트의 언어적 내용과 의미적 특징이 댓글의 진위를 반영할 수 있으\\n며, 따라서 텍스트 분석 기법은 허위 댓글을 구별하는 가장 중요한 수단이라\\n고 강조\\n[16]\\n5 비판 기반 분석\\n가짜 리뷰어들이 종종 비정상적인 평가 패턴을 보이며, 실제 사용자와 크게 \\n다른 분포를 보인다는 사실을 발견했습니다. 이는 탐지의 핵심 단서로 작용\\n합니다.\\n[41]\\n6 시간 기반\\n분석\\n가짜 리뷰와 실제 사용자 리뷰의 차이를 발견. 가짜 리뷰는 짧은 시간 내 \\n집중적으로 게시. 시간적 패턴이 뚜렷한 것이 특징. 실제 사용자 리뷰가 구매 \\n후 무작위로 작성\\n[16]\\n7 차트 구조 기반 \\n분석\\n시간적 밀집성과 그래프 구조를 결합해 리뷰 그룹 정의 확대. 사기 사용자들\\n은 시간 일관성과 높은 연결성을 보임. [19]\\n8 PU 학습 알고리즘 \\n및 행동 밀도\\nPU 학습과 행동 밀도를 결합하여 최소한의 라벨링된 데이터를 사용으로 \\n가짜 댓글을 감지함으로써 실제로 높은 효과를 보여줌. [20]\\n<표 1> 조작 리뷰 분류기법 사례')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(query)\n",
    "print(len(docs))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb060445-98ac-479f-a884-ed13b8411d39",
   "metadata": {},
   "source": [
    "### [2] 다중 쿼리 검색기: Multi Query Retriever\n",
    ": LangChain에서 질문을 여러 방식으로 바꿔서 검색 정확도를 높이는 아주 강력한 Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dcd5f5a-4a18-43df-bf65-d51f51d445f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ee7ee2e-0312-4ed6-b8aa-9f5ca93e9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[결과 1] LangChain의 검색 시스템은 Retriever를 사용한다.\n",
      "[결과 2] LangChain은 LLM 애플리케이션 개발 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_classic.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 1. 문서 데이터\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain은 LLM 애플리케이션 개발 프레임워크이다.\"),\n",
    "    Document(page_content=\"Retriever는 문서에서 관련 정보를 검색한다.\"),\n",
    "    Document(page_content=\"FAISS는 빠른 벡터 검색 라이브러리이다.\"),\n",
    "    Document(page_content=\"LangChain의 검색 시스템은 Retriever를 사용한다.\"),\n",
    "]\n",
    "\n",
    "# 2. 벡터스토어 생성\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 3. LLM 및 기본 검색기\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# 4. MultiQueryRetriever 생성\n",
    "multi_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=base_retriever,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# 5. 질의 실행\n",
    "query = \"LangChain의 문서 검색 구조는?\"\n",
    "results = multi_retriever.invoke(query)\n",
    "\n",
    "# 6. 결과 출력\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"[결과 {i}] {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1820c3ea-671e-43fa-a750-801dcdf057f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내부 질의문 직접 보기\n",
    "# queries = multi_retriever.get_queries(\"LangChain의 문서 검색 구조는?\")  #오류 발생\n",
    "# print(\"LLM이 생성한 내부 질의문들:\")\n",
    "# for i, q in enumerate(queries, 1):\n",
    "#     print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a0ab642-300a-439f-b948-8f84f61a515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM이 생성한 내부 질의문들:\n",
      "[원본]:LangChain의 문서 검색 구조는?\n",
      "1. LangChain에서 문서 검색 시스템은 어떻게 구성되어 있나요?\n",
      "2. LangChain의 문서 검색 아키텍처는 어떤 식으로 이루어져 있습니까?\n",
      "3. LangChain의 문서 검색 방식은 무엇인가요?\n"
     ]
    }
   ],
   "source": [
    "# MultiQueryRetriever 내부의 동작 흉내낸 드드\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1. LLM 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 2. MultiQueryRetriever 내부 프롬프트와 동일한 구조\n",
    "prompt_text = \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate\n",
    "multiple alternative questions that are semantically similar\n",
    "to the input question. These questions will be used to retrieve\n",
    "relevant documents.\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Generate {num_queries} alternative questions:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# 3. 안전한 사용자 정의 함수\n",
    "def generate_queries(question, num_queries=3):\n",
    "    \"\"\"MultiQueryRetriever 내부 LLM 호출을 재현한 사용자 정의 함수\"\"\"\n",
    "    formatted_prompt = prompt.format(question=question, num_queries=num_queries)\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    queries = [\n",
    "        line.strip(\"-•0123456789. \").strip()\n",
    "        for line in response.content.split(\"\\n\")\n",
    "        if line.strip()\n",
    "    ]\n",
    "    return queries\n",
    "\n",
    "# 4. 테스트 실행\n",
    "queries = generate_queries(\"LangChain의 문서 검색 구조는?\", num_queries=3)\n",
    "\n",
    "print(\"LLM이 생성한 내부 질의문들:\\n[원본]:LangChain의 문서 검색 구조는?\")\n",
    "for i, q in enumerate(queries, 1):\n",
    "    print(f\"{i}. {q}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecec2be-7e93-42d4-b83e-39a61849e0cd",
   "metadata": {},
   "source": [
    "### [3] 문맥 압축 검색기: ContextualCompressionRetriever\n",
    "이 모듈은 RAG의 검색 결과를 “요약·압축”해서 전달함으로써,\n",
    "LLM이 더 작은 입력으로도 중요한 정보만 이해할 수 있게 해주는 Retriever 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d004a280-2e92-490e-8c08-c5cfefb1c964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[요약 1] Retriever는 문서에서 관련 정보를 검색합니다.\n",
      "[요약 2] ContextualCompressionRetriever는 검색된 문서를 요약해 전달합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# 1. 기본 문서\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain은 LLM 애플리케이션 개발 프레임워크입니다.\"),\n",
    "    Document(page_content=\"Retriever는 문서에서 관련 정보를 검색합니다.\"),\n",
    "    Document(page_content=\"ContextualCompressionRetriever는 검색된 문서를 요약해 전달합니다.\")\n",
    "]\n",
    "\n",
    "# 2. 벡터스토어 생성\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 3. LLM 기반 요약 압축기 생성\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# 4. ContextualCompressionRetriever 생성\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "# 5. 질의 실행\n",
    "query = \"문서 검색 결과를 요약해주는 Retriever는?\"\n",
    "results = compression_retriever.invoke(query)\n",
    "\n",
    "# 6. 결과 출력\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"[요약 {i}] {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b9574-c7a7-4a8a-b50f-71dd01f218ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 16 페이지 로드됨\n"
     ]
    }
   ],
   "source": [
    "# LangChain의 RAG 수행 예제\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# 2. PDF 문서 로드\n",
    "loader = PyPDFLoader(\"유튜브 기반 사용자 콘텐츠에서의 리뷰 이상 탐지.pdf\")\n",
    "docs = loader.load()\n",
    "print(f\"총 {len(docs)} 페이지 로드됨\")\n",
    "\n",
    "# 3. 임베딩 모델 설정\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 4. FAISS 벡터스토어 생성\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 5. 기본 Retriever 생성\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# 6. LLM 기반 문서 압축기 생성\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# 7. ContextualCompressionRetriever 생성\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=base_retriever,\n",
    "    base_compressor=compressor\n",
    ")\n",
    "\n",
    "# 8. RAG 파이프라인 구성\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "아래 문서를 참고하여 질문에 답하세요.\n",
    "\n",
    "문서 요약 내용:\n",
    "{context}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableParallel({\"context\": compression_retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 9. 질의 실행\n",
    "query = \"이 논문에서 리뷰 이상 탐지를 수행하는 주요 방법은 무엇인가요?\"\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# 10. 결과 출력\n",
    "print(\"\\n질문:\", query)\n",
    "print(\"\\n답변:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98049d8-d8cc-4dd5-b670-c4f0dcbcbfa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
