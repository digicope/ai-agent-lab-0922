{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a130c772-5ef1-4d51-b13c-62a48bf2228e",
   "metadata": {},
   "source": [
    "## LLM ëª¨ë¸\n",
    "LangChainì—ì„œ LLMì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(GPT, Claude, Gemini, Llama ë“±)ì„ ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì—°ê²°í•´ í…ìŠ¤íŠ¸ ìƒì„±Â·ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” í•µì‹¬ ì—”ì§„ì´ë‹¤.<br>\n",
    "LLMì€ AIì˜ ë‡Œ ì—­í• ì„ í•˜ë©° LangChainì€ ì´ë¥¼ ì²´ì¸Â·ì—ì´ì „íŠ¸ êµ¬ì¡°ë¡œ í™•ì¥í•´ ë‹¤ì–‘í•œ ì‘ìš© ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•œë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461cd27f-fd21-4e62-94ac-cc1f413c29cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ì˜ ë‚´ìš© ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "load_dotenv(\"C:/env/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8cef9-1cc2-4dea-9794-83823dad80ed",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c827a797-3b73-4cb0-a459-4bbebd9f4302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•˜ëŠ˜ì´ íŒŒë€ ì´ìœ ëŠ” ëŒ€ê¸° ì¤‘ì˜ ì‚°ë€ í˜„ìƒ ë•Œë¬¸ì…ë‹ˆë‹¤. íƒœì–‘ë¹›ì€ ì—¬ëŸ¬ ìƒ‰ì˜ ë¹›ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©°, ê° ìƒ‰ì€ ì„œë¡œ ë‹¤ë¥¸ íŒŒì¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ ì¤‘ íŒŒë€ìƒ‰ ë¹›ì€ íŒŒì¥ì´ ì§§ê¸° ë•Œë¬¸ì— ëŒ€ê¸° ì¤‘ì˜ ë¨¼ì§€ì™€ ê³µê¸° ë¶„ìì— ì˜í•´ ë” ë§ì´ ì‚°ë€ë©ë‹ˆë‹¤. \n",
      "\n",
      "íƒœì–‘ì´ í•˜ëŠ˜ ë†’ì´ ìˆì„ ë•Œ, ìš°ë¦¬ê°€ ë³´ëŠ” í•˜ëŠ˜ì€ ì£¼ë¡œ ì´ ì‚°ë€ëœ íŒŒë€ìƒ‰ ë¹›ì„ ë°˜ì‚¬í•˜ì—¬ ë‚˜íƒ€ë‚˜ë¯€ë¡œ, í•˜ëŠ˜ì´ íŒŒë—ê²Œ ë³´ì´ëŠ” ê²ƒì…ë‹ˆë‹¤. í•´ê°€ ì§€ê±°ë‚˜ ì§€í‰ì„  ê°€ê¹Œì´ ìˆì„ ë•ŒëŠ”, íƒœì–‘ë¹›ì´ ëŒ€ê¸°ë¥¼ í†µê³¼í•˜ëŠ” ê±°ë¦¬ê°€ ê¸¸ì–´ì ¸, ì§§ì€ íŒŒì¥ì˜ íŒŒë€ìƒ‰ ë¹›ì€ ë§ì´ ì‚°ë€ë˜ê³  ê¸´ íŒŒì¥ì˜ ë¹¨ê°„ìƒ‰ì´ë‚˜ ì£¼í™©ìƒ‰ ë¹›ì´ ë”ìš± ë‘ë“œëŸ¬ì§€ê²Œ ë³´ì—¬ì§€ë©´ì„œ ì•„ë¦„ë‹¤ìš´ ì„ì–‘ì˜ ìƒ‰ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "response= model.invoke(\"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf485e30-17c7-4470-9add-08fa52bc521d",
   "metadata": {},
   "source": [
    "### Anthropic\n",
    "https://docs.claude.com/en/docs/about-claude/models/overview?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b97f36-b891-4e20-b622-a8c4eae003aa",
   "metadata": {},
   "source": [
    ".env íŒŒì¼ì— ANTHROPIC_API_KEY=\"YOUR_API_KEY\" ë¥¼ ì¶”ê°€í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf36cda-a9f7-4ba2-8035-43a0df0aa4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-anthropic\n",
    "# notebook Kernel Restart!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11987ec2-5b61-4e80-9661-5b5d4b6eccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# í•˜ëŠ˜ì´ íŒŒë€ ì´ìœ \n",
      "\n",
      "í•˜ëŠ˜ì´ íŒŒë—ê²Œ ë³´ì´ëŠ” ì´ìœ ëŠ” **ë ˆì¼ë¦¬ ì‚°ë€(Rayleigh scattering)** ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
      "\n",
      "## ì›ë¦¬\n",
      "\n",
      "1. **íƒœì–‘ë¹›ì˜ êµ¬ì„±**\n",
      "   - íƒœì–‘ë¹›ì€ ì—¬ëŸ¬ ìƒ‰ê¹”(íŒŒì¥)ì´ ì„ì¸ ë°±ìƒ‰ê´‘ì…ë‹ˆë‹¤\n",
      "\n",
      "2. **ëŒ€ê¸°ì™€ì˜ ìƒí˜¸ì‘ìš©**\n",
      "   - íƒœì–‘ë¹›ì´ ëŒ€ê¸°ë¥¼ í†µê³¼í•  ë•Œ ê³µê¸° ë¶„ìì™€ ì¶©ëŒí•©ë‹ˆë‹¤\n",
      "   - íŒŒë€ìƒ‰ ê³„ì—´ì˜ ì§§ì€ íŒŒì¥ì€ ë” ë§ì´ ì‚°ë€ë©ë‹ˆë‹¤\n",
      "   - ë¹¨ê°„ìƒ‰ ê³„ì—´ì˜ ê¸´ íŒŒì¥ì€ ëœ ì‚°ë€ë©ë‹ˆë‹¤\n",
      "\n",
      "3. **ìš°ë¦¬ ëˆˆì— ë³´ì´ëŠ” ê²ƒ**\n",
      "   - ì‚¬ë°©ìœ¼ë¡œ ì‚°ë€ëœ íŒŒë€ë¹›ì´ ìš°ë¦¬ ëˆˆì— ë“¤ì–´ì˜µë‹ˆë‹¤\n",
      "   - ê·¸ë˜ì„œ í•˜ëŠ˜ ì „ì²´ê°€ íŒŒë—ê²Œ ë³´ì…ë‹ˆë‹¤\n",
      "\n",
      "## ì¬ë¯¸ìˆëŠ” ì‚¬ì‹¤\n",
      "\n",
      "- **ì¼ëª°ì´ ë¹¨ê°„ ì´ìœ **: í•´ì§ˆë…˜ì—ëŠ” ë¹›ì´ ëŒ€ê¸°ë¥¼ ë” ê¸¸ê²Œ í†µê³¼í•˜ë©´ì„œ íŒŒë€ë¹›ì€ ëª¨ë‘ ì‚°ë€ë˜ê³ , ë¹¨ê°„ë¹›ë§Œ ìš°ë¦¬ì—ê²Œ ë„ë‹¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤\n",
      "\n",
      "- **ìš°ì£¼ì—ì„œ ë³¸ í•˜ëŠ˜**: ëŒ€ê¸°ê°€ ì—†ëŠ” ìš°ì£¼ì—ì„œëŠ” í•˜ëŠ˜ì´ ê²€ê²Œ ë³´ì…ë‹ˆë‹¤\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\",  # ì‚¬ìš© ê°€ëŠ¥í•œ ë²„ì „: opus, sonnet, haiku ë“±\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ë©”ì‹œì§€ êµ¬ì„±\n",
    "messages = [HumanMessage(content=\"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\")]\n",
    "\n",
    "# ëª¨ë¸ ì‘ë‹µ\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49857d7a-218f-49fc-885d-cc480e0042a9",
   "metadata": {},
   "source": [
    "### Google Gemini\n",
    "https://ai.google.dev/gemini-api/docs/models/gemini?hl=ko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e5970-2bba-4b32-b447-d32125e1b884",
   "metadata": {},
   "source": [
    ".env íŒŒì¼ì— GOOGLE_API_KEY=\"YOUR_API_KEY\"ë¥¼ ì¶”ê°€í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccdbb860-fd62-4b9f-8a27-ef4eeec27c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0230888-984e-499a-8ada-3e6da67e1eb1",
   "metadata": {},
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "# model: \"gemini-2.5-pro-latest\" ë˜ëŠ” \"gemini-2.5-flash-latest\", \"gemini-pro\" ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\") \n",
    "\n",
    "# ë©”ì‹œì§€ êµ¬ì„±\n",
    "prompt = \"ì•ˆë…•í•˜ì„¸ìš”! ë‚˜ëŠ” í™ê¸¸ë™ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "# ëª¨ë¸ ì‘ë‹µ\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e82fe-3887-4a7b-9bfd-d5c4980946cb",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "OllamaëŠ” ë¡œì»¬ í™˜ê²½ì—ì„œ Llama 3, Mistral, Gemma ë“± ë‹¤ì–‘í•œ LLMì„ ì§ì ‘ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ í”Œë«í¼ì´ë‹¤.\n",
    "í´ë¼ìš°ë“œ ìš”ê¸ˆ ì—†ì´ ë¹ ë¥´ê³  ë³´ì•ˆì„± ë†’ê²Œ AI ëª¨ë¸ì„ êµ¬ë™Â·ì—°ë™í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ê°€ì¥ í° ì¥ì ì´ë‹¤. <br>\n",
    " https://ollama.com/\n",
    " \n",
    "- Llama 3: ì„±ëŠ¥ ìµœê³ , ë…¼ë¦¬Â·ì½”ë”©Â·ëŒ€í™” í’ˆì§ˆì´ ê°€ì¥ ìš°ìˆ˜í•˜ì§€ë§Œ ëª¨ë¸ì´ ì»¤ì„œ ì†ë„ëŠ” ë‹¤ì†Œ ëŠë¦¼. <br>\n",
    "- Mistral 7B : ì„±ëŠ¥ê³¼ ì†ë„ì˜ ê· í˜•í˜•, ì¤‘ê°„ í¬ê¸° ëª¨ë¸ë¡œ ê°€ë³ê³  ë¹ ë¥´ë©° ì¶”ë¡ ë ¥ë„ ì–‘í˜¸. <br>\n",
    "- Gemma 2 / 3 : âš¡ì†ë„ ê°€ì¥ ë¹ ë¦„, ì‘ì€ í™˜ê²½ì— ì í•©í•˜ì§€ë§Œ ì´í•´ë ¥Â·ì •í™•ë„ëŠ” ë‚®ìŒ. <br>\n",
    "ê³ ì„±ëŠ¥ì´ ëª©í‘œë¼ë©´ Llama 3, ì†ë„Â·íš¨ìœ¨ì„ ì›í•˜ë©´ Mistral, ì €ì‚¬ì–‘ í™˜ê²½ì´ë©´ Gemmaê°€ ê°€ì¥ ì í•©í•˜ë‹¤. <br>\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ff3cfb9-c17d-43c7-bc1e-91b833479b92",
   "metadata": {},
   "source": [
    "https://ollama.com/ ì—ì„œ OllamaSetup.exe íŒŒì¼ ë‹¤ìš´ë¡œë“œ (1.12GB) ì„¤ì¹˜\n",
    "Ollama ì‹¤í–‰ í›„ \"gemma3:4b\" ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í›„ ì§„í–‰í•œë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670b03b8-4c45-48ae-9dd9-3725f845c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e013e6-3e5d-48c2-9e07-9960ad620bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•˜ëŠ˜ì´ íŒŒë—ê²Œ ë³´ì´ëŠ” ì´ìœ ëŠ” **ë ˆì¼ë¦¬ ì‚°ë€(Rayleigh scattering)**ì´ë¼ëŠ” í˜„ìƒ ë•Œë¬¸ì…ë‹ˆë‹¤. ì¢€ ë” ìì„¸íˆ ì„¤ëª…í•´ ë“œë¦´ê²Œìš”.\n",
      "\n",
      "1. **íƒœì–‘ë¹›ì˜ êµ¬ì„±:** íƒœì–‘ë¹›ì€ ë¹¨ê°„ìƒ‰, ì£¼í™©ìƒ‰, ë…¸ë€ìƒ‰, ì´ˆë¡ìƒ‰, íŒŒë€ìƒ‰, ë³´ë¼ìƒ‰ ë“± ë‹¤ì–‘í•œ ìƒ‰ê¹”ì˜ ë¹›ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ì´ ë¹›ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ íŒŒì¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. íŒŒì¥ì´ ì§§ì„ìˆ˜ë¡ ì—ë„ˆì§€ê°€ ë†’ê³ , íŒŒì¥ì´ ê¸¸ìˆ˜ë¡ ì—ë„ˆì§€ê°€ ë‚®ìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **ëŒ€ê¸° ì¤‘ì˜ ì…ì:** ì§€êµ¬ë¥¼ ë‘˜ëŸ¬ì‹¼ ëŒ€ê¸°ì—ëŠ” ì§ˆì†Œ, ì‚°ì†Œì™€ ê°™ì€ ì‘ì€ ì…ìë“¤ì´ ë– ë‹¤ë‹ˆê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ë ˆì¼ë¦¬ ì‚°ë€:** íƒœì–‘ë¹›ì´ ëŒ€ê¸° ì¤‘ì˜ ì‘ì€ ì…ìë“¤ê³¼ ë¶€ë”ªíˆë©´ì„œ ë¹›ì´ ì‚¬ë°©ìœ¼ë¡œ í©ì–´ì§€ëŠ” í˜„ìƒì„ ë ˆì¼ë¦¬ ì‚°ë€ì´ë¼ê³  í•©ë‹ˆë‹¤. íŠ¹íˆ íŒŒì¥ì´ ì§§ì€ íŒŒë€ìƒ‰ê³¼ ë³´ë¼ìƒ‰ ë¹›ì€ íŒŒì¥ì´ ê¸¸í•œ ë¹¨ê°„ìƒ‰ ë¹›ë³´ë‹¤ ëŒ€ê¸° ì¤‘ì˜ ì…ìë“¤ê³¼ ë” ì‰½ê²Œ ì‚°ë€ë©ë‹ˆë‹¤.\n",
      "\n",
      "4. **íŒŒë€ìƒ‰ ë¹›ì´ ë” ë§ì´ ë³´ì´ëŠ” ì´ìœ :** íƒœì–‘ë¹›ì´ ëŒ€ê¸° ì¤‘ì„ í†µê³¼í•  ë•Œ, íŒŒë€ìƒ‰ ë¹›ì´ ë‹¤ë¥¸ ìƒ‰ê¹”ì˜ ë¹›ë³´ë‹¤ í›¨ì”¬ ë” ë§ì´ ì‚°ë€ë©ë‹ˆë‹¤. ì´ ì‚°ë€ëœ íŒŒë€ìƒ‰ ë¹›ì´ ìš°ë¦¬ ëˆˆì— ë“¤ì–´ì˜¤ë©´ì„œ í•˜ëŠ˜ì´ íŒŒë—ê²Œ ë³´ì´ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "**ë³´ë¼ìƒ‰ ë¹›ì€ ì™œ íŒŒë€ìƒ‰ë³´ë‹¤ ë” ë§ì´ ì‚°ë€ë ê¹Œìš”?**\n",
      "\n",
      "ë³´ë¼ìƒ‰ ë¹›ì€ íŒŒë€ìƒ‰ ë¹›ë³´ë‹¤ íŒŒì¥ì´ ë” ì§§ê¸° ë•Œë¬¸ì—, ëŒ€ê¸° ì¤‘ì˜ ì…ìë“¤ì— ì˜í•´ ë” ë§ì´ ì‚°ë€ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ íƒœì–‘ë¹› ìì²´ì— ë³´ë¼ìƒ‰ ë¹›ì˜ ì–‘ì´ íŒŒë€ìƒ‰ ë¹›ë³´ë‹¤ ì ê³ , ìš°ë¦¬ ëˆˆì´ íŒŒë€ìƒ‰ ë¹›ì— ë” ë¯¼ê°í•˜ê¸° ë•Œë¬¸ì— í•˜ëŠ˜ì´ íŒŒë—ê²Œ ë³´ì´ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "**í•´ê°€ ì§ˆ ë•Œ í•˜ëŠ˜ì´ ë¶‰ê²Œ ë³´ì´ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?**\n",
      "\n",
      "í•´ê°€ ì§ˆ ë•ŒëŠ” íƒœì–‘ë¹›ì´ ëŒ€ê¸°ì¸µì„ ë” ê¸´ ê±°ë¦¬ë¥¼ í†µê³¼í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ íŒŒë€ìƒ‰ ë¹›ì€ ëŒ€ë¶€ë¶„ ì‚°ë€ë˜ì–´ ì‚¬ë¼ì§€ê³ , ìƒëŒ€ì ìœ¼ë¡œ íŒŒì¥ì´ ê¸´ ë¹¨ê°„ìƒ‰ ë¹›ì´ë‚˜ ì£¼í™©ìƒ‰ ë¹›ì´ ìš°ë¦¬ ëˆˆì— ë„ë‹¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì´í•´ê°€ ë˜ì…¨ë‚˜ìš”? ğŸ˜Š ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Ollama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "llm = ChatOllama(model=\"gemma3:4b\")\n",
    "\n",
    "response= llm.invoke(\"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f3a87b-ab4b-45df-98a3-84249ab1fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Ollama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "llm = ChatOllama(model=\"gemma3:12b\")\n",
    "\n",
    "response= llm.invoke(\"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\")\n",
    "print(response.content)\n",
    "# ì‘ë‹µ ì˜¤ë˜ ê±¸ë¦¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f3715-eed1-468d-b0d3-66dc2edeb99a",
   "metadata": {},
   "source": [
    "### HuggingFacePipeline\n",
    "HuggingFacePipelineì€ LangChainì—ì„œ Hugging Faceì˜ Transformer ëª¨ë¸ì„ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ ì—°ê²°í•´ì£¼ëŠ” í´ë˜ìŠ¤ì´ë‹¤.<br>\n",
    "OpenAI APIì²˜ëŸ¼ ì™¸ë¶€ LLMì„ ì“°ì§€ ì•Šê³  ë¡œì»¬ ë˜ëŠ” Hugging Face Hub ëª¨ë¸ì„ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668591fd-2048-44e1-953d-6c4ce4358524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch  langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "735b595c-0dad-4c1d-95e8-7bea9ab891bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain Why the sky is blue.\n",
      "\n",
      "That's why the \"Blue Sky\" is so great. It's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# 1. Hugging Face íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",     # ì˜ì–´ ì „ìš© ëª¨ë¸    ,  ì„±ëŠ¥ ë‚®ìŒ      \n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 2. LangChainìš© ë˜í¼ë¡œ ê°ì‹¸ê¸°\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 3. ëª¨ë¸ í˜¸ì¶œ\n",
    "response = llm.invoke(\"Explain Why the sky is blue.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb4fb54a-d541-4426-9b98-876c3f9d6992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eb40dd016747998d17a2cc5df24361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# HuggingFace ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # ì‚¬ìš©í•  ëª¨ë¸ì˜ ID\n",
    "    task=\"text-generation\",          # ìˆ˜í–‰í•  ì‘ì—…ì„ ì§€ì • -> í…ìŠ¤íŠ¸ ìƒì„±\n",
    "                                     \n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í¬ê¸° 12.5 GB  , 100 Mbpsì—ì„œ ì•½ 17ë¶„ ì˜ˆìƒ, ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ì¬ì‹¤í–‰ì‹œëŠ” ìºì‹œì—ì„œ ì½ì–´ì„œ ë°”ë¡œ ì‹¤í–‰ë¨\n",
    "# ë‹¤ìš´ë¡œë“œ ê²½ë¡œ : C:\\Users\\storm\\.cache\\huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aff4ab-b628-4f9c-aa4f-f508834a9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í˜¸ì¶œ (ì¶”ë¡ )  : # CPU í™˜ê²½ì—ì„œ ìˆ˜ì‹­ë¶„ ì†Œìš”\n",
    "prompt = \"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\"\n",
    "response = hf.invoke(prompt)  # CPU í™˜ê²½ì—ì„œ ìˆ˜ì‹­ë¶„ ì†Œìš”\n",
    "\n",
    "print(\"ì§ˆë¬¸:\", prompt)\n",
    "print(\"ëª¨ë¸ ì‘ë‹µ:\", response)\n",
    "\n",
    "# ì¼ë°˜ ë°ìŠ¤í¬íƒ‘ (i5, 16GB RAM) 6~8ì½”ì–´ --> 20 ~ 40ë¶„\n",
    "# í† í° ìƒì„± ì†ë„ ì´ˆë‹¹ 0.5~1 í† í°n\n",
    "# CUDA ì§€ì› GPU (ì˜ˆ: RTX 3060 ì´ìƒ) device_map=\"auto\" ë˜ëŠ” device=0 ì„¤ì •ìœ¼ë¡œ GPUë¡œ ì „í™˜ â†’ 512í† í° ê¸°ì¤€ 5 ~ 10ì´ˆ ì´ë‚´ ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761e98e-0263-455f-a4d2-8ef0dff94723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUê°€ ìˆì„ ê²½ìš° : device_map=\"auto\" ë˜ëŠ” device=0 ìœ¼ë¡œ ì„¤ì •\n",
    "# GPU ì—†ìœ¼ë©´ ì˜¤ë¥˜\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,\n",
    "    # model_kwargs={\"device_map\": \"auto\"},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í˜¸ì¶œ (ì¶”ë¡ )\n",
    "prompt = \"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\"\n",
    "response = hf.invoke(prompt)\n",
    "\n",
    "print(\"ì§ˆë¬¸:\", prompt)\n",
    "print(\"ëª¨ë¸ ì‘ë‹µ:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b157bf2-ba5d-4edc-a959-3e732a5eec81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1645ece5a58e4b2b9f784c7b2a5ef63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"EleutherAI/polyglot-ko-1.3b\", # ì„±ëŠ¥ ë‚®ìŒ\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100, \"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€? ê·¸ ì´ìœ ëŠ”\")\n",
    "print(response)\n",
    "# ëª¨ë¸ í¬ê¸° ì•½ 2.1 GB  , 100 Mbpsì—ì„œ ì•½ 3~5ë¶„ ì˜ˆìƒ, ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ì¬ì‹¤í–‰ì‹œëŠ” ìºì‹œì—ì„œ ì½ì–´ì„œ ë°”ë¡œ ì‹¤í–‰ë¨\n",
    "# ë‹¤ìš´ë¡œë“œ ê²½ë¡œ : C:\\Users\\storm\\.cache\\huggingface\n",
    "\n",
    "# Polyglot-Ko-1.3BëŠ” â€œInstruction (ì§ˆë¬¸-ë‹µë³€)â€ ëª¨ë¸ì´ ì•„ë‹˜\n",
    "# ì´ ëª¨ë¸ì€ ë‹¨ìˆœ ì–¸ì–´ ëª¨ë¸ (Language Model) ì…ë‹ˆë‹¤.\n",
    "# â€œQ&A êµ¬ì¡°â€ë¡œ í•™ìŠµëœ ê²Œ ì•„ë‹ˆë¼ â€œë¬¸ì¥ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ”â€ í˜•íƒœë¡œ í•™ìŠµë¨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63b16a-30c6-4d58-b168-a8e6afa02a29",
   "metadata": {},
   "source": [
    "### GPT4All\n",
    "GPT4Allì€ Nomic AIê°€ ê°œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ë¡œì»¬ LLM(Local Large Language Model) í”„ë ˆì„ì›Œí¬ì´ë‹¤. <br>\n",
    "ì´ ëª¨ë¸ì€ ì¸í„°ë„· ì—°ê²° ì—†ì´ ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë©°, GPU ì—†ì´ë„ CPU ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡  ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì´ íŠ¹ì§•ì´ë‹¤. <br>\n",
    "https://www.nomic.ai/gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8e6f4-76e5-4f13-af00-7236cb6eb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install  gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d66a3c-d0b0-4e74-b5b0-6736513fd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTP4All Falcon ëª¨ë¸ : (3.9GB) , í•œêµ­ì–´ ì§€ì› ì•ˆë¨\n",
    "# ìœˆë„ìš° ë©”ë‰´ì˜ GTP4All í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•´ì„œ ëª¨ë¸ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•´ ë†“ì€ ë‹¤ìŒ ì‹¤í–‰\n",
    "# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ:  C:\\Users\\storm\\AppData\\Local\\nomic.ai\\GPT4All\\gpt4all-falcon-newbpe-q4_0.gguf\n",
    "\n",
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "model_path = \"C:/Users/storm/AppData/Local/nomic.ai/GPT4All/gpt4all-falcon-newbpe-q4_0.gguf\"\n",
    "\n",
    "llm = GPT4All(model=model_path)\n",
    "response = llm.invoke(\"Why is the sky blue?\") # í•œêµ­ì–´ëŠ” ì§€ì› ì•ˆë¨\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf6cea-dfcb-4bc1-9038-7287921a64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf í•œêµ­ì–´ ì§€ì› ëª¨ë¸ : 5.7GB \n",
    "# ìœˆë„ìš° ë©”ë‰´ì˜ GTP4All í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•´ì„œ ëª¨ë¸ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•´ ë†“ì€ ë‹¤ìŒ ì‹¤í–‰\n",
    "# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ:  C:\\Users\\storm\\AppData\\Local\\nomic.ai\\GPT4All\\EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf\n",
    "\n",
    "rom langchain_community.llms import GPT4All\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ \n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
    "<s>Human: {question}</s>\n",
    "<s>Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "#  ëª¨ë¸ ì´ˆê¸°í™”\n",
    "# modelëŠ” GPT4All ëª¨ë¸ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •\n",
    "model_path = \"C:/Users/storm/AppData/Local/nomic.ai/GPT4All/EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf\"\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=model_path,\n",
    "    # backend=\"gpu\",  # GPU ì„¤ì •\n",
    "    streaming=True,   # ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],  # ì½œë°± ì„¤ì •\n",
    ")\n",
    "\n",
    "# ì²´ì¸ ìƒì„±\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "response = chain.invoke({\"question\": \"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\"})   # CPUì—ì„œ ë™ì‘ ì‘ë‹µ ëŠë¦¼\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f6db3-46b8-48af-a5e3-1639f303f9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
