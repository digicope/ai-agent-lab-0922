{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a130c772-5ef1-4d51-b13c-62a48bf2228e",
   "metadata": {},
   "source": [
    "## LLM 모델\n",
    "LangChain에서 LLM은 대규모 언어 모델(GPT, Claude, Gemini, Llama 등)을 일관된 인터페이스로 연결해 텍스트 생성·분석을 수행하는 핵심 엔진이다.<br>\n",
    "LLM은 AI의 뇌 역할을 하며 LangChain은 이를 체인·에이전트 구조로 확장해 다양한 응용 서비스를 구현한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461cd27f-fd21-4e62-94ac-cc1f413c29cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일의 내용 불러오기\n",
    "load_dotenv(\"C:/env/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8cef9-1cc2-4dea-9794-83823dad80ed",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c827a797-3b73-4cb0-a459-4bbebd9f4302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘이 파란 이유는 대기 중의 산란 현상 때문입니다. 태양에서 나오는 빛은 여러 색깔의 빛으로 구성되어 있으며, 각각의 색깔은 서로 다른 파장을 가지고 있습니다. 파란색 빛의 파장은 다른 색보다 짧기 때문에 대기 중의 분자와 가장 많이 산란됩니다. \n",
      "\n",
      "이 과정은 \" 레일리 산란(Rayleigh scattering)\"이라고 부르며, 태양의 빛이 대기를 통과할 때 파란색 빛이 더 많이 산란되어 우리의 눈에 더 많이 도달하게 됩니다. 그래서 우리는 하늘이 파랗게 보이는 것입니다. 해가 뜨거나 질 때, 태양의 빛이 대기를 더 많이 통과하게 되면 붉은색이나 주황색 빛이 더 많이 산란되어 하늘이 붉게 보이기도 합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "response= model.invoke(\"하늘은 왜 파란가요?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf485e30-17c7-4470-9add-08fa52bc521d",
   "metadata": {},
   "source": [
    "### Anthropic\n",
    "https://docs.claude.com/en/docs/about-claude/models/overview?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b97f36-b891-4e20-b622-a8c4eae003aa",
   "metadata": {},
   "source": [
    ".env 파일에 ANTHROPIC_API_KEY=\"YOUR_API_KEY\" 를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf36cda-a9f7-4ba2-8035-43a0df0aa4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-anthropic\n",
    "# notebook Kernel Restart!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11987ec2-5b61-4e80-9661-5b5d4b6eccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 하늘이 파란 이유\n",
      "\n",
      "하늘이 파랗게 보이는 것은 **레일리 산란(Rayleigh scattering)** 때문입니다.\n",
      "\n",
      "## 원리\n",
      "\n",
      "1. **태양빛의 구성**\n",
      "   - 태양빛은 여러 색깔(파장)이 섞인 백색광입니다\n",
      "\n",
      "2. **대기와의 상호작용**\n",
      "   - 태양빛이 대기를 통과할 때 공기 분자와 충돌합니다\n",
      "   - 파란색 계열의 짧은 파장은 사방으로 더 많이 산란됩니다\n",
      "   - 빨간색 계열의 긴 파장은 비교적 직진합니다\n",
      "\n",
      "3. **우리 눈에 보이는 것**\n",
      "   - 산란된 파란 빛이 사방에서 우리 눈에 들어옵니다\n",
      "   - 그래서 하늘 전체가 파랗게 보입니다\n",
      "\n",
      "## 다른 현상들\n",
      "\n",
      "- **일몰이 붉은 이유**: 태양빛이 더 긴 거리를 통과하면서 파란빛은 모두 산란되고 빨간빛만 남기 때문\n",
      "- **우주가 검은 이유**: 대기가 없어 산란이 일어나지 않기 때문\n",
      "\n",
      "간단히 말하면, 파란색 빛이 공기 분자에 의해 더 잘 흩어지기 때문에 하늘이 파랗게 보이는 것입니다!\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 모델 초기화\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\",  # 사용 가능한 버전: opus, sonnet, haiku 등\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 메시지 구성\n",
    "messages = [HumanMessage(content=\"하늘은 왜 파란가요?\")]\n",
    "\n",
    "# 모델 응답\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49857d7a-218f-49fc-885d-cc480e0042a9",
   "metadata": {},
   "source": [
    "### Google Gemini\n",
    "https://ai.google.dev/gemini-api/docs/models/gemini?hl=ko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e5970-2bba-4b32-b447-d32125e1b884",
   "metadata": {},
   "source": [
    ".env 파일에 GOOGLE_API_KEY=\"YOUR_API_KEY\"를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccdbb860-fd62-4b9f-8a27-ef4eeec27c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0230888-984e-499a-8ada-3e6da67e1eb1",
   "metadata": {},
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# 모델 초기화\n",
    "# model: \"gemini-2.5-pro-latest\" 또는 \"gemini-2.5-flash-latest\", \"gemini-pro\" 등을 사용할 수 있습니다.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\") \n",
    "\n",
    "# 메시지 구성\n",
    "prompt = \"안녕하세요! 나는 홍길동입니다.\"\n",
    "\n",
    "# 모델 응답\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e82fe-3887-4a7b-9bfd-d5c4980946cb",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "Ollama는 로컬 환경에서 Llama 3, Mistral, Gemma 등 다양한 LLM을 직접 실행할 수 있게 해주는 오픈소스 플랫폼이다.\n",
    "클라우드 요금 없이 빠르고 보안성 높게 AI 모델을 구동·연동할 수 있다는 점이 가장 큰 장점이다. <br>\n",
    " https://ollama.com/\n",
    " \n",
    "- Llama 3: 성능 최고, 논리·코딩·대화 품질이 가장 우수하지만 모델이 커서 속도는 다소 느림. <br>\n",
    "- Mistral 7B : 성능과 속도의 균형형, 중간 크기 모델로 가볍고 빠르며 추론력도 양호. <br>\n",
    "- Gemma 2 / 3 : ⚡속도 가장 빠름, 작은 환경에 적합하지만 이해력·정확도는 낮음. <br>\n",
    "고성능이 목표라면 Llama 3, 속도·효율을 원하면 Mistral, 저사양 환경이면 Gemma가 가장 적합하다. <br>\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ff3cfb9-c17d-43c7-bc1e-91b833479b92",
   "metadata": {},
   "source": [
    "https://ollama.com/ 에서 OllamaSetup.exe 파일 다운로드 (1.12GB) 설치\n",
    "Ollama 실행 후 \"gemma3:4b\" 모델 다운로드 후 진행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670b03b8-4c45-48ae-9dd9-3725f845c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e013e6-3e5d-48c2-9e07-9960ad620bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘이 파랗게 보이는 현상은 **빛의 산란(Rayleigh scattering)** 현상 때문입니다. 좀 더 자세히 설명해 드릴게요.\n",
      "\n",
      "**1. 햇빛의 구성:**\n",
      "\n",
      "*   햇빛은 여러 색깔의 빛(빨강, 주황, 노랑, 초록, 파랑, 남색, 보라 등)이 섞여 있습니다.\n",
      "*   각 색깔의 빛은 고유한 파장의 특성을 가지고 있죠.\n",
      "\n",
      "**2. 대기권의 산란:**\n",
      "\n",
      "*   햇빛이 지구 대기권으로 들어오면, 대기 중의 작은 입자(질소, 산소 등)에 의해 빛이 산란됩니다.\n",
      "*   파장이 짧은 빛(파랑, 남색 등)은 짧은 거리를 이동하는 데 더 쉽게 산란됩니다.\n",
      "\n",
      "**3. Rayleigh scattering의 원리:**\n",
      "\n",
      "*   Rayleigh scattering는 빛이 대기 중의 입자(질소, 수증기 등)에 의해 흩어지는 현상을 말합니다.\n",
      "*   파장이 짧은 파란 빛은 파장이 긴 빨강 빛보다 더 잘 흩어지기 때문에 하늘을 파랗게 만드는 것입니다.\n",
      "\n",
      "**4. 구름은 백색으로 보이는 이유:**\n",
      "\n",
      "*   구름은 수증기가 만들어진 것으로, 빛이 닿는 정도에 따라 다양한 색깔의 빛을 띠게 됩니다.\n",
      "*   파란 빛을 더 많이 띠는 구름은 하늘 전체를 파랗게 만들어주는 역할을 합니다. 하지만 실제로는 구름의 색깔이 파란색을 띠는 것이 아니라, 빛의 산란에 의해 다른 색깔의 빛이 더 많이 섞여 보이는 것이죠.\n",
      "\n",
      "**요약:**\n",
      "\n",
      "하늘이 파랗게 보이는 이유는 햇빛이 대기 중의 작은 입자에 의해 산란되어 파란색 빛이 더 많이 흩어지기 때문입니다.\n",
      "\n",
      "혹시 더 궁금한 점이 있으신가요? 😊\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Ollama 모델을 불러옵니다.\n",
    "llm = ChatOllama(model=\"gemma3:4b\")\n",
    "\n",
    "response= llm.invoke(\"하늘은 왜 파란가요?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f3a87b-ab4b-45df-98a3-84249ab1fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Ollama 모델을 불러옵니다.\n",
    "llm = ChatOllama(model=\"gemma3:12b\")\n",
    "\n",
    "response= llm.invoke(\"하늘은 왜 파란가요?\")\n",
    "print(response.content)\n",
    "# 응답 오래 걸림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f3715-eed1-468d-b0d3-66dc2edeb99a",
   "metadata": {},
   "source": [
    "### HuggingFacePipeline\n",
    "HuggingFacePipeline은 LangChain에서 Hugging Face의 Transformer 모델을 직접 호출할 수 있도록 연결해주는 클래스이다.<br>\n",
    "OpenAI API처럼 외부 LLM을 쓰지 않고 로컬 또는 Hugging Face Hub 모델을 바로 사용할 수 있게 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668591fd-2048-44e1-953d-6c4ce4358524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch  langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "735b595c-0dad-4c1d-95e8-7bea9ab891bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain Why the sky is blue.\n",
      "\n",
      "That's why the \"Blue Sky\" is so great. It's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# 1. Hugging Face 파이프라인 생성\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",     # 영어 전용 모델    ,  성능 낮음      \n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 2. LangChain용 래퍼로 감싸기\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 3. 모델 호출\n",
    "response = llm.invoke(\"Explain Why the sky is blue.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4fb54a-d541-4426-9b98-876c3f9d6992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abe22e120824c54ba93ae83963afc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# HuggingFace 모델을 다운로드\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # 사용할 모델의 ID\n",
    "    task=\"text-generation\",          # 수행할 작업을 지정 -> 텍스트 생성\n",
    "                                     \n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "# 모델 크기 12.5 GB  , 100 Mbps에서 약 17분 예상, 다운로드 완료 후 재실행시는 캐시에서 읽어서 바로 실행됨\n",
    "# 다운로드 경로 : C:\\Users\\storm\\.cache\\huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aff4ab-b628-4f9c-aa4f-f508834a9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 호출 (추론)  : # CPU 환경에서 수십분 소요\n",
    "prompt = \"하늘은 왜 파란가요?\"\n",
    "response = hf.invoke(prompt)  # CPU 환경에서 수십분 소요\n",
    "\n",
    "print(\"질문:\", prompt)\n",
    "print(\"모델 응답:\", response)\n",
    "\n",
    "# 일반 데스크탑 (i5, 16GB RAM) 6~8코어 --> 20 ~ 40분\n",
    "# 토큰 생성 속도 초당 0.5~1 토큰n\n",
    "# CUDA 지원 GPU (예: RTX 3060 이상) device_map=\"auto\" 또는 device=0 설정으로 GPU로 전환 → 512토큰 기준 5 ~ 10초 이내 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761e98e-0263-455f-a4d2-8ef0dff94723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU가 있을 경우 : device_map=\"auto\" 또는 device=0 으로 설정\n",
    "# GPU 없으면 오류\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,\n",
    "    # model_kwargs={\"device_map\": \"auto\"},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "# 모델 호출 (추론)\n",
    "prompt = \"하늘은 왜 파란가요?\"\n",
    "response = hf.invoke(prompt)\n",
    "\n",
    "print(\"질문:\", prompt)\n",
    "print(\"모델 응답:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b157bf2-ba5d-4edc-a959-3e732a5eec81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7c2e8bbac0442a9f9d52739c38673c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘은 왜 파란가? 그 이유는?’ 이렇게 생각하는 것은 어리석은 일입니다. 우리가 ‘하늘이 왜 파랗지?’라고 생각하는 것은 우리의 마음이 어리석은 데가 있기 때문입니다. 우리가 ‘저 하늘에는 무엇이 있을까?’ 하고 생각하는 것도 역시 어리석은 생각입니다. 그것은 우리가 그 하늘을 생각할 줄 모르고, 또 그 하늘을 알려고도 하지 않기 때문입니다. 이렇게 생각하는 것이\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"EleutherAI/polyglot-ko-1.3b\", # 성능 낮음\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100, \"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"하늘은 왜 파란가? 그 이유는\")\n",
    "print(response)\n",
    "# 모델 크기 약 2.1 GB  , 100 Mbps에서 약 3~5분 예상, 다운로드 완료 후 재실행시는 캐시에서 읽어서 바로 실행됨\n",
    "# 다운로드 경로 : C:\\Users\\storm\\.cache\\huggingface\n",
    "\n",
    "# Polyglot-Ko-1.3B는 “Instruction (질문-답변)” 모델이 아님\n",
    "# 이 모델은 단순 언어 모델 (Language Model) 입니다.\n",
    "# “Q&A 구조”로 학습된 게 아니라 “문장 다음에 올 단어를 예측하는” 형태로 학습됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63b16a-30c6-4d58-b168-a8e6afa02a29",
   "metadata": {},
   "source": [
    "### GPT4All\n",
    "GPT4All은 Nomic AI가 개발한 오픈소스 로컬 LLM(Local Large Language Model) 프레임워크이다. <br>\n",
    "이 모델은 인터넷 연결 없이 실행할 수 있으며, GPU 없이도 CPU 기반으로 추론 가능하다는 점이 특징이다. <br>\n",
    "https://www.nomic.ai/gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8e6f4-76e5-4f13-af00-7236cb6eb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install  gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d66a3c-d0b0-4e74-b5b0-6736513fd14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The sky appears blue because of a process called Rayleigh scattering. When sunlight passes through the Earth's atmosphere, it scatters in all directions, including towards the blue end of the visible spectrum. This scattering causes the blue light to be scattered more than other colors, making the sky appear blue.\n"
     ]
    }
   ],
   "source": [
    "# GTP4All Falcon 모델 : (3.9GB) , 한국어 지원 안됨\n",
    "# 윈도우 메뉴의 GTP4All 프로그램을 실행해서 모델을 미리 다운로드해 놓은 다음 실행\n",
    "# 모델 다운로드 경로:  C:\\Users\\storm\\AppData\\Local\\nomic.ai\\GPT4All\\gpt4all-falcon-newbpe-q4_0.gguf\n",
    "\n",
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "model_path = \"C:/Users/storm/AppData/Local/nomic.ai/GPT4All/gpt4all-falcon-newbpe-q4_0.gguf\"\n",
    "\n",
    "llm = GPT4All(model=model_path)\n",
    "response = llm.invoke(\"Why is the sky blue?\") # 한국어는 지원 안됨\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bcf6cea-dfcb-4bc1-9038-7287921a64e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘이 파란색으로 보이는 이유는 대기 중 산소와 질소가 포함된 가스 분자들이 태양광을 흡수하고 재방출하기 때문입니다. 이 과정은 빛의 산란이라고 알려져 있습니다.\n",
      "\n",
      "태양에서 나오는 햇빛은 주로 파랑과 보라색 같은 단파장 색깔로 구성되어 있으며, 이는 공기 입자에 의해 더 잘 산란됩니다. 대기 중 가스 분자들은 태양광을 흡수하고 재방출하는데, 이 과정에서 일부 빛을 다시 지구 표면으로 되돌려 보내게 됩니다.\n",
      "\n",
      "이 빛의 산란은 하늘에 파란색을 부여합니다. 하지만 실제로는 햇빛과 공기 입자 사이의 상호작용 때문에 다양한 색조가 나타납니다. 예를 들어, 대기 중 수증기가 태양광을 흡수하고 재방출하면 하늘이 더 푸른색으로 보일 수 있습니다.\n",
      "\n",
      "하늘이 파란색으로 보이는 이유는 대기 중 산소와 질소가 포함된 가스 분자들이 태양광을 흡수하고 재방출하기 때문입니다. 이 과정은 빛의 산란이라고 알려져 있습니다.는 대기 중 물방울 수가 많아져 햇빛을 더 강하게 산란시켜 하늘을 더욱 선명하게 보이게 합니다.\n",
      "\n",
      "태양에서 나오는 햇빛은 주로 파랑과 보라색 같은 단파장 색깔로 구성되어 있으며, 이는 공기 입자에 의해 더 잘 산란됩니다. 대기 중 가스 분자들은 태양광을 흡수하고 재방출하는데, 이 과정에서 일부 빛을 다시 지구 표면으로 되돌려 보내게 됩니다.\n",
      "\n",
      "이 빛의 산란은 하늘에 파란색을 부여합니다. 하지만 실제로는 햇빛과 공기 입자 사이의 상호작용 때문에 다양한 색조가 나타납니다. 예를 들어, 대기 중 수증기가 태양광을 흡수하고 재방출하면 하늘이 더 푸른색으로 보일 수 있습니다.\n",
      "\n",
      "또한, 지평선 근처의 구름이나 먼지 같은 다른 요소들도 빛 산란에 영향을 줄 수 있으며, 이로 인해 하늘 색깔이 달라질 수 있습니다. 예를 들어, 해질녘과 새벽에는 대기 중 물방울 수가 많아져 햇빛을 더 강하게 산란시켜 하늘을 더욱 선명하게 보이게 합니다.\n"
     ]
    }
   ],
   "source": [
    "# EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf 한국어 지원 모델 : 5.7GB \n",
    "# 윈도우 메뉴의 GTP4All 프로그램을 실행해서 모델을 미리 다운로드해 놓은 다음 실행\n",
    "# 모델 다운로드 경로:  C:\\Users\\storm\\AppData\\Local\\nomic.ai\\GPT4All\\EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf\n",
    "\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 프롬프트 \n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
    "<s>Human: {question}</s>\n",
    "<s>Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "#  모델 초기화\n",
    "# model는 GPT4All 모델 파일의 경로를 지정\n",
    "model_path = \"C:/Users/storm/AppData/Local/nomic.ai/GPT4All/EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf\"\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=model_path,\n",
    "    # backend=\"gpu\",  # GPU 설정\n",
    "    streaming=True,   # 스트리밍 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],  # 콜백 설정\n",
    ")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 실행\n",
    "response = chain.invoke({\"question\": \"하늘은 왜 파란가요?\"})   # CPU에서 동작 응답 느림\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f6db3-46b8-48af-a5e3-1639f303f9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
