{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a130c772-5ef1-4d51-b13c-62a48bf2228e",
   "metadata": {},
   "source": [
    "## LLM ëª¨ë¸\n",
    "LangChainì—ì„œ LLMì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(GPT, Claude, Gemini, Llama ë“±)ì„ ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì—°ê²°í•´ í…ìŠ¤íŠ¸ ìƒì„±Â·ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” í•µì‹¬ ì—”ì§„ì´ë‹¤.<br>\n",
    "LLMì€ AIì˜ ë‡Œ ì—­í• ì„ í•˜ë©° LangChainì€ ì´ë¥¼ ì²´ì¸Â·ì—ì´ì „íŠ¸ êµ¬ì¡°ë¡œ í™•ì¥í•´ ë‹¤ì–‘í•œ ì‘ìš© ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•œë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461cd27f-fd21-4e62-94ac-cc1f413c29cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ì˜ ë‚´ìš© ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "load_dotenv(\"C:/env/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8cef9-1cc2-4dea-9794-83823dad80ed",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c827a797-3b73-4cb0-a459-4bbebd9f4302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•˜ëŠ˜ì´ íŒŒë€ ì´ìœ ëŠ” ëŒ€ê¸° ì¤‘ì˜ ì‚°ë€ í˜„ìƒ ë•Œë¬¸ì…ë‹ˆë‹¤. íƒœì–‘ì—ì„œ ë‚˜ì˜¤ëŠ” ë¹›ì€ ì—¬ëŸ¬ ìƒ‰ê¹”ì˜ ë¹›ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ê°ê°ì˜ ìƒ‰ê¹”ì€ ì„œë¡œ ë‹¤ë¥¸ íŒŒì¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. íŒŒë€ìƒ‰ ë¹›ì˜ íŒŒì¥ì€ ë‹¤ë¥¸ ìƒ‰ë³´ë‹¤ ì§§ê¸° ë•Œë¬¸ì— ëŒ€ê¸° ì¤‘ì˜ ë¶„ìì™€ ê°€ì¥ ë§ì´ ì‚°ë€ë©ë‹ˆë‹¤. \n",
      "\n",
      "ì´ ê³¼ì •ì€ \" ë ˆì¼ë¦¬ ì‚°ë€(Rayleigh scattering)\"ì´ë¼ê³  ë¶€ë¥´ë©°, íƒœì–‘ì˜ ë¹›ì´ ëŒ€ê¸°ë¥¼ í†µê³¼í•  ë•Œ íŒŒë€ìƒ‰ ë¹›ì´ ë” ë§ì´ ì‚°ë€ë˜ì–´ ìš°ë¦¬ì˜ ëˆˆì— ë” ë§ì´ ë„ë‹¬í•˜ê²Œ ë©ë‹ˆë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” í•˜ëŠ˜ì´ íŒŒë—ê²Œ ë³´ì´ëŠ” ê²ƒì…ë‹ˆë‹¤. í•´ê°€ ëœ¨ê±°ë‚˜ ì§ˆ ë•Œ, íƒœì–‘ì˜ ë¹›ì´ ëŒ€ê¸°ë¥¼ ë” ë§ì´ í†µê³¼í•˜ê²Œ ë˜ë©´ ë¶‰ì€ìƒ‰ì´ë‚˜ ì£¼í™©ìƒ‰ ë¹›ì´ ë” ë§ì´ ì‚°ë€ë˜ì–´ í•˜ëŠ˜ì´ ë¶‰ê²Œ ë³´ì´ê¸°ë„ í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "response= model.invoke(\"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf485e30-17c7-4470-9add-08fa52bc521d",
   "metadata": {},
   "source": [
    "### Anthropic\n",
    "https://docs.claude.com/en/docs/about-claude/models/overview?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b97f36-b891-4e20-b622-a8c4eae003aa",
   "metadata": {},
   "source": [
    ".env íŒŒì¼ì— ANTHROPIC_API_KEY=\"YOUR_API_KEY\" ë¥¼ ì¶”ê°€í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf36cda-a9f7-4ba2-8035-43a0df0aa4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-anthropic\n",
    "# notebook Kernel Restart!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11987ec2-5b61-4e80-9661-5b5d4b6eccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# í•˜ëŠ˜ì´ íŒŒë€ ì´ìœ \n",
      "\n",
      "í•˜ëŠ˜ì´ íŒŒë—ê²Œ ë³´ì´ëŠ” ê²ƒì€ **ë ˆì¼ë¦¬ ì‚°ë€(Rayleigh scattering)** ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
      "\n",
      "## ì›ë¦¬\n",
      "\n",
      "1. **íƒœì–‘ë¹›ì˜ êµ¬ì„±**\n",
      "   - íƒœì–‘ë¹›ì€ ì—¬ëŸ¬ ìƒ‰ê¹”(íŒŒì¥)ì´ ì„ì¸ ë°±ìƒ‰ê´‘ì…ë‹ˆë‹¤\n",
      "\n",
      "2. **ëŒ€ê¸°ì™€ì˜ ìƒí˜¸ì‘ìš©**\n",
      "   - íƒœì–‘ë¹›ì´ ëŒ€ê¸°ë¥¼ í†µê³¼í•  ë•Œ ê³µê¸° ë¶„ìì™€ ì¶©ëŒí•©ë‹ˆë‹¤\n",
      "   - íŒŒë€ìƒ‰ ê³„ì—´ì˜ ì§§ì€ íŒŒì¥ì€ ì‚¬ë°©ìœ¼ë¡œ ë” ë§ì´ ì‚°ë€ë©ë‹ˆë‹¤\n",
      "   - ë¹¨ê°„ìƒ‰ ê³„ì—´ì˜ ê¸´ íŒŒì¥ì€ ë¹„êµì  ì§ì§„í•©ë‹ˆë‹¤\n",
      "\n",
      "3. **ìš°ë¦¬ ëˆˆì— ë³´ì´ëŠ” ê²ƒ**\n",
      "   - ì‚°ë€ëœ íŒŒë€ ë¹›ì´ ì‚¬ë°©ì—ì„œ ìš°ë¦¬ ëˆˆì— ë“¤ì–´ì˜µë‹ˆë‹¤\n",
      "   - ê·¸ë˜ì„œ í•˜ëŠ˜ ì „ì²´ê°€ íŒŒë—ê²Œ ë³´ì…ë‹ˆë‹¤\n",
      "\n",
      "## ë‹¤ë¥¸ í˜„ìƒë“¤\n",
      "\n",
      "- **ì¼ëª°ì´ ë¶‰ì€ ì´ìœ **: íƒœì–‘ë¹›ì´ ë” ê¸´ ê±°ë¦¬ë¥¼ í†µê³¼í•˜ë©´ì„œ íŒŒë€ë¹›ì€ ëª¨ë‘ ì‚°ë€ë˜ê³  ë¹¨ê°„ë¹›ë§Œ ë‚¨ê¸° ë•Œë¬¸\n",
      "- **ìš°ì£¼ê°€ ê²€ì€ ì´ìœ **: ëŒ€ê¸°ê°€ ì—†ì–´ ì‚°ë€ì´ ì¼ì–´ë‚˜ì§€ ì•Šê¸° ë•Œë¬¸\n",
      "\n",
      "ê°„ë‹¨íˆ ë§í•˜ë©´, íŒŒë€ìƒ‰ ë¹›ì´ ê³µê¸° ë¶„ìì— ì˜í•´ ë” ì˜ í©ì–´ì§€ê¸° ë•Œë¬¸ì— í•˜ëŠ˜ì´ íŒŒë—ê²Œ ë³´ì´ëŠ” ê²ƒì…ë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\",  # ì‚¬ìš© ê°€ëŠ¥í•œ ë²„ì „: opus, sonnet, haiku ë“±\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ë©”ì‹œì§€ êµ¬ì„±\n",
    "messages = [HumanMessage(content=\"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\")]\n",
    "\n",
    "# ëª¨ë¸ ì‘ë‹µ\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49857d7a-218f-49fc-885d-cc480e0042a9",
   "metadata": {},
   "source": [
    "### Google Gemini\n",
    "https://ai.google.dev/gemini-api/docs/models/gemini?hl=ko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e5970-2bba-4b32-b447-d32125e1b884",
   "metadata": {},
   "source": [
    ".env íŒŒì¼ì— GOOGLE_API_KEY=\"YOUR_API_KEY\"ë¥¼ ì¶”ê°€í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccdbb860-fd62-4b9f-8a27-ef4eeec27c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0230888-984e-499a-8ada-3e6da67e1eb1",
   "metadata": {},
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "# model: \"gemini-2.5-pro-latest\" ë˜ëŠ” \"gemini-2.5-flash-latest\", \"gemini-pro\" ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\") \n",
    "\n",
    "# ë©”ì‹œì§€ êµ¬ì„±\n",
    "prompt = \"ì•ˆë…•í•˜ì„¸ìš”! ë‚˜ëŠ” í™ê¸¸ë™ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "# ëª¨ë¸ ì‘ë‹µ\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e82fe-3887-4a7b-9bfd-d5c4980946cb",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "OllamaëŠ” ë¡œì»¬ í™˜ê²½ì—ì„œ Llama 3, Mistral, Gemma ë“± ë‹¤ì–‘í•œ LLMì„ ì§ì ‘ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ í”Œë«í¼ì´ë‹¤.\n",
    "í´ë¼ìš°ë“œ ìš”ê¸ˆ ì—†ì´ ë¹ ë¥´ê³  ë³´ì•ˆì„± ë†’ê²Œ AI ëª¨ë¸ì„ êµ¬ë™Â·ì—°ë™í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ê°€ì¥ í° ì¥ì ì´ë‹¤. <br>\n",
    " https://ollama.com/\n",
    " \n",
    "- Llama 3: ì„±ëŠ¥ ìµœê³ , ë…¼ë¦¬Â·ì½”ë”©Â·ëŒ€í™” í’ˆì§ˆì´ ê°€ì¥ ìš°ìˆ˜í•˜ì§€ë§Œ ëª¨ë¸ì´ ì»¤ì„œ ì†ë„ëŠ” ë‹¤ì†Œ ëŠë¦¼. <br>\n",
    "- Mistral 7B : ì„±ëŠ¥ê³¼ ì†ë„ì˜ ê· í˜•í˜•, ì¤‘ê°„ í¬ê¸° ëª¨ë¸ë¡œ ê°€ë³ê³  ë¹ ë¥´ë©° ì¶”ë¡ ë ¥ë„ ì–‘í˜¸. <br>\n",
    "- Gemma 2 / 3 : âš¡ì†ë„ ê°€ì¥ ë¹ ë¦„, ì‘ì€ í™˜ê²½ì— ì í•©í•˜ì§€ë§Œ ì´í•´ë ¥Â·ì •í™•ë„ëŠ” ë‚®ìŒ. <br>\n",
    "ê³ ì„±ëŠ¥ì´ ëª©í‘œë¼ë©´ Llama 3, ì†ë„Â·íš¨ìœ¨ì„ ì›í•˜ë©´ Mistral, ì €ì‚¬ì–‘ í™˜ê²½ì´ë©´ Gemmaê°€ ê°€ì¥ ì í•©í•˜ë‹¤. <br>\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ff3cfb9-c17d-43c7-bc1e-91b833479b92",
   "metadata": {},
   "source": [
    "https://ollama.com/ ì—ì„œ OllamaSetup.exe íŒŒì¼ ë‹¤ìš´ë¡œë“œ (1.12GB) ì„¤ì¹˜\n",
    "Ollama ì‹¤í–‰ í›„ \"gemma3:4b\" ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í›„ ì§„í–‰í•œë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670b03b8-4c45-48ae-9dd9-3725f845c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e013e6-3e5d-48c2-9e07-9960ad620bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•˜ëŠ˜ì´ íŒŒë—ê²Œ ë³´ì´ëŠ” í˜„ìƒì€ **ë¹›ì˜ ì‚°ë€(Rayleigh scattering)** í˜„ìƒ ë•Œë¬¸ì…ë‹ˆë‹¤. ì¢€ ë” ìì„¸íˆ ì„¤ëª…í•´ ë“œë¦´ê²Œìš”.\n",
      "\n",
      "**1. í–‡ë¹›ì˜ êµ¬ì„±:**\n",
      "\n",
      "*   í–‡ë¹›ì€ ì—¬ëŸ¬ ìƒ‰ê¹”ì˜ ë¹›(ë¹¨ê°•, ì£¼í™©, ë…¸ë‘, ì´ˆë¡, íŒŒë‘, ë‚¨ìƒ‰, ë³´ë¼ ë“±)ì´ ì„ì—¬ ìˆìŠµë‹ˆë‹¤.\n",
      "*   ê° ìƒ‰ê¹”ì˜ ë¹›ì€ ê³ ìœ í•œ íŒŒì¥ì˜ íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆì£ .\n",
      "\n",
      "**2. ëŒ€ê¸°ê¶Œì˜ ì‚°ë€:**\n",
      "\n",
      "*   í–‡ë¹›ì´ ì§€êµ¬ ëŒ€ê¸°ê¶Œìœ¼ë¡œ ë“¤ì–´ì˜¤ë©´, ëŒ€ê¸° ì¤‘ì˜ ì‘ì€ ì…ì(ì§ˆì†Œ, ì‚°ì†Œ ë“±)ì— ì˜í•´ ë¹›ì´ ì‚°ë€ë©ë‹ˆë‹¤.\n",
      "*   íŒŒì¥ì´ ì§§ì€ ë¹›(íŒŒë‘, ë‚¨ìƒ‰ ë“±)ì€ ì§§ì€ ê±°ë¦¬ë¥¼ ì´ë™í•˜ëŠ” ë° ë” ì‰½ê²Œ ì‚°ë€ë©ë‹ˆë‹¤.\n",
      "\n",
      "**3. Rayleigh scatteringì˜ ì›ë¦¬:**\n",
      "\n",
      "*   Rayleigh scatteringëŠ” ë¹›ì´ ëŒ€ê¸° ì¤‘ì˜ ì…ì(ì§ˆì†Œ, ìˆ˜ì¦ê¸° ë“±)ì— ì˜í•´ í©ì–´ì§€ëŠ” í˜„ìƒì„ ë§í•©ë‹ˆë‹¤.\n",
      "*   íŒŒì¥ì´ ì§§ì€ íŒŒë€ ë¹›ì€ íŒŒì¥ì´ ê¸´ ë¹¨ê°• ë¹›ë³´ë‹¤ ë” ì˜ í©ì–´ì§€ê¸° ë•Œë¬¸ì— í•˜ëŠ˜ì„ íŒŒë—ê²Œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "**4. êµ¬ë¦„ì€ ë°±ìƒ‰ìœ¼ë¡œ ë³´ì´ëŠ” ì´ìœ :**\n",
      "\n",
      "*   êµ¬ë¦„ì€ ìˆ˜ì¦ê¸°ê°€ ë§Œë“¤ì–´ì§„ ê²ƒìœ¼ë¡œ, ë¹›ì´ ë‹¿ëŠ” ì •ë„ì— ë”°ë¼ ë‹¤ì–‘í•œ ìƒ‰ê¹”ì˜ ë¹›ì„ ë ê²Œ ë©ë‹ˆë‹¤.\n",
      "*   íŒŒë€ ë¹›ì„ ë” ë§ì´ ë ëŠ” êµ¬ë¦„ì€ í•˜ëŠ˜ ì „ì²´ë¥¼ íŒŒë—ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” êµ¬ë¦„ì˜ ìƒ‰ê¹”ì´ íŒŒë€ìƒ‰ì„ ë ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë¹›ì˜ ì‚°ë€ì— ì˜í•´ ë‹¤ë¥¸ ìƒ‰ê¹”ì˜ ë¹›ì´ ë” ë§ì´ ì„ì—¬ ë³´ì´ëŠ” ê²ƒì´ì£ .\n",
      "\n",
      "**ìš”ì•½:**\n",
      "\n",
      "í•˜ëŠ˜ì´ íŒŒë—ê²Œ ë³´ì´ëŠ” ì´ìœ ëŠ” í–‡ë¹›ì´ ëŒ€ê¸° ì¤‘ì˜ ì‘ì€ ì…ìì— ì˜í•´ ì‚°ë€ë˜ì–´ íŒŒë€ìƒ‰ ë¹›ì´ ë” ë§ì´ í©ì–´ì§€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
      "\n",
      "í˜¹ì‹œ ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”? ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Ollama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "llm = ChatOllama(model=\"gemma3:4b\")\n",
    "\n",
    "response= llm.invoke(\"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f3a87b-ab4b-45df-98a3-84249ab1fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Ollama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "llm = ChatOllama(model=\"gemma3:12b\")\n",
    "\n",
    "response= llm.invoke(\"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\")\n",
    "print(response.content)\n",
    "# ì‘ë‹µ ì˜¤ë˜ ê±¸ë¦¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f3715-eed1-468d-b0d3-66dc2edeb99a",
   "metadata": {},
   "source": [
    "### HuggingFacePipeline\n",
    "HuggingFacePipelineì€ LangChainì—ì„œ Hugging Faceì˜ Transformer ëª¨ë¸ì„ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ ì—°ê²°í•´ì£¼ëŠ” í´ë˜ìŠ¤ì´ë‹¤.<br>\n",
    "OpenAI APIì²˜ëŸ¼ ì™¸ë¶€ LLMì„ ì“°ì§€ ì•Šê³  ë¡œì»¬ ë˜ëŠ” Hugging Face Hub ëª¨ë¸ì„ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668591fd-2048-44e1-953d-6c4ce4358524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch  langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "735b595c-0dad-4c1d-95e8-7bea9ab891bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain Why the sky is blue.\n",
      "\n",
      "That's why the \"Blue Sky\" is so great. It's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# 1. Hugging Face íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",     # ì˜ì–´ ì „ìš© ëª¨ë¸    ,  ì„±ëŠ¥ ë‚®ìŒ      \n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 2. LangChainìš© ë˜í¼ë¡œ ê°ì‹¸ê¸°\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 3. ëª¨ë¸ í˜¸ì¶œ\n",
    "response = llm.invoke(\"Explain Why the sky is blue.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4fb54a-d541-4426-9b98-876c3f9d6992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abe22e120824c54ba93ae83963afc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# HuggingFace ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # ì‚¬ìš©í•  ëª¨ë¸ì˜ ID\n",
    "    task=\"text-generation\",          # ìˆ˜í–‰í•  ì‘ì—…ì„ ì§€ì • -> í…ìŠ¤íŠ¸ ìƒì„±\n",
    "                                     \n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í¬ê¸° 12.5 GB  , 100 Mbpsì—ì„œ ì•½ 17ë¶„ ì˜ˆìƒ, ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ì¬ì‹¤í–‰ì‹œëŠ” ìºì‹œì—ì„œ ì½ì–´ì„œ ë°”ë¡œ ì‹¤í–‰ë¨\n",
    "# ë‹¤ìš´ë¡œë“œ ê²½ë¡œ : C:\\Users\\storm\\.cache\\huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aff4ab-b628-4f9c-aa4f-f508834a9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í˜¸ì¶œ (ì¶”ë¡ )  : # CPU í™˜ê²½ì—ì„œ ìˆ˜ì‹­ë¶„ ì†Œìš”\n",
    "prompt = \"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\"\n",
    "response = hf.invoke(prompt)  # CPU í™˜ê²½ì—ì„œ ìˆ˜ì‹­ë¶„ ì†Œìš”\n",
    "\n",
    "print(\"ì§ˆë¬¸:\", prompt)\n",
    "print(\"ëª¨ë¸ ì‘ë‹µ:\", response)\n",
    "\n",
    "# ì¼ë°˜ ë°ìŠ¤í¬íƒ‘ (i5, 16GB RAM) 6~8ì½”ì–´ --> 20 ~ 40ë¶„\n",
    "# í† í° ìƒì„± ì†ë„ ì´ˆë‹¹ 0.5~1 í† í°n\n",
    "# CUDA ì§€ì› GPU (ì˜ˆ: RTX 3060 ì´ìƒ) device_map=\"auto\" ë˜ëŠ” device=0 ì„¤ì •ìœ¼ë¡œ GPUë¡œ ì „í™˜ â†’ 512í† í° ê¸°ì¤€ 5 ~ 10ì´ˆ ì´ë‚´ ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761e98e-0263-455f-a4d2-8ef0dff94723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUê°€ ìˆì„ ê²½ìš° : device_map=\"auto\" ë˜ëŠ” device=0 ìœ¼ë¡œ ì„¤ì •\n",
    "# GPU ì—†ìœ¼ë©´ ì˜¤ë¥˜\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,\n",
    "    # model_kwargs={\"device_map\": \"auto\"},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í˜¸ì¶œ (ì¶”ë¡ )\n",
    "prompt = \"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\"\n",
    "response = hf.invoke(prompt)\n",
    "\n",
    "print(\"ì§ˆë¬¸:\", prompt)\n",
    "print(\"ëª¨ë¸ ì‘ë‹µ:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b157bf2-ba5d-4edc-a959-3e732a5eec81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7c2e8bbac0442a9f9d52739c38673c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€? ê·¸ ì´ìœ ëŠ”?â€™ ì´ë ‡ê²Œ ìƒê°í•˜ëŠ” ê²ƒì€ ì–´ë¦¬ì„ì€ ì¼ì…ë‹ˆë‹¤. ìš°ë¦¬ê°€ â€˜í•˜ëŠ˜ì´ ì™œ íŒŒë—ì§€?â€™ë¼ê³  ìƒê°í•˜ëŠ” ê²ƒì€ ìš°ë¦¬ì˜ ë§ˆìŒì´ ì–´ë¦¬ì„ì€ ë°ê°€ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ìš°ë¦¬ê°€ â€˜ì € í•˜ëŠ˜ì—ëŠ” ë¬´ì—‡ì´ ìˆì„ê¹Œ?â€™ í•˜ê³  ìƒê°í•˜ëŠ” ê²ƒë„ ì—­ì‹œ ì–´ë¦¬ì„ì€ ìƒê°ì…ë‹ˆë‹¤. ê·¸ê²ƒì€ ìš°ë¦¬ê°€ ê·¸ í•˜ëŠ˜ì„ ìƒê°í•  ì¤„ ëª¨ë¥´ê³ , ë˜ ê·¸ í•˜ëŠ˜ì„ ì•Œë ¤ê³ ë„ í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ ìƒê°í•˜ëŠ” ê²ƒì´\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"EleutherAI/polyglot-ko-1.3b\", # ì„±ëŠ¥ ë‚®ìŒ\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100, \"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€? ê·¸ ì´ìœ ëŠ”\")\n",
    "print(response)\n",
    "# ëª¨ë¸ í¬ê¸° ì•½ 2.1 GB  , 100 Mbpsì—ì„œ ì•½ 3~5ë¶„ ì˜ˆìƒ, ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ì¬ì‹¤í–‰ì‹œëŠ” ìºì‹œì—ì„œ ì½ì–´ì„œ ë°”ë¡œ ì‹¤í–‰ë¨\n",
    "# ë‹¤ìš´ë¡œë“œ ê²½ë¡œ : C:\\Users\\storm\\.cache\\huggingface\n",
    "\n",
    "# Polyglot-Ko-1.3BëŠ” â€œInstruction (ì§ˆë¬¸-ë‹µë³€)â€ ëª¨ë¸ì´ ì•„ë‹˜\n",
    "# ì´ ëª¨ë¸ì€ ë‹¨ìˆœ ì–¸ì–´ ëª¨ë¸ (Language Model) ì…ë‹ˆë‹¤.\n",
    "# â€œQ&A êµ¬ì¡°â€ë¡œ í•™ìŠµëœ ê²Œ ì•„ë‹ˆë¼ â€œë¬¸ì¥ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ”â€ í˜•íƒœë¡œ í•™ìŠµë¨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63b16a-30c6-4d58-b168-a8e6afa02a29",
   "metadata": {},
   "source": [
    "### GPT4All\n",
    "GPT4Allì€ Nomic AIê°€ ê°œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ë¡œì»¬ LLM(Local Large Language Model) í”„ë ˆì„ì›Œí¬ì´ë‹¤. <br>\n",
    "ì´ ëª¨ë¸ì€ ì¸í„°ë„· ì—°ê²° ì—†ì´ ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë©°, GPU ì—†ì´ë„ CPU ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡  ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì´ íŠ¹ì§•ì´ë‹¤. <br>\n",
    "https://www.nomic.ai/gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8e6f4-76e5-4f13-af00-7236cb6eb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install  gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d66a3c-d0b0-4e74-b5b0-6736513fd14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The sky appears blue because of a process called Rayleigh scattering. When sunlight passes through the Earth's atmosphere, it scatters in all directions, including towards the blue end of the visible spectrum. This scattering causes the blue light to be scattered more than other colors, making the sky appear blue.\n"
     ]
    }
   ],
   "source": [
    "# GTP4All Falcon ëª¨ë¸ : (3.9GB) , í•œêµ­ì–´ ì§€ì› ì•ˆë¨\n",
    "# ìœˆë„ìš° ë©”ë‰´ì˜ GTP4All í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•´ì„œ ëª¨ë¸ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•´ ë†“ì€ ë‹¤ìŒ ì‹¤í–‰\n",
    "# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ:  C:\\Users\\storm\\AppData\\Local\\nomic.ai\\GPT4All\\gpt4all-falcon-newbpe-q4_0.gguf\n",
    "\n",
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "model_path = \"C:/Users/storm/AppData/Local/nomic.ai/GPT4All/gpt4all-falcon-newbpe-q4_0.gguf\"\n",
    "\n",
    "llm = GPT4All(model=model_path)\n",
    "response = llm.invoke(\"Why is the sky blue?\") # í•œêµ­ì–´ëŠ” ì§€ì› ì•ˆë¨\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bcf6cea-dfcb-4bc1-9038-7287921a64e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•˜ëŠ˜ì´ íŒŒë€ìƒ‰ìœ¼ë¡œ ë³´ì´ëŠ” ì´ìœ ëŠ” ëŒ€ê¸° ì¤‘ ì‚°ì†Œì™€ ì§ˆì†Œê°€ í¬í•¨ëœ ê°€ìŠ¤ ë¶„ìë“¤ì´ íƒœì–‘ê´‘ì„ í¡ìˆ˜í•˜ê³  ì¬ë°©ì¶œí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ë¹›ì˜ ì‚°ë€ì´ë¼ê³  ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "íƒœì–‘ì—ì„œ ë‚˜ì˜¤ëŠ” í–‡ë¹›ì€ ì£¼ë¡œ íŒŒë‘ê³¼ ë³´ë¼ìƒ‰ ê°™ì€ ë‹¨íŒŒì¥ ìƒ‰ê¹”ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ê³µê¸° ì…ìì— ì˜í•´ ë” ì˜ ì‚°ë€ë©ë‹ˆë‹¤. ëŒ€ê¸° ì¤‘ ê°€ìŠ¤ ë¶„ìë“¤ì€ íƒœì–‘ê´‘ì„ í¡ìˆ˜í•˜ê³  ì¬ë°©ì¶œí•˜ëŠ”ë°, ì´ ê³¼ì •ì—ì„œ ì¼ë¶€ ë¹›ì„ ë‹¤ì‹œ ì§€êµ¬ í‘œë©´ìœ¼ë¡œ ë˜ëŒë ¤ ë³´ë‚´ê²Œ ë©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ ë¹›ì˜ ì‚°ë€ì€ í•˜ëŠ˜ì— íŒŒë€ìƒ‰ì„ ë¶€ì—¬í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” í–‡ë¹›ê³¼ ê³µê¸° ì…ì ì‚¬ì´ì˜ ìƒí˜¸ì‘ìš© ë•Œë¬¸ì— ë‹¤ì–‘í•œ ìƒ‰ì¡°ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ëŒ€ê¸° ì¤‘ ìˆ˜ì¦ê¸°ê°€ íƒœì–‘ê´‘ì„ í¡ìˆ˜í•˜ê³  ì¬ë°©ì¶œí•˜ë©´ í•˜ëŠ˜ì´ ë” í‘¸ë¥¸ìƒ‰ìœ¼ë¡œ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "í•˜ëŠ˜ì´ íŒŒë€ìƒ‰ìœ¼ë¡œ ë³´ì´ëŠ” ì´ìœ ëŠ” ëŒ€ê¸° ì¤‘ ì‚°ì†Œì™€ ì§ˆì†Œê°€ í¬í•¨ëœ ê°€ìŠ¤ ë¶„ìë“¤ì´ íƒœì–‘ê´‘ì„ í¡ìˆ˜í•˜ê³  ì¬ë°©ì¶œí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ë¹›ì˜ ì‚°ë€ì´ë¼ê³  ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.ëŠ” ëŒ€ê¸° ì¤‘ ë¬¼ë°©ìš¸ ìˆ˜ê°€ ë§ì•„ì ¸ í–‡ë¹›ì„ ë” ê°•í•˜ê²Œ ì‚°ë€ì‹œì¼œ í•˜ëŠ˜ì„ ë”ìš± ì„ ëª…í•˜ê²Œ ë³´ì´ê²Œ í•©ë‹ˆë‹¤.\n",
      "\n",
      "íƒœì–‘ì—ì„œ ë‚˜ì˜¤ëŠ” í–‡ë¹›ì€ ì£¼ë¡œ íŒŒë‘ê³¼ ë³´ë¼ìƒ‰ ê°™ì€ ë‹¨íŒŒì¥ ìƒ‰ê¹”ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ê³µê¸° ì…ìì— ì˜í•´ ë” ì˜ ì‚°ë€ë©ë‹ˆë‹¤. ëŒ€ê¸° ì¤‘ ê°€ìŠ¤ ë¶„ìë“¤ì€ íƒœì–‘ê´‘ì„ í¡ìˆ˜í•˜ê³  ì¬ë°©ì¶œí•˜ëŠ”ë°, ì´ ê³¼ì •ì—ì„œ ì¼ë¶€ ë¹›ì„ ë‹¤ì‹œ ì§€êµ¬ í‘œë©´ìœ¼ë¡œ ë˜ëŒë ¤ ë³´ë‚´ê²Œ ë©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ ë¹›ì˜ ì‚°ë€ì€ í•˜ëŠ˜ì— íŒŒë€ìƒ‰ì„ ë¶€ì—¬í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” í–‡ë¹›ê³¼ ê³µê¸° ì…ì ì‚¬ì´ì˜ ìƒí˜¸ì‘ìš© ë•Œë¬¸ì— ë‹¤ì–‘í•œ ìƒ‰ì¡°ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ëŒ€ê¸° ì¤‘ ìˆ˜ì¦ê¸°ê°€ íƒœì–‘ê´‘ì„ í¡ìˆ˜í•˜ê³  ì¬ë°©ì¶œí•˜ë©´ í•˜ëŠ˜ì´ ë” í‘¸ë¥¸ìƒ‰ìœ¼ë¡œ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë˜í•œ, ì§€í‰ì„  ê·¼ì²˜ì˜ êµ¬ë¦„ì´ë‚˜ ë¨¼ì§€ ê°™ì€ ë‹¤ë¥¸ ìš”ì†Œë“¤ë„ ë¹› ì‚°ë€ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìœ¼ë©°, ì´ë¡œ ì¸í•´ í•˜ëŠ˜ ìƒ‰ê¹”ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•´ì§ˆë…˜ê³¼ ìƒˆë²½ì—ëŠ” ëŒ€ê¸° ì¤‘ ë¬¼ë°©ìš¸ ìˆ˜ê°€ ë§ì•„ì ¸ í–‡ë¹›ì„ ë” ê°•í•˜ê²Œ ì‚°ë€ì‹œì¼œ í•˜ëŠ˜ì„ ë”ìš± ì„ ëª…í•˜ê²Œ ë³´ì´ê²Œ í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf í•œêµ­ì–´ ì§€ì› ëª¨ë¸ : 5.7GB \n",
    "# ìœˆë„ìš° ë©”ë‰´ì˜ GTP4All í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•´ì„œ ëª¨ë¸ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•´ ë†“ì€ ë‹¤ìŒ ì‹¤í–‰\n",
    "# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ:  C:\\Users\\storm\\AppData\\Local\\nomic.ai\\GPT4All\\EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf\n",
    "\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ \n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
    "<s>Human: {question}</s>\n",
    "<s>Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "#  ëª¨ë¸ ì´ˆê¸°í™”\n",
    "# modelëŠ” GPT4All ëª¨ë¸ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •\n",
    "model_path = \"C:/Users/storm/AppData/Local/nomic.ai/GPT4All/EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf\"\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=model_path,\n",
    "    # backend=\"gpu\",  # GPU ì„¤ì •\n",
    "    streaming=True,   # ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],  # ì½œë°± ì„¤ì •\n",
    ")\n",
    "\n",
    "# ì²´ì¸ ìƒì„±\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "response = chain.invoke({\"question\": \"í•˜ëŠ˜ì€ ì™œ íŒŒë€ê°€ìš”?\"})   # CPUì—ì„œ ë™ì‘ ì‘ë‹µ ëŠë¦¼\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f6db3-46b8-48af-a5e3-1639f303f9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
