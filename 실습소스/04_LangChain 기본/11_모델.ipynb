{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a130c772-5ef1-4d51-b13c-62a48bf2228e",
   "metadata": {},
   "source": [
    "## LLM 모델\n",
    "LangChain에서 LLM은 대규모 언어 모델(GPT, Claude, Gemini, Llama 등)을 일관된 인터페이스로 연결해 텍스트 생성·분석을 수행하는 핵심 엔진이다.<br>\n",
    "LLM은 AI의 뇌 역할을 하며 LangChain은 이를 체인·에이전트 구조로 확장해 다양한 응용 서비스를 구현한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461cd27f-fd21-4e62-94ac-cc1f413c29cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일의 내용 불러오기\n",
    "load_dotenv(\"C:/env/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8cef9-1cc2-4dea-9794-83823dad80ed",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c827a797-3b73-4cb0-a459-4bbebd9f4302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘이 파란 이유는 대기 중의 산란 현상 때문입니다. 태양빛은 여러 색의 빛으로 이루어져 있으며, 각 색은 서로 다른 파장을 가지고 있습니다. 이 중 파란색 빛은 파장이 짧기 때문에 대기 중의 먼지와 공기 분자에 의해 더 많이 산란됩니다. \n",
      "\n",
      "태양이 하늘 높이 있을 때, 우리가 보는 하늘은 주로 이 산란된 파란색 빛을 반사하여 나타나므로, 하늘이 파랗게 보이는 것입니다. 해가 지거나 지평선 가까이 있을 때는, 태양빛이 대기를 통과하는 거리가 길어져, 짧은 파장의 파란색 빛은 많이 산란되고 긴 파장의 빨간색이나 주황색 빛이 더욱 두드러지게 보여지면서 아름다운 석양의 색을 만들어냅니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "response= model.invoke(\"하늘은 왜 파란가요?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf485e30-17c7-4470-9add-08fa52bc521d",
   "metadata": {},
   "source": [
    "### Anthropic\n",
    "https://docs.claude.com/en/docs/about-claude/models/overview?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b97f36-b891-4e20-b622-a8c4eae003aa",
   "metadata": {},
   "source": [
    ".env 파일에 ANTHROPIC_API_KEY=\"YOUR_API_KEY\" 를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf36cda-a9f7-4ba2-8035-43a0df0aa4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-anthropic\n",
    "# notebook Kernel Restart!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11987ec2-5b61-4e80-9661-5b5d4b6eccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 하늘이 파란 이유\n",
      "\n",
      "하늘이 파랗게 보이는 이유는 **레일리 산란(Rayleigh scattering)** 때문입니다.\n",
      "\n",
      "## 원리\n",
      "\n",
      "1. **태양빛의 구성**\n",
      "   - 태양빛은 여러 색깔(파장)이 섞인 백색광입니다\n",
      "\n",
      "2. **대기와의 상호작용**\n",
      "   - 태양빛이 대기를 통과할 때 공기 분자와 충돌합니다\n",
      "   - 파란색 계열의 짧은 파장은 더 많이 산란됩니다\n",
      "   - 빨간색 계열의 긴 파장은 덜 산란됩니다\n",
      "\n",
      "3. **우리 눈에 보이는 것**\n",
      "   - 사방으로 산란된 파란빛이 우리 눈에 들어옵니다\n",
      "   - 그래서 하늘 전체가 파랗게 보입니다\n",
      "\n",
      "## 재미있는 사실\n",
      "\n",
      "- **일몰이 빨간 이유**: 해질녘에는 빛이 대기를 더 길게 통과하면서 파란빛은 모두 산란되고, 빨간빛만 우리에게 도달하기 때문입니다\n",
      "\n",
      "- **우주에서 본 하늘**: 대기가 없는 우주에서는 하늘이 검게 보입니다\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 모델 초기화\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\",  # 사용 가능한 버전: opus, sonnet, haiku 등\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 메시지 구성\n",
    "messages = [HumanMessage(content=\"하늘은 왜 파란가요?\")]\n",
    "\n",
    "# 모델 응답\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49857d7a-218f-49fc-885d-cc480e0042a9",
   "metadata": {},
   "source": [
    "### Google Gemini\n",
    "https://ai.google.dev/gemini-api/docs/models/gemini?hl=ko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e5970-2bba-4b32-b447-d32125e1b884",
   "metadata": {},
   "source": [
    ".env 파일에 GOOGLE_API_KEY=\"YOUR_API_KEY\"를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccdbb860-fd62-4b9f-8a27-ef4eeec27c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0230888-984e-499a-8ada-3e6da67e1eb1",
   "metadata": {},
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# 모델 초기화\n",
    "# model: \"gemini-2.5-pro-latest\" 또는 \"gemini-2.5-flash-latest\", \"gemini-pro\" 등을 사용할 수 있습니다.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\") \n",
    "\n",
    "# 메시지 구성\n",
    "prompt = \"안녕하세요! 나는 홍길동입니다.\"\n",
    "\n",
    "# 모델 응답\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e82fe-3887-4a7b-9bfd-d5c4980946cb",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "Ollama는 로컬 환경에서 Llama 3, Mistral, Gemma 등 다양한 LLM을 직접 실행할 수 있게 해주는 오픈소스 플랫폼이다.\n",
    "클라우드 요금 없이 빠르고 보안성 높게 AI 모델을 구동·연동할 수 있다는 점이 가장 큰 장점이다. <br>\n",
    " https://ollama.com/\n",
    " \n",
    "- Llama 3: 성능 최고, 논리·코딩·대화 품질이 가장 우수하지만 모델이 커서 속도는 다소 느림. <br>\n",
    "- Mistral 7B : 성능과 속도의 균형형, 중간 크기 모델로 가볍고 빠르며 추론력도 양호. <br>\n",
    "- Gemma 2 / 3 : ⚡속도 가장 빠름, 작은 환경에 적합하지만 이해력·정확도는 낮음. <br>\n",
    "고성능이 목표라면 Llama 3, 속도·효율을 원하면 Mistral, 저사양 환경이면 Gemma가 가장 적합하다. <br>\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ff3cfb9-c17d-43c7-bc1e-91b833479b92",
   "metadata": {},
   "source": [
    "https://ollama.com/ 에서 OllamaSetup.exe 파일 다운로드 (1.12GB) 설치\n",
    "Ollama 실행 후 \"gemma3:4b\" 모델 다운로드 후 진행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670b03b8-4c45-48ae-9dd9-3725f845c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e013e6-3e5d-48c2-9e07-9960ad620bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘이 파랗게 보이는 이유는 **레일리 산란(Rayleigh scattering)**이라는 현상 때문입니다. 좀 더 자세히 설명해 드릴게요.\n",
      "\n",
      "1. **태양빛의 구성:** 태양빛은 빨간색, 주황색, 노란색, 초록색, 파란색, 보라색 등 다양한 색깔의 빛으로 이루어져 있습니다. 이 빛들은 서로 다른 파장을 가지고 있습니다. 파장이 짧을수록 에너지가 높고, 파장이 길수록 에너지가 낮습니다.\n",
      "\n",
      "2. **대기 중의 입자:** 지구를 둘러싼 대기에는 질소, 산소와 같은 작은 입자들이 떠다니고 있습니다.\n",
      "\n",
      "3. **레일리 산란:** 태양빛이 대기 중의 작은 입자들과 부딪히면서 빛이 사방으로 흩어지는 현상을 레일리 산란이라고 합니다. 특히 파장이 짧은 파란색과 보라색 빛은 파장이 길한 빨간색 빛보다 대기 중의 입자들과 더 쉽게 산란됩니다.\n",
      "\n",
      "4. **파란색 빛이 더 많이 보이는 이유:** 태양빛이 대기 중을 통과할 때, 파란색 빛이 다른 색깔의 빛보다 훨씬 더 많이 산란됩니다. 이 산란된 파란색 빛이 우리 눈에 들어오면서 하늘이 파랗게 보이는 것입니다.\n",
      "\n",
      "**보라색 빛은 왜 파란색보다 더 많이 산란될까요?**\n",
      "\n",
      "보라색 빛은 파란색 빛보다 파장이 더 짧기 때문에, 대기 중의 입자들에 의해 더 많이 산란됩니다. 하지만 태양빛 자체에 보라색 빛의 양이 파란색 빛보다 적고, 우리 눈이 파란색 빛에 더 민감하기 때문에 하늘이 파랗게 보이는 것입니다.\n",
      "\n",
      "**해가 질 때 하늘이 붉게 보이는 이유는 무엇일까요?**\n",
      "\n",
      "해가 질 때는 태양빛이 대기층을 더 긴 거리를 통과해야 합니다. 이 과정에서 파란색 빛은 대부분 산란되어 사라지고, 상대적으로 파장이 긴 빨간색 빛이나 주황색 빛이 우리 눈에 도달하기 때문입니다.\n",
      "\n",
      "이해가 되셨나요? 😊 궁금한 점이 있다면 언제든지 다시 질문해주세요!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Ollama 모델을 불러옵니다.\n",
    "llm = ChatOllama(model=\"gemma3:4b\")\n",
    "\n",
    "response= llm.invoke(\"하늘은 왜 파란가요?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f3a87b-ab4b-45df-98a3-84249ab1fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Ollama 모델을 불러옵니다.\n",
    "llm = ChatOllama(model=\"gemma3:12b\")\n",
    "\n",
    "response= llm.invoke(\"하늘은 왜 파란가요?\")\n",
    "print(response.content)\n",
    "# 응답 오래 걸림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f3715-eed1-468d-b0d3-66dc2edeb99a",
   "metadata": {},
   "source": [
    "### HuggingFacePipeline\n",
    "HuggingFacePipeline은 LangChain에서 Hugging Face의 Transformer 모델을 직접 호출할 수 있도록 연결해주는 클래스이다.<br>\n",
    "OpenAI API처럼 외부 LLM을 쓰지 않고 로컬 또는 Hugging Face Hub 모델을 바로 사용할 수 있게 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668591fd-2048-44e1-953d-6c4ce4358524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch  langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "735b595c-0dad-4c1d-95e8-7bea9ab891bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain Why the sky is blue.\n",
      "\n",
      "That's why the \"Blue Sky\" is so great. It's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky is blue, it's the only place in the world where it can be seen.\n",
      "\n",
      "The sky\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# 1. Hugging Face 파이프라인 생성\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",     # 영어 전용 모델    ,  성능 낮음      \n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 2. LangChain용 래퍼로 감싸기\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 3. 모델 호출\n",
    "response = llm.invoke(\"Explain Why the sky is blue.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb4fb54a-d541-4426-9b98-876c3f9d6992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eb40dd016747998d17a2cc5df24361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# HuggingFace 모델을 다운로드\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # 사용할 모델의 ID\n",
    "    task=\"text-generation\",          # 수행할 작업을 지정 -> 텍스트 생성\n",
    "                                     \n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "# 모델 크기 12.5 GB  , 100 Mbps에서 약 17분 예상, 다운로드 완료 후 재실행시는 캐시에서 읽어서 바로 실행됨\n",
    "# 다운로드 경로 : C:\\Users\\storm\\.cache\\huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aff4ab-b628-4f9c-aa4f-f508834a9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 호출 (추론)  : # CPU 환경에서 수십분 소요\n",
    "prompt = \"하늘은 왜 파란가요?\"\n",
    "response = hf.invoke(prompt)  # CPU 환경에서 수십분 소요\n",
    "\n",
    "print(\"질문:\", prompt)\n",
    "print(\"모델 응답:\", response)\n",
    "\n",
    "# 일반 데스크탑 (i5, 16GB RAM) 6~8코어 --> 20 ~ 40분\n",
    "# 토큰 생성 속도 초당 0.5~1 토큰n\n",
    "# CUDA 지원 GPU (예: RTX 3060 이상) device_map=\"auto\" 또는 device=0 설정으로 GPU로 전환 → 512토큰 기준 5 ~ 10초 이내 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761e98e-0263-455f-a4d2-8ef0dff94723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU가 있을 경우 : device_map=\"auto\" 또는 device=0 으로 설정\n",
    "# GPU 없으면 오류\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,\n",
    "    # model_kwargs={\"device_map\": \"auto\"},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "# 모델 호출 (추론)\n",
    "prompt = \"하늘은 왜 파란가요?\"\n",
    "response = hf.invoke(prompt)\n",
    "\n",
    "print(\"질문:\", prompt)\n",
    "print(\"모델 응답:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b157bf2-ba5d-4edc-a959-3e732a5eec81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1645ece5a58e4b2b9f784c7b2a5ef63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"EleutherAI/polyglot-ko-1.3b\", # 성능 낮음\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100, \"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"하늘은 왜 파란가? 그 이유는\")\n",
    "print(response)\n",
    "# 모델 크기 약 2.1 GB  , 100 Mbps에서 약 3~5분 예상, 다운로드 완료 후 재실행시는 캐시에서 읽어서 바로 실행됨\n",
    "# 다운로드 경로 : C:\\Users\\storm\\.cache\\huggingface\n",
    "\n",
    "# Polyglot-Ko-1.3B는 “Instruction (질문-답변)” 모델이 아님\n",
    "# 이 모델은 단순 언어 모델 (Language Model) 입니다.\n",
    "# “Q&A 구조”로 학습된 게 아니라 “문장 다음에 올 단어를 예측하는” 형태로 학습됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63b16a-30c6-4d58-b168-a8e6afa02a29",
   "metadata": {},
   "source": [
    "### GPT4All\n",
    "GPT4All은 Nomic AI가 개발한 오픈소스 로컬 LLM(Local Large Language Model) 프레임워크이다. <br>\n",
    "이 모델은 인터넷 연결 없이 실행할 수 있으며, GPU 없이도 CPU 기반으로 추론 가능하다는 점이 특징이다. <br>\n",
    "https://www.nomic.ai/gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8e6f4-76e5-4f13-af00-7236cb6eb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install  gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d66a3c-d0b0-4e74-b5b0-6736513fd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTP4All Falcon 모델 : (3.9GB) , 한국어 지원 안됨\n",
    "# 윈도우 메뉴의 GTP4All 프로그램을 실행해서 모델을 미리 다운로드해 놓은 다음 실행\n",
    "# 모델 다운로드 경로:  C:\\Users\\storm\\AppData\\Local\\nomic.ai\\GPT4All\\gpt4all-falcon-newbpe-q4_0.gguf\n",
    "\n",
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "model_path = \"C:/Users/storm/AppData/Local/nomic.ai/GPT4All/gpt4all-falcon-newbpe-q4_0.gguf\"\n",
    "\n",
    "llm = GPT4All(model=model_path)\n",
    "response = llm.invoke(\"Why is the sky blue?\") # 한국어는 지원 안됨\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf6cea-dfcb-4bc1-9038-7287921a64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf 한국어 지원 모델 : 5.7GB \n",
    "# 윈도우 메뉴의 GTP4All 프로그램을 실행해서 모델을 미리 다운로드해 놓은 다음 실행\n",
    "# 모델 다운로드 경로:  C:\\Users\\storm\\AppData\\Local\\nomic.ai\\GPT4All\\EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf\n",
    "\n",
    "rom langchain_community.llms import GPT4All\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 프롬프트 \n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
    "<s>Human: {question}</s>\n",
    "<s>Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "#  모델 초기화\n",
    "# model는 GPT4All 모델 파일의 경로를 지정\n",
    "model_path = \"C:/Users/storm/AppData/Local/nomic.ai/GPT4All/EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf\"\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=model_path,\n",
    "    # backend=\"gpu\",  # GPU 설정\n",
    "    streaming=True,   # 스트리밍 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],  # 콜백 설정\n",
    ")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 실행\n",
    "response = chain.invoke({\"question\": \"하늘은 왜 파란가요?\"})   # CPU에서 동작 응답 느림\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f6db3-46b8-48af-a5e3-1639f303f9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
